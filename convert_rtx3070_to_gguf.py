#!/usr/bin/env python3
"""
RTX 3070 ìµœì¢… LoRA ì–´ëŒ‘í„°ë¥¼ GGUF ì–‘ìí™” íŒŒì¼ë¡œ ë³€í™˜
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def convert_final_lora_to_merged():
    """RTX 3070 ìµœì¢… LoRA ì–´ëŒ‘í„°ë¥¼ ê¸°ë³¸ ëª¨ë¸ê³¼ ë³‘í•©"""
    
    print("ğŸ”„ RTX 3070 ìµœì¢… ëª¨ë¸ì„ ë³‘í•© ëª¨ë¸ë¡œ ë³€í™˜ ì‹œì‘...")
    
    # ê²½ë¡œ ì„¤ì • - ìµœì¢… ì™„ë£Œ ëª¨ë¸ ì‚¬ìš©
    base_model_path = "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"
    adapter_path = "./models/rtx3070_optimized_final"
    output_path = "./models/rtx3070_final_merged"
    
    # ì–´ëŒ‘í„° ì¡´ì¬ í™•ì¸
    if not os.path.exists(adapter_path):
        print(f"âŒ ìµœì¢… ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {adapter_path}")
        return False
    
    print(f"ğŸ“ ì‚¬ìš©í•  ìµœì¢… ëª¨ë¸: {adapter_path}")
    
    try:
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        # 1. ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (CPU ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)
        print("ğŸ“š ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘... (CPU ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½)")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float32,  # CPUì—ì„œëŠ” float32 ì‚¬ìš©
            device_map="cpu",  # CPUë¡œ ê°•ì œ ë¡œë“œí•˜ì—¬ GPU ë©”ëª¨ë¦¬ ì ˆì•½
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # 2. í† í¬ë‚˜ì´ì € ë¡œë“œ
        print("ğŸ”¤ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_path)
        
        # 3. LoRA ì–´ëŒ‘í„° ì ìš© (CPUì—ì„œ)
        print("ğŸ”§ RTX 3070 ìµœì¢… LoRA ì–´ëŒ‘í„° ì ìš© ì¤‘...")
        model = PeftModel.from_pretrained(
            base_model, 
            adapter_path,
            torch_dtype=torch.float32
        )
        
        # 4. LoRAë¥¼ ê¸°ë³¸ ëª¨ë¸ê³¼ ë³‘í•©
        print("ğŸ”€ LoRA ì–´ëŒ‘í„°ë¥¼ ê¸°ë³¸ ëª¨ë¸ê³¼ ë³‘í•© ì¤‘...")
        merged_model = model.merge_and_unload()
        
        # 5. ë³‘í•©ëœ ëª¨ë¸ ì €ì¥
        print("ğŸ’¾ ë³‘í•©ëœ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘...")
        if not os.path.exists(output_path):
            os.makedirs(output_path)
            
        merged_model.save_pretrained(output_path, safe_serialization=True)
        tokenizer.save_pretrained(output_path)
        
        print(f"âœ… ë³‘í•©ëœ ìµœì¢… ëª¨ë¸ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        model_files = os.listdir(output_path)
        print(f"ğŸ“‚ ì €ì¥ëœ íŒŒì¼ë“¤: {model_files}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ë³€ìˆ˜ë“¤ ì‚­ì œ
        try:
            del base_model, model, merged_model
        except:
            pass

if __name__ == "__main__":
    # ì‹œìŠ¤í…œ ì •ë³´ ì¶œë ¥
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory
        print(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {total_memory // 1024**3}GB")
    else:
        print("ğŸ–¥ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    # ë³€í™˜ ì‹¤í–‰
    success = convert_final_lora_to_merged()
    
    if success:
        print("\nğŸ‰ RTX 3070 ìµœì¢… ëª¨ë¸ ë³‘í•© ì™„ë£Œ!")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. llama.cppë¥¼ ì‚¬ìš©í•´ì„œ GGUFë¡œ ë³€í™˜:")
        print("   python llama.cpp/convert.py ./models/rtx3070_final_merged --outdir ./models/ --outfile rtx3070_final.gguf")
        print("2. Q5_K_M ì–‘ìí™”:")
        print("   ./llama.cpp/quantize ./models/rtx3070_final.gguf ./models/rtx3070_final.Q5_K_M.gguf Q5_K_M")
    else:
        print("\nâŒ ë³‘í•©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
