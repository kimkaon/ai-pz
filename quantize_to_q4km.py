#!/usr/bin/env python3
"""
RTX 3070 GGUF ëª¨ë¸ì„ Q4_K_Mìœ¼ë¡œ ì–‘ìí™”í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
13.5GB â†’ ì•½ 4-5GBë¡œ í¬ê¸° ì¶•ì†Œ
"""

import os
import sys
import subprocess
from pathlib import Path

def main():
    print("ğŸ”¥ RTX 3070 GGUF ëª¨ë¸ Q4_K_M ì–‘ìí™” ì‹œì‘")
    print("=" * 60)
    
    # ê²½ë¡œ ì„¤ì •
    project_root = Path("e:/Work/ai pz2")
    original_model = project_root / "models" / "rtx3070_final_merged.gguf"
    quantized_model = project_root / "models" / "rtx3070_final_merged_q4km.gguf"
    llama_cpp_dir = project_root / "llama.cpp"
    
    print(f"ğŸ“‚ ì›ë³¸ ëª¨ë¸: {original_model}")
    print(f"ğŸ“‚ ì–‘ìí™” ëª¨ë¸: {quantized_model}")
    print(f"ğŸ“‚ llama.cpp ë””ë ‰í† ë¦¬: {llama_cpp_dir}")
    
    # ì›ë³¸ ëª¨ë¸ ì¡´ì¬ í™•ì¸
    if not original_model.exists():
        print(f"âŒ ì›ë³¸ GGUF ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {original_model}")
        return False
    
    # ì›ë³¸ ëª¨ë¸ í¬ê¸° í™•ì¸
    original_size_gb = original_model.stat().st_size / (1024**3)
    print(f"ğŸ“Š ì›ë³¸ ëª¨ë¸ í¬ê¸°: {original_size_gb:.1f}GB")
    
    # llama.cpp ë””ë ‰í† ë¦¬ í™•ì¸
    if not llama_cpp_dir.exists():
        print(f"âŒ llama.cpp ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {llama_cpp_dir}")
        return False
    
    # ì–‘ìí™” ë°©ë²•ë“¤ ì‹œë„
    print("\nğŸ”§ ì–‘ìí™” ë°©ë²• ì‹œë„ ì¤‘...")
    
    # ë°©ë²• 1: llama.cppì˜ quantize ì‹¤í–‰íŒŒì¼ ì‚¬ìš©
    quantize_exe = None
    possible_paths = [
        llama_cpp_dir / "build" / "bin" / "quantize.exe",
        llama_cpp_dir / "build" / "Release" / "quantize.exe",
        llama_cpp_dir / "build" / "Debug" / "quantize.exe",
        llama_cpp_dir / "quantize.exe",
        llama_cpp_dir / "build" / "quantize.exe"
    ]
    
    for path in possible_paths:
        if path.exists():
            quantize_exe = path
            print(f"âœ… quantize ì‹¤í–‰íŒŒì¼ ë°œê²¬: {quantize_exe}")
            break
    
    if quantize_exe:
        try:
            print(f"âš¡ quantize.exeë¡œ Q4_K_M ì–‘ìí™” ì‹œì‘...")
            cmd = [
                str(quantize_exe),
                str(original_model),
                str(quantized_model),
                "Q4_K_M"
            ]
            
            print(f"ğŸ”„ ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
            
            # ì–‘ìí™” ì‹¤í–‰
            result = subprocess.run(
                cmd,
                cwd=str(llama_cpp_dir),
                capture_output=True,
                text=True,
                timeout=1800  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
            )
            
            if result.returncode == 0:
                print("âœ… quantize.exeë¡œ ì–‘ìí™” ì„±ê³µ!")
                print("ğŸ“¤ ì¶œë ¥:", result.stdout)
                
                # ê²°ê³¼ í™•ì¸
                if quantized_model.exists():
                    quantized_size_gb = quantized_model.stat().st_size / (1024**3)
                    compression_ratio = (1 - quantized_size_gb / original_size_gb) * 100
                    
                    print(f"ğŸ‰ ì–‘ìí™” ì™„ë£Œ!")
                    print(f"ğŸ“Š ì–‘ìí™” ëª¨ë¸ í¬ê¸°: {quantized_size_gb:.1f}GB")
                    print(f"ğŸ“‰ ì••ì¶•ë¥ : {compression_ratio:.1f}% ì ˆì•½")
                    print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {quantized_model}")
                    return True
                else:
                    print("âŒ ì–‘ìí™” íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return False
            else:
                print(f"âŒ quantize.exe ì‹¤í–‰ ì‹¤íŒ¨: {result.stderr}")
        
        except subprocess.TimeoutExpired:
            print("âŒ ì–‘ìí™” ì‹œê°„ ì´ˆê³¼ (30ë¶„)")
            return False
        except Exception as e:
            print(f"âŒ quantize.exe ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
    
    # ë°©ë²• 2: Python llama-cpp-pythonìœ¼ë¡œ ì–‘ìí™” ì‹œë„
    print("\nğŸ Python llama-cpp-pythonìœ¼ë¡œ ì‹œë„...")
    
    try:
        from llama_cpp import Llama
        
        print("âš¡ llama-cpp-pythonìœ¼ë¡œ ëª¨ë¸ ë¡œë“œ ë° ì–‘ìí™”...")
        
        # ì›ë³¸ ëª¨ë¸ ë¡œë“œ
        llm = Llama(
            model_path=str(original_model),
            n_gpu_layers=0,  # CPUë¡œ ë¡œë“œ
            n_ctx=512,       # ë©”ëª¨ë¦¬ ì ˆì•½
            n_threads=8,
            verbose=True
        )
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print("âŒ llama-cpp-pythonì€ ì§ì ‘ ì–‘ìí™”ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        print("ğŸ’¡ ëŒ€ì•ˆ: quantize.exeë¥¼ ì§ì ‘ ë¹Œë“œí•˜ê±°ë‚˜ ë‹¤ë¥¸ ë„êµ¬ ì‚¬ìš© í•„ìš”")
        
    except ImportError:
        print("âŒ llama-cpp-pythonì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    except Exception as e:
        print(f"âŒ Python ì–‘ìí™” ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 3: ggml-python ì‚¬ìš© ì‹œë„
    print("\nğŸ”„ ëŒ€ì•ˆ: llama.cpp ë¹Œë“œ ë° quantize.exe ìƒì„±...")
    
    try:
        # CMakeë¡œ ë¹Œë“œ ì‹œë„
        print("âš¡ CMakeë¡œ llama.cpp ë¹Œë“œ ì‹œë„...")
        
        build_dir = llama_cpp_dir / "build"
        if not build_dir.exists():
            build_dir.mkdir()
        
        # CMake ì„¤ì •
        cmake_cmd = [
            "cmake",
            "..",
            "-DLLAMA_BUILD_TESTS=OFF",
            "-DLLAMA_BUILD_EXAMPLES=ON",
            "-DLLAMA_BUILD_SERVER=OFF"
        ]
        
        print(f"ğŸ”§ CMake ì„¤ì •: {' '.join(cmake_cmd)}")
        result = subprocess.run(
            cmake_cmd,
            cwd=str(build_dir),
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if result.returncode == 0:
            print("âœ… CMake ì„¤ì • ì„±ê³µ")
            
            # ë¹Œë“œ ì‹¤í–‰
            build_cmd = ["cmake", "--build", ".", "--config", "Release"]
            print(f"ğŸ”¨ ë¹Œë“œ ì‹¤í–‰: {' '.join(build_cmd)}")
            
            result = subprocess.run(
                build_cmd,
                cwd=str(build_dir),
                capture_output=True,
                text=True,
                timeout=1200  # 20ë¶„
            )
            
            if result.returncode == 0:
                print("âœ… llama.cpp ë¹Œë“œ ì„±ê³µ!")
                
                # quantize.exe ì¬í™•ì¸
                for path in possible_paths:
                    if path.exists():
                        print(f"ğŸ¯ quantize.exe ë°œê²¬: {path}")
                        # ë‹¤ì‹œ ì–‘ìí™” ì‹œë„
                        return main()  # ì¬ê·€ í˜¸ì¶œ
                
                print("âŒ ë¹Œë“œ í›„ì—ë„ quantize.exeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            else:
                print(f"âŒ ë¹Œë“œ ì‹¤íŒ¨: {result.stderr}")
        else:
            print(f"âŒ CMake ì„¤ì • ì‹¤íŒ¨: {result.stderr}")
            
    except subprocess.TimeoutExpired:
        print("âŒ ë¹Œë“œ ì‹œê°„ ì´ˆê³¼")
    except Exception as e:
        print(f"âŒ ë¹Œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    print("\nğŸ’¡ ìˆ˜ë™ í•´ê²° ë°©ë²•:")
    print("1. Visual Studio Community 2022 ì„¤ì¹˜")
    print("2. CMake 3.12+ ì„¤ì¹˜")
    print("3. llama.cpp í´ë”ì—ì„œ:")
    print("   mkdir build && cd build")
    print("   cmake ..")
    print("   cmake --build . --config Release")
    print("4. ìƒì„±ëœ quantize.exeë¡œ ìˆ˜ë™ ì–‘ìí™”:")
    print(f"   quantize.exe {original_model} {quantized_model} Q4_K_M")
    
    return False

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ ì–‘ìí™” ì™„ë£Œ! RTX 3070ì—ì„œ GPU ê°€ì†ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    else:
        print("\nğŸ˜… ìë™ ì–‘ìí™” ì‹¤íŒ¨. ìˆ˜ë™ ë¹Œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    
    input("\nì•„ë¬´ í‚¤ë‚˜ ëˆŒëŸ¬ ì¢…ë£Œ...")
