"""
ì´ˆê¸°í™” ëª¨ë“ˆ
main.pyì—ì„œ ë¶„ë¦¬ëœ ì´ˆê¸°í™” ê´€ë ¨ ê¸°ëŠ¥
"""

import torch
import warnings
import os
import logging

# ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤
_global_llm = None

def setup_environment():
    """í™˜ê²½ ì„¤ì • ì´ˆê¸°í™”"""
    # ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
    warnings.filterwarnings("ignore")
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
    os.environ['PYTORCH_WARNINGS'] = '0'
    
    # ì˜¤í”„ë¼ì¸ ëª¨ë“œ ì„¤ì •
    os.environ['HF_HUB_OFFLINE'] = '1'
    os.environ['TRANSFORMERS_OFFLINE'] = '1'
    
    # faster_whisper ë¡œê·¸ ìˆ¨ê¸°ê¸°
    logging.getLogger("faster_whisper").setLevel(logging.WARNING)
    logging.getLogger("ctranslate2").setLevel(logging.WARNING)
    
    # transformers ê²½ê³  ìˆ¨ê¸°ê¸°
    logging.getLogger("transformers").setLevel(logging.ERROR)
    logging.getLogger("transformers.generation_utils").setLevel(logging.ERROR)
    
    print("main.pyì—ì„œ GPU ì´ë¦„:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A")

def get_global_llm():
    """ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°"""
    return _global_llm

def set_global_llm(llm_instance):
    """ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •"""
    global _global_llm
    _global_llm = llm_instance

def initialize_system():
    """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    global _global_llm
    
    # í™˜ê²½ ì„¤ì •
    setup_environment()
    
    # GPU ì„¤ì •
    print("âš¡ GPU ì„¤ì • ì¤‘...")
    from config_gpu import setup_gpu
    setup_gpu(cuda_device=0)
    
    # ê¸°ë³¸ LLM ë¡œë”©
    print("ï¿½ ê¸°ë³¸ LLM ë¡œë”© ì¤‘...")
    from nous_hermes2_mistral_loader import load_nous_hermes2_mistral
    _global_llm = load_nous_hermes2_mistral()
    print("âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        from finetuning_integration import get_finetuning_stats
        from hybrid_finetuning_integration import initialize_hybrid_system
        initialize_hybrid_system(_global_llm)
        print("âœ… í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    except ImportError:
        print("âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ì—†ìŒ")
    
    # ì „ì—­ model_manager ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§ì ‘ ì‚¬ìš©
    import models.model_manager as mm
    model_manager = mm.model_manager
    model_manager.detect_available_models()
    
    print("ğŸ¯ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

def get_global_llm():
    """ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return _global_llm

def cleanup_system():
    """ì‹œìŠ¤í…œ ì •ë¦¬"""
    try:
        from models.model_manager import FINETUNING_ENABLED
        if FINETUNING_ENABLED:
            from finetuning_integration import cleanup_finetuning_system
            cleanup_finetuning_system()
    except:
        pass
