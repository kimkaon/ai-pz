#!/usr/bin/env python3
"""
GGUF ëª¨ë¸ ì–‘ìí™” ìŠ¤í¬ë¦½íŠ¸
RTX 3070 8GB VRAMì— ë§ë„ë¡ ëª¨ë¸ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

def quantize_gguf_model():
    """GGUF ëª¨ë¸ì„ Q4_K_Mìœ¼ë¡œ ì–‘ìí™”í•˜ì—¬ í¬ê¸°ë¥¼ ëŒ€í­ ì¤„ì…ë‹ˆë‹¤."""
    
    # ê²½ë¡œ ì„¤ì •
    original_model = "e:/Work/ai pz2/models/rtx3070_final_merged.gguf"
    quantized_model = "e:/Work/ai pz2/models/rtx3070_final_merged_q4km.gguf"
    
    # llama.cpp ë¹Œë“œ ê²½ë¡œ í™•ì¸
    llama_cpp_path = "e:/Work/ai pz2/llama.cpp"
    
    print("ğŸ”„ GGUF ëª¨ë¸ ì–‘ìí™” ì‹œì‘...")
    print(f"ğŸ“ ì›ë³¸ ëª¨ë¸: {original_model}")
    print(f"ğŸ“ ì–‘ìí™” ëª¨ë¸: {quantized_model}")
    
    # ì›ë³¸ ëª¨ë¸ ì¡´ì¬ í™•ì¸
    if not os.path.exists(original_model):
        print(f"âŒ ì›ë³¸ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {original_model}")
        return False
    
    # ì›ë³¸ ëª¨ë¸ í¬ê¸° í™•ì¸
    original_size_gb = os.path.getsize(original_model) / (1024**3)
    print(f"ğŸ“Š ì›ë³¸ ëª¨ë¸ í¬ê¸°: {original_size_gb:.1f}GB")
    
    # ì–‘ìí™” ë°©ë²•ë“¤ê³¼ ì˜ˆìƒ í¬ê¸°
    quantization_options = {
        "Q4_K_M": {"size_ratio": 0.35, "quality": "ë†’ìŒ", "desc": "4ë¹„íŠ¸ ì–‘ìí™” (ì¶”ì²œ)"},
        "Q5_K_M": {"size_ratio": 0.45, "quality": "ë§¤ìš° ë†’ìŒ", "desc": "5ë¹„íŠ¸ ì–‘ìí™”"},
        "Q3_K_M": {"size_ratio": 0.25, "quality": "ë³´í†µ", "desc": "3ë¹„íŠ¸ ì–‘ìí™” (ê°€ì¥ ì‘ìŒ)"},
        "Q6_K": {"size_ratio": 0.55, "quality": "ìµœê³ ", "desc": "6ë¹„íŠ¸ ì–‘ìí™”"}
    }
    
    print("\nğŸ¯ ì–‘ìí™” ì˜µì…˜:")
    for i, (qtype, info) in enumerate(quantization_options.items(), 1):
        expected_size = original_size_gb * info["size_ratio"]
        print(f"{i}. {qtype}: ~{expected_size:.1f}GB ({info['desc']}) - í’ˆì§ˆ: {info['quality']}")
    
    # ê¸°ë³¸ê°’ìœ¼ë¡œ Q4_K_M ì„ íƒ (RTX 3070ì— ìµœì )
    selected_quantization = "Q4_K_M"
    expected_size = original_size_gb * quantization_options[selected_quantization]["size_ratio"]
    
    print(f"\nâœ… {selected_quantization} ì–‘ìí™” ì„ íƒë¨")
    print(f"ğŸ“‰ ì˜ˆìƒ í¬ê¸°: {original_size_gb:.1f}GB â†’ {expected_size:.1f}GB")
    print(f"ğŸ’¾ VRAM ì ˆì•½: {original_size_gb - expected_size:.1f}GB")
    
    # llama.cpp quantize ë°”ì´ë„ˆë¦¬ ì°¾ê¸°
    possible_paths = [
        f"{llama_cpp_path}/build/bin/llama-quantize.exe",
        f"{llama_cpp_path}/build/Release/llama-quantize.exe", 
        f"{llama_cpp_path}/build/Debug/llama-quantize.exe",
        f"{llama_cpp_path}/llama-quantize.exe",
        f"{llama_cpp_path}/quantize.exe"
    ]
    
    quantize_binary = None
    for path in possible_paths:
        if os.path.exists(path):
            quantize_binary = path
            break
    
    if not quantize_binary:
        print("âŒ llama-quantize ë°”ì´ë„ˆë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ í•´ê²°ë°©ë²•:")
        print("1. llama.cpp ë¹Œë“œ: cd llama.cpp && mkdir build && cd build && cmake .. && cmake --build . --config Release")
        print("2. ë˜ëŠ” HuggingFaceì—ì„œ ì´ë¯¸ ì–‘ìí™”ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
        print("3. ë˜ëŠ” AutoGPTQ ë“± ë‹¤ë¥¸ ì–‘ìí™” ë„êµ¬ ì‚¬ìš©")
        return False
    
    print(f"ğŸ”§ ì–‘ìí™” ë„êµ¬ ë°œê²¬: {quantize_binary}")
    
    # ì–‘ìí™” ì‹¤í–‰
    import subprocess
    
    print(f"\nğŸš€ ì–‘ìí™” ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    try:
        cmd = [
            quantize_binary,
            original_model,
            quantized_model, 
            selected_quantization
        ]
        
        print(f"ğŸ’» ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=3600  # 1ì‹œê°„ ì œí•œ
        )
        
        if result.returncode == 0:
            if os.path.exists(quantized_model):
                quantized_size_gb = os.path.getsize(quantized_model) / (1024**3)
                compression_ratio = (1 - quantized_size_gb / original_size_gb) * 100
                
                print(f"\nğŸ‰ ì–‘ìí™” ì™„ë£Œ!")
                print(f"ğŸ“Š ê²°ê³¼:")
                print(f"   - ì›ë³¸: {original_size_gb:.1f}GB")
                print(f"   - ì–‘ìí™”: {quantized_size_gb:.1f}GB")
                print(f"   - ì••ì¶•ë¥ : {compression_ratio:.1f}%")
                print(f"   - ì ˆì•½: {original_size_gb - quantized_size_gb:.1f}GB")
                
                print(f"\nâœ… RTX 3070 8GB VRAMì— ë¡œë“œ ê°€ëŠ¥: {'ì˜ˆ' if quantized_size_gb < 7 else 'ì•„ë‹ˆì˜¤'}")
                return True
            else:
                print("âŒ ì–‘ìí™” ì™„ë£Œí–ˆì§€ë§Œ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
        else:
            print(f"âŒ ì–‘ìí™” ì‹¤íŒ¨: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ ì–‘ìí™” ì‹œê°„ ì´ˆê³¼ (1ì‹œê°„)")
        return False
    except Exception as e:
        print(f"âŒ ì–‘ìí™” ì˜¤ë¥˜: {e}")
        return False

def suggest_alternatives():
    """ì–‘ìí™” ëŒ€ì•ˆ ë°©ë²•ë“¤ ì œì•ˆ"""
    print("\nğŸ“‹ ëŒ€ì•ˆ ë°©ë²•ë“¤:")
    print("\n1. **HuggingFace Hubì—ì„œ ì´ë¯¸ ì–‘ìí™”ëœ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**")
    print("   - TheBlokeì˜ GGUF ëª¨ë¸ë“¤: https://huggingface.co/TheBloke")
    print("   - ê²€ìƒ‰: 'Mistral 7B GGUF Q4_K_M'")
    
    print("\n2. **AutoGPTQë¡œ ì–‘ìí™”**")
    print("   - pip install auto-gptq")
    print("   - ë” ì •ë°€í•œ ì–‘ìí™” ì œì–´ ê°€ëŠ¥")
    
    print("\n3. **BitsAndBytes 4bit/8bit ë¡œë”©**")
    print("   - transformersì™€ í•¨ê»˜ ì‚¬ìš©")
    print("   - ì‹¤ì‹œê°„ ì–‘ìí™” (ë©”ëª¨ë¦¬ ì ˆì•½)")
    
    print("\n4. **ëª¨ë¸ í”„ë£¨ë‹**")
    print("   - ë¶ˆí•„ìš”í•œ ë ˆì´ì–´/íŒŒë¼ë¯¸í„° ì œê±°")
    print("   - ë” ë³µì¡í•˜ì§€ë§Œ ë” í° ì••ì¶• ê°€ëŠ¥")

if __name__ == "__main__":
    print("ğŸ¯ RTX 3070 ìµœì í™”: GGUF ëª¨ë¸ ì–‘ìí™”")
    print("=" * 50)
    
    success = quantize_gguf_model()
    
    if not success:
        suggest_alternatives()
    else:
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("1. model_manager.pyì—ì„œ ì–‘ìí™”ëœ ëª¨ë¸ ê²½ë¡œ ì—…ë°ì´íŠ¸")
        print("2. GPU ë ˆì´ì–´ ìˆ˜ë¥¼ 32+ ë¡œ ì¦ê°€ (ë” ë§ì€ GPU í™œìš©)")
        print("3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° íŠœë‹")
