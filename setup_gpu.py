#!/usr/bin/env python3
"""
GPU λ° ν™κ²½ μ„¤μ • μƒνƒ ν™•μΈ μ¤ν¬λ¦½νΈ
"""

import sys

def check_environment():
    """ν™κ²½ μƒνƒ μ „μ²΄ μ κ²€"""
    print("=== AI PZ2 ν™κ²½ μƒνƒ μ κ²€ ===\n")
    
    # 1. PyTorch λ° CUDA
    print("1. PyTorch & CUDA μƒνƒ:")
    try:
        import torch
        print(f"   β… PyTorch λ²„μ „: {torch.__version__}")
        
        if hasattr(torch, 'cuda'):
            if torch.cuda.is_available():
                print(f"   β… CUDA μ‚¬μ© κ°€λ¥: {torch.cuda.get_device_name(0)}")
                print(f"   π’Ύ GPU λ©”λ¨λ¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            else:
                print("   β CUDA μ‚¬μ© λ¶κ°€")
        else:
            print("   β PyTorchκ°€ CPU μ „μ© λ²„μ „μ…λ‹λ‹¤")
            print("   π”§ ν•΄κ²°: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    except ImportError:
        print("   β PyTorchκ°€ μ„¤μΉλμ§€ μ•μ•μµλ‹λ‹¤")
    
    # 2. Transformers
    print("\n2. Transformers λΌμ΄λΈλ¬λ¦¬:")
    try:
        import transformers
        print(f"   β… Transformers λ²„μ „: {transformers.__version__}")
    except ImportError:
        print("   β Transformers λ―Έμ„¤μΉ")
        print("   π”§ ν•΄κ²°: pip install transformers")
    
    # 3. νμΈνλ‹ κ΄€λ ¨ ν¨ν‚¤μ§€
    print("\n3. νμΈνλ‹ ν¨ν‚¤μ§€:")
    
    packages = {
        'datasets': 'datasets',
        'peft': 'peft', 
        'bitsandbytes': 'bitsandbytes',
        'accelerate': 'accelerate'
    }
    
    missing_packages = []
    for name, import_name in packages.items():
        try:
            __import__(import_name)
            print(f"   β… {name}")
        except ImportError:
            print(f"   β {name}")
            missing_packages.append(name)
    
    if missing_packages:
        print(f"\n   π”§ ν•΄κ²°: pip install {' '.join(missing_packages)}")
    
    # 4. GGUF λ¨λΈ μ§€μ› (llama-cpp-python)
    print("\n4. GGUF λ¨λΈ μ§€μ›:")
    try:
        import llama_cpp
        print(f"   β… llama-cpp-python μ„¤μΉλ¨")
    except ImportError:
        print("   β llama-cpp-python λ―Έμ„¤μΉ")
        print("   π”§ ν•΄κ²°: pip install llama-cpp-python")
    
    # 5. ν„μ¬ λ©”μΈ λ¨λΈ ν™•μΈ
    print("\n5. λ©”μΈ ν”„λ΅κ·Έλ¨ λ¨λΈ:")
    import os
    model_path = "models/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M.gguf"
    if os.path.exists(model_path):
        size_gb = os.path.getsize(model_path) / 1024**3
        print(f"   β… {model_path} ({size_gb:.1f}GB)")
    else:
        print(f"   β {model_path} μ°Ύμ„ μ μ—†μ")
    
    # 6. νμΈνλ‹ λ°μ΄ν„° ν™•μΈ
    print("\n6. νμΈνλ‹ λ°μ΄ν„°:")
    data_files = [
        "finetuning/datasets/processed_english/unified_train.jsonl",
        "finetuning/datasets/processed_english/unified_validation.jsonl"
    ]
    
    for file_path in data_files:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = sum(1 for line in f if line.strip() and not line.startswith('#'))
            print(f"   β… {os.path.basename(file_path)}: {lines}κ° λ°μ΄ν„°")
        else:
            print(f"   β {file_path} μ—†μ")
    
    print("\n=== κ¶μ¥ νμΈνλ‹ λ°©λ²• ===")
    
    # PyTorch CUDA μ—¬λ¶€μ— λ”°λ¥Έ κ¶μ¥μ‚¬ν•­
    try:
        import torch
        if hasattr(torch, 'cuda') and torch.cuda.is_available():
            print("π€ GPU νμΈνλ‹ κ¶μ¥:")
            print("   - λ¨λΈ: NousResearch/Nous-Hermes-2-Mistral-7B-DPO")
            print("   - λ°©λ²•: LoRA + 4bit μ–‘μν™”")
            print("   - μμƒ μ‹κ°„: 30λ¶„-1μ‹κ°„")
            print("   - GPU λ©”λ¨λ¦¬: 6-8GB μ‚¬μ©")
        else:
            print("π’» CPU νμΈνλ‹:")
            print("   - λ¨λΈ: microsoft/DialoGPT-small")
            print("   - λ°©λ²•: μ „μ²΄ νμΈνλ‹")
            print("   - μμƒ μ‹κ°„: 2-4μ‹κ°„")
            print("   - RAM: 8-16GB μ‚¬μ©")
    except:
        pass

if __name__ == "__main__":
    check_environment()
