#!/usr/bin/env python3
"""
GPU ë° í™˜ê²½ ì„¤ì • ìƒíƒœ í™•ì¸ ìŠ¤í¬ë¦½íŠ¸
"""

import sys
import os

def check_environment():
    """í™˜ê²½ ìƒíƒœ ì „ì²´ ì ê²€"""
    print("=== AI PZ2 í™˜ê²½ ìƒíƒœ ì ê²€ ===\n")
    
    # 1. PyTorch ë° CUDA
    print("1. PyTorch & CUDA ìƒíƒœ:")
    try:
        import torch
        print(f"   âœ… PyTorch ë²„ì „: {torch.__version__}")
        
        if hasattr(torch, 'cuda'):
            if torch.cuda.is_available():
                print(f"   âœ… CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
                print(f"   ðŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            else:
                print("   âŒ CUDA ì‚¬ìš© ë¶ˆê°€")
        else:
            print("   âŒ PyTorchê°€ CPU ì „ìš© ë²„ì „ìž…ë‹ˆë‹¤")
            print("   ðŸ”§ í•´ê²°: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    except ImportError:
        print("   âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    # 2. Transformers
    print("\n2. Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬:")
    try:
        import transformers
        print(f"   âœ… Transformers ë²„ì „: {transformers.__version__}")
    except ImportError:
        print("   âŒ Transformers ë¯¸ì„¤ì¹˜")
        print("   ðŸ”§ í•´ê²°: pip install transformers")
    
    # 3. íŒŒì¸íŠœë‹ ê´€ë ¨ íŒ¨í‚¤ì§€
    print("\n3. íŒŒì¸íŠœë‹ íŒ¨í‚¤ì§€:")
    
    packages = {
        'datasets': 'datasets',
        'peft': 'peft', 
        'bitsandbytes': 'bitsandbytes',
        'accelerate': 'accelerate'
    }
    
    missing_packages = []
    for name, import_name in packages.items():
        try:
            __import__(import_name)
            print(f"   âœ… {name}")
        except ImportError:
            print(f"   âŒ {name}")
            missing_packages.append(name)
    
    if missing_packages:
        print(f"\n   ðŸ”§ í•´ê²°: pip install {' '.join(missing_packages)}")
    
    # 4. GGUF ëª¨ë¸ ì§€ì› (llama-cpp-python)
    print("\n4. GGUF ëª¨ë¸ ì§€ì›:")
    try:
        import llama_cpp
        print(f"   âœ… llama-cpp-python ì„¤ì¹˜ë¨")
    except ImportError:
        print("   âŒ llama-cpp-python ë¯¸ì„¤ì¹˜")
        print("   ðŸ”§ í•´ê²°: pip install llama-cpp-python")
    
    # 5. RTX 3070 ìµœì í™” ëª¨ë¸ í™•ì¸
    print("\n5. RTX 3070 ìµœì í™” ëª¨ë¸:")
    rtx_models = ["models/rtx3070_optimized_best", "models/rtx3070_optimized_final"]
    
    for model_path in rtx_models:
        if os.path.exists(model_path):
            adapter_file = os.path.join(model_path, "adapter_model.safetensors")
            config_file = os.path.join(model_path, "adapter_config.json")
            
            if os.path.exists(adapter_file) and os.path.exists(config_file):
                size_mb = os.path.getsize(adapter_file) / 1024**2
                print(f"   âœ… {os.path.basename(model_path)}: LoRA ì–´ëŒ‘í„° ({size_mb:.1f}MB)")
            else:
                print(f"   âš ï¸ {os.path.basename(model_path)}: ë¶ˆì™„ì „í•œ ëª¨ë¸")
        else:
            print(f"   âŒ {os.path.basename(model_path)}: ì—†ìŒ")
    
    # 6. ë©”ì¸ GGUF ëª¨ë¸ í™•ì¸
    print("\n6. ê¸°ë³¸ GGUF ëª¨ë¸:")
    model_path = "models/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M.gguf"
    if os.path.exists(model_path):
        size_gb = os.path.getsize(model_path) / 1024**3
        print(f"   âœ… {model_path} ({size_gb:.1f}GB)")
    else:
        print(f"   âŒ {model_path} ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    # 6. íŒŒì¸íŠœë‹ ë°ì´í„° í™•ì¸
    print("\n6. íŒŒì¸íŠœë‹ ë°ì´í„°:")
    data_files = [
        "finetuning/datasets/processed_english/unified_train.jsonl",
        "finetuning/datasets/processed_english/unified_validation.jsonl"
    ]
    
    for file_path in data_files:
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = sum(1 for line in f if line.strip() and not line.startswith('#'))
            print(f"   âœ… {os.path.basename(file_path)}: {lines}ê°œ ë°ì´í„°")
        else:
            print(f"   âŒ {file_path} ì—†ìŒ")
    
    print("\n=== RTX 3070 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ===")
    
    # PyTorch CUDA ì—¬ë¶€ì— ë”°ë¥¸ ê¶Œìž¥ì‚¬í•­
    try:
        import torch
        if hasattr(torch, 'cuda') and torch.cuda.is_available():
            print("ðŸš€ GPU íŒŒì¸íŠœë‹ ê¶Œìž¥:")
            print("   - ëª¨ë¸: NousResearch/Nous-Hermes-2-Mistral-7B-DPO")
            print("   - ë°©ë²•: LoRA + 4bit ì–‘ìží™”")
            print("   - ì˜ˆìƒ ì‹œê°„: 30ë¶„-1ì‹œê°„")
            print("   - GPU ë©”ëª¨ë¦¬: 6-8GB ì‚¬ìš©")
            
            print("\nðŸ”§ RTX 3070 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸:")
            try:
                # ê°„ë‹¨í•œ RTX 3070 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
                from transformers import AutoTokenizer
                
                rtx_path = "./models/rtx3070_optimized_best"
                if os.path.exists(rtx_path):
                    print("   ï¿½ ëª¨ë¸ í´ë” í™•ì¸ë¨")
                    
                    try:
                        tokenizer = AutoTokenizer.from_pretrained(rtx_path, local_files_only=True)
                        print("   âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
                    except Exception as e:
                        print(f"   âš ï¸ í† í¬ë‚˜ì´ì € ë¡œë”© ì‹¤íŒ¨: {e}")
                        print("   ðŸ’¡ ê¸°ë³¸ ëª¨ë¸ì—ì„œ í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìžˆìŠµë‹ˆë‹¤")
                    
                    # ì–´ëŒ‘í„° íŒŒì¼ í™•ì¸
                    adapter_file = os.path.join(rtx_path, "adapter_model.safetensors")
                    if os.path.exists(adapter_file):
                        print("   âœ… LoRA ì–´ëŒ‘í„° íŒŒì¼ í™•ì¸ë¨")
                    else:
                        print("   âŒ LoRA ì–´ëŒ‘í„° íŒŒì¼ ì—†ìŒ")
                        
                else:
                    print("   âŒ RTX 3070 ëª¨ë¸ í´ë” ì—†ìŒ")
                    
            except ImportError:
                print("   âŒ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ìŒ")
                
        else:
            print("ï¿½ðŸ’» CPU íŒŒì¸íŠœë‹:")
            print("   - ëª¨ë¸: microsoft/DialoGPT-small")
            print("   - ë°©ë²•: ì „ì²´ íŒŒì¸íŠœë‹")
            print("   - ì˜ˆìƒ ì‹œê°„: 2-4ì‹œê°„")
            print("   - RAM: 8-16GB ì‚¬ìš©")
    except:
        pass

def test_rtx3070_model():
    """RTX 3070 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\n=== RTX 3070 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ===")
    
    try:
        import torch
        if not torch.cuda.is_available():
            print("âŒ CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤")
            return False
            
        print("ðŸ”„ RTX 3070 ëª¨ë¸ ë¡œë”© ì‹œë„ ì¤‘...")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        
        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
        from peft import PeftModel
        import gc
        
        gc.collect()
        
        # ëª¨ë¸ ê²½ë¡œ
        model_path = "./models/rtx3070_optimized_best"
        if not os.path.exists(model_path):
            print(f"âŒ ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {model_path}")
            return False
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
        except:
            tokenizer = AutoTokenizer.from_pretrained("NousResearch/Nous-Hermes-2-Mistral-7B-DPO")
            print("âœ… ê¸°ë³¸ í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
            
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # 4bit ì–‘ìží™” ì„¤ì •
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=False,
        )
        
        # ë©”ëª¨ë¦¬ ì œí•œ
        max_memory = {0: "7GB"}
        
        try:
            # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
            print("ðŸ”„ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘...")
            base = AutoModelForCausalLM.from_pretrained(
                "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
                quantization_config=bnb_config,
                torch_dtype=torch.float16,
                device_map="auto",
                max_memory=max_memory,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )
            print("âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
            
            # LoRA ì–´ëŒ‘í„° ë¡œë“œ
            print("ðŸ”„ LoRA ì–´ëŒ‘í„° ë¡œë”© ì¤‘...")
            model = PeftModel.from_pretrained(base, model_path)
            model.eval()
            print("âœ… RTX 3070 ëª¨ë¸ ë¡œë“œ ì™„ì „ ì„±ê³µ!")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
            test_prompt = "Hello, how are you?"
            inputs = tokenizer.encode(test_prompt, return_tensors="pt").to("cuda")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs,
                    max_new_tokens=20,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"ðŸ§ª í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {response}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Transformers ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            
            # GGUF í´ë°± í…ŒìŠ¤íŠ¸
            print("ðŸ”„ GGUF ëª¨ë¸ í´ë°± í…ŒìŠ¤íŠ¸...")
            try:
                from llama_cpp import Llama
                gguf_path = "./models/Nous-Hermes-2-Mistral-7B-DPO.Q5_K_M.gguf"
                
                if os.path.exists(gguf_path):
                    model = Llama(
                        model_path=gguf_path,
                        n_gpu_layers=-1,
                        n_ctx=1024,
                        verbose=False
                    )
                    
                    response = model("Hello", max_tokens=10)
                    print("âœ… GGUF ëª¨ë¸ í´ë°± ì„±ê³µ!")
                    print(f"ðŸ§ª í…ŒìŠ¤íŠ¸ ì‘ë‹µ: {response['choices'][0]['text']}")
                    return True
                else:
                    print("âŒ GGUF ëª¨ë¸ íŒŒì¼ ì—†ìŒ")
                    return False
                    
            except Exception as gguf_e:
                print(f"âŒ GGUF ëª¨ë¸ë„ ì‹¤íŒ¨: {gguf_e}")
                return False
        
    except Exception as e:
        print(f"âŒ ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    check_environment()
    
    # RTX 3070 ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì˜µì…˜
    print("\n" + "="*50)
    test_choice = input("RTX 3070 ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
    
    if test_choice in ['y', 'yes']:
        success = test_rtx3070_model()
        if success:
            print("\nðŸŽ‰ RTX 3070 ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ìž‘ë™í•©ë‹ˆë‹¤!")
        else:
            print("\nâš ï¸ RTX 3070 ëª¨ë¸ ë¡œë”©ì— ë¬¸ì œê°€ ìžˆìŠµë‹ˆë‹¤.")
            print("   í•´ê²° ë°©ì•ˆ:")
            print("   1. GPU ë©”ëª¨ë¦¬ í™•ì¸ (ë‹¤ë¥¸ í”„ë¡œê·¸ëž¨ ì¢…ë£Œ)")
            print("   2. CUDA ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸")
            print("   3. ê°€ìƒí™˜ê²½ íŒ¨í‚¤ì§€ ìž¬ì„¤ì¹˜")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--test":
        test_rtx3070_model()
    else:
        main()
