#!/usr/bin/env python3
"""
ì˜ì–´ ëŒ€í™” ë°ì´í„° ê¸°ë°˜ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
ê¸°ì¡´ simple_korean_finetuning.pyë¥¼ ì˜ì–´ ë°ì´í„°ë¡œ ìˆ˜ì •
"""

import torch
import os
import json
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset, load_dataset
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
import argparse

def setup_environment():
    """í™˜ê²½ ì„¤ì • ë° í™•ì¸"""
    print("ğŸš€ ì˜ì–´ ëŒ€í™” íŒŒì¸íŠœë‹ í™˜ê²½ ì„¤ì •")
    print(f"PyTorch: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA version: {torch.version.cuda}")
        torch.cuda.empty_cache()
    else:
        print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

def load_model_and_tokenizer(model_name="microsoft/DialoGPT-medium"):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    ì˜ì–´ ëŒ€í™”ì— íŠ¹í™”ëœ DialoGPT ì‚¬ìš©
    """
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
        trust_remote_code=True
    )
    
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ - íŒŒë¼ë¯¸í„°: {model.num_parameters()}")
    return model, tokenizer

def load_english_data():
    """ì˜ì–´ ëŒ€í™” ë°ì´í„° ë¡œë“œ"""
    print("ğŸ“š ì˜ì–´ ë°ì´í„°ì…‹ ë¡œë”©...")
    
    # ì˜ì–´ ëŒ€í™” ë°ì´í„° ì¤€ë¹„
    data_files = [
        "./datasets/processed_english/unified_train.jsonl",
        "./datasets/processed_english/dialogue_qna_train.jsonl",
        "./datasets/processed_english/dialogue_general_train.jsonl"
    ]
    
    conversations = []
    
    for file_path in data_files:
        if os.path.exists(file_path):
            print(f"ğŸ“‚ ë¡œë”©: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith('#'):
                        try:
                            item = json.loads(line)
                            if 'input_text' in item and 'target_text' in item:
                                # ì˜ì–´ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í¬ë§·
                                text = f"User: {item['input_text']}<|endoftext|>Assistant: {item['target_text']}<|endoftext|>"
                                conversations.append({"text": text})
                        except json.JSONDecodeError:
                            continue
    
    print(f"âœ… ì´ {len(conversations)}ê°œ ëŒ€í™” ë¡œë“œë¨")
    
    if len(conversations) == 0:
        print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì˜ì–´ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        conversations = [
            {"text": "User: What is artificial intelligence?<|endoftext|>Assistant: Artificial Intelligence (AI) is a branch of computer science that aims to create intelligent machines capable of performing tasks that typically require human intelligence.<|endoftext|>"},
            {"text": "User: How does machine learning work?<|endoftext|>Assistant: Machine learning is a subset of AI that enables computers to learn and improve from experience without being explicitly programmed for every task.<|endoftext|>"},
            {"text": "User: Explain deep learning.<|endoftext|>Assistant: Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to model and understand complex patterns in data.<|endoftext|>"},
            {"text": "User: What is the difference between AI and ML?<|endoftext|>Assistant: AI is the broader concept of creating intelligent machines, while ML is a specific approach within AI that focuses on learning from data.<|endoftext|>"},
            {"text": "User: How can I start learning programming?<|endoftext|>Assistant: Start with a beginner-friendly language like Python, practice regularly with small projects, and use online resources like tutorials and coding platforms.<|endoftext|>"}
        ]
    
    return Dataset.from_list(conversations)

def setup_lora_config():
    """LoRA ì„¤ì •"""
    return LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,  # ë” ë†’ì€ rankë¡œ ì„±ëŠ¥ í–¥ìƒ
        lora_alpha=32,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj"]  # DialoGPTì— ë§ëŠ” íƒ€ê²Ÿ ëª¨ë“ˆ
    )

def tokenize_function(examples, tokenizer, max_length=512):
    """í† í°í™” í•¨ìˆ˜"""
    return tokenizer(
        examples["text"],
        truncation=True,
        padding=True,
        max_length=max_length,
        return_tensors="pt"
    )

def main():
    print("=" * 60)
    print("ğŸš€ ì˜ì–´ ëŒ€í™” íŒŒì¸íŠœë‹ ì‹œì‘")
    print("=" * 60)
    
    # í™˜ê²½ ì„¤ì •
    setup_environment()
    
    # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer()
    
    # LoRA ì„¤ì • ì ìš©
    print("ğŸ”§ LoRA ì„¤ì • ì ìš©...")
    peft_config = setup_lora_config()
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # ë°ì´í„° ë¡œë“œ ë° í† í°í™”
    dataset = load_english_data()
    tokenized_dataset = dataset.map(
        lambda x: tokenize_function(x, tokenizer),
        batched=True
    )
    
    # í›ˆë ¨ ì„¤ì •
    training_args = TrainingArguments(
        output_dir="./models/english_unified",
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=2 if torch.cuda.is_available() else 1,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        logging_steps=10,
        save_steps=500,
        save_total_limit=2,
        prediction_loss_only=True,
        fp16=torch.cuda.is_available(),
        dataloader_pin_memory=False,  # ë©”ëª¨ë¦¬ ë¬¸ì œ ë°©ì§€
        remove_unused_columns=False,
    )
    
    # ë°ì´í„° ì»¬ë ‰í„°
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    print("ğŸ¯ íŒŒì¸íŠœë‹ ì‹œì‘...")
    try:
        trainer.train()
        print("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")
        
        # ëª¨ë¸ ì €ì¥
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        trainer.save_model()
        tokenizer.save_pretrained("./models/english_unified")
        
        # í…ŒìŠ¤íŠ¸
        print("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
        test_prompts = [
            "What is machine learning?",
            "How do neural networks work?",
            "Explain artificial intelligence."
        ]
        
        model.eval()
        for prompt in test_prompts:
            print(f"\nì…ë ¥: {prompt}")
            input_text = f"User: {prompt}<|endoftext|>Assistant:"
            inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=tokenizer.pad_token_id
                )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            assistant_response = response.split("Assistant:")[-1].strip()
            print(f"ì‘ë‹µ: {assistant_response}")
        
        print("\nğŸ‰ ì˜ì–´ ëŒ€í™” íŒŒì¸íŠœë‹ ì™„ë£Œ!")
        print("ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: ./models/english_unified")
        
    except Exception as e:
        print(f"âŒ íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {e}")
        return False
    
    return True

if __name__ == "__main__":
    main()
