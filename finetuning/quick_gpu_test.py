#!/usr/bin/env python3
"""
GPU í™˜ê²½ ë¹ ë¥¸ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
import time

def test_gpu_environment():
    """GPU í™˜ê²½ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ” GPU í™˜ê²½ ê²€ì¦")
    print("=" * 40)
    
    # ê¸°ë³¸ ì •ë³´
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤!")
        return False
    
    # GPU ì •ë³´
    print(f"CUDA version: {torch.version.cuda}")
    print(f"GPU count: {torch.cuda.device_count()}")
    print(f"Current GPU: {torch.cuda.get_device_name(0)}")
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    total_memory = torch.cuda.get_device_properties(0).total_memory
    print(f"Total GPU memory: {total_memory / 1e9:.1f} GB")
    
    # GPU ì—°ì‚° í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª GPU ì—°ì‚° í…ŒìŠ¤íŠ¸")
    device = torch.device("cuda:0")
    
    # ê°„ë‹¨í•œ í…ì„œ ì—°ì‚°
    start_time = time.time()
    x = torch.randn(1000, 1000, device=device)
    y = torch.randn(1000, 1000, device=device)
    z = torch.matmul(x, y)
    torch.cuda.synchronize()
    end_time = time.time()
    
    print(f"âœ… ë§¤íŠ¸ë¦­ìŠ¤ ê³±ì…ˆ (1000x1000) ì†Œìš”ì‹œê°„: {end_time - start_time:.4f}ì´ˆ")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    allocated = torch.cuda.memory_allocated(0)
    reserved = torch.cuda.memory_reserved(0)
    print(f"GPU ë©”ëª¨ë¦¬ - í• ë‹¹: {allocated / 1e6:.1f} MB, ì˜ˆì•½: {reserved / 1e6:.1f} MB")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del x, y, z
    torch.cuda.empty_cache()
    print("âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
    
    return True

def test_transformers_import():
    """Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ“š Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸")
    
    try:
        from transformers import AutoTokenizer
        print("âœ… AutoTokenizer ì„í¬íŠ¸ ì„±ê³µ")
        
        from transformers import AutoModelForCausalLM
        print("âœ… AutoModelForCausalLM ì„í¬íŠ¸ ì„±ê³µ")
        
        from peft import LoraConfig
        print("âœ… PEFT (LoRA) ì„í¬íŠ¸ ì„±ê³µ")
        
        return True
    except Exception as e:
        print(f"âŒ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_basic_model_loading():
    """ê°„ë‹¨í•œ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¤– ê¸°ë³¸ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    
    try:
        # ê°€ì¥ ì‘ì€ GPT-2 ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        from transformers import GPT2LMHeadModel, GPT2Tokenizer
        
        model_name = "gpt2"
        print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        
        tokenizer = GPT2Tokenizer.from_pretrained(model_name)
        model = GPT2LMHeadModel.from_pretrained(model_name)
        
        if torch.cuda.is_available():
            model = model.to("cuda")
            print("âœ… ëª¨ë¸ì„ GPUë¡œ ì´ë™ ì™„ë£Œ")
        
        # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        inputs = tokenizer("Hello", return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=10, do_sample=False)
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… ìƒì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        del model, tokenizer
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ GPU íŒŒì¸íŠœë‹ í™˜ê²½ ë¹ ë¥¸ ê²€ì¦")
    print("=" * 60)
    
    # 1. GPU í™˜ê²½ í…ŒìŠ¤íŠ¸
    gpu_ok = test_gpu_environment()
    
    # 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ í…ŒìŠ¤íŠ¸
    import_ok = test_transformers_import()
    
    # 3. ê¸°ë³¸ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    model_ok = test_basic_model_loading()
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print(f"GPU í™˜ê²½: {'âœ… ì„±ê³µ' if gpu_ok else 'âŒ ì‹¤íŒ¨'}")
    print(f"ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸: {'âœ… ì„±ê³µ' if import_ok else 'âŒ ì‹¤íŒ¨'}")
    print(f"ëª¨ë¸ ë¡œë”©: {'âœ… ì„±ê³µ' if model_ok else 'âŒ ì‹¤íŒ¨'}")
    
    if all([gpu_ok, import_ok, model_ok]):
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
        print("âœ… GPU ê¸°ë°˜ íŒŒì¸íŠœë‹ í™˜ê²½ì´ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì´ì œ ì‹¤ì œ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print("ğŸ”§ í™˜ê²½ ì„¤ì •ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
