#!/usr/bin/env python3
"""
ë©”ì¸ í”„ë¡œê·¸ë¨ê³¼ í˜¸í™˜ë˜ëŠ” Nous-Hermes-2-Mistral íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
GPU í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ìµœì í™”
"""

import os
import json
import torch
from pathlib import Path
import logging

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (TensorFlow ë¬¸ì œ í•´ê²°)
os.environ['USE_TF'] = 'NO'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'

try:
    from transformers import (
        AutoTokenizer, 
        AutoModelForCausalLM,
        TrainingArguments, 
        Trainer,
        DataCollatorForLanguageModeling,
        BitsAndBytesConfig
    )
    from datasets import Dataset
    from peft import LoraConfig, get_peft_model, TaskType
    TRANSFORMERS_AVAILABLE = True
except ImportError as e:
    print(f"Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¬¸ì œ: {e}")
    TRANSFORMERS_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_gpu_status():
    """GPU ìƒíƒœ í™•ì¸ ë° ì„¤ì •"""
    logger.info("=== GPU ìƒíƒœ í™•ì¸ ===")
    
    # CUDA í™•ì¸
    if hasattr(torch, 'cuda') and torch.cuda.is_available():
        device = torch.device("cuda:0")
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {gpu_name}")
        logger.info(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        return device, True
    else:
        logger.warning("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        return torch.device("cpu"), False

def install_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜"""
    logger.info("=== í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸ ë° ì„¤ì¹˜ ===")
    
    packages_to_install = []
    
    # PyTorch CUDA ë²„ì „ í™•ì¸
    if not hasattr(torch, 'cuda') or not torch.cuda.is_available():
        logger.warning("PyTorch CUDA ë²„ì „ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        print("\nğŸ”§ PyTorch CUDA ì„¤ì¹˜ ëª…ë ¹ì–´:")
        print("pip uninstall torch torchvision torchaudio")
        print("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return False
    
    # Transformers í™•ì¸
    if not TRANSFORMERS_AVAILABLE:
        packages_to_install.append("transformers")
    
    # ê¸°íƒ€ í•„ìš” íŒ¨í‚¤ì§€
    try:
        import datasets
    except ImportError:
        packages_to_install.append("datasets")
    
    try:
        import peft
    except ImportError:
        packages_to_install.append("peft")
    
    try:
        import bitsandbytes
    except ImportError:
        packages_to_install.append("bitsandbytes")
    
    if packages_to_install:
        logger.info(f"ì„¤ì¹˜ í•„ìš”í•œ íŒ¨í‚¤ì§€: {packages_to_install}")
        install_cmd = f"pip install {' '.join(packages_to_install)}"
        print(f"\nğŸ”§ ì„¤ì¹˜ ëª…ë ¹ì–´: {install_cmd}")
        return False
    
    return True

def load_unified_data(data_dir):
    """í†µí•© ë°ì´í„° ë¡œë“œ (ë©”ì¸ í”„ë¡œê·¸ë¨ í˜¸í™˜)"""
    logger.info("í†µí•© ë°ì´í„° ë¡œë”© ì¤‘...")
    
    train_data = []
    val_data = []
    
    # unified_train.jsonl ë¡œë“œ
    train_file = Path(data_dir) / "unified_train.jsonl"
    with open(train_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                try:
                    item = json.loads(line)
                    # Nous-Hermes í˜•ì‹ì— ë§ê²Œ ë³€í™˜
                    conversation = f"<|im_start|>system\nYou are a helpful AI assistant.<|im_end|>\n<|im_start|>user\n{item['input_text'].split('User: ')[-1].split('\\nAssistant:')[0]}<|im_end|>\n<|im_start|>assistant\n{item['target_text']}<|im_end|>"
                    train_data.append({"text": conversation})
                except json.JSONDecodeError:
                    continue
    
    # unified_validation.jsonl ë¡œë“œ
    val_file = Path(data_dir) / "unified_validation.jsonl" 
    with open(val_file, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                try:
                    item = json.loads(line)
                    conversation = f"<|im_start|>system\nYou are a helpful AI assistant.<|im_end|>\n<|im_start|>user\n{item['input_text'].split('User: ')[-1].split('\\nAssistant:')[0]}<|im_end|>\n<|im_start|>assistant\n{item['target_text']}<|im_end|>"
                    val_data.append({"text": conversation})
                except json.JSONDecodeError:
                    continue
    
    logger.info(f"í•™ìŠµ ë°ì´í„°: {len(train_data)}ê°œ")
    logger.info(f"ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ")
    
    return Dataset.from_list(train_data), Dataset.from_list(val_data)

def main():
    """ë©”ì¸ íŒŒì¸íŠœë‹ í•¨ìˆ˜"""
    
    logger.info("=== Nous-Hermes-2-Mistral íŒŒì¸íŠœë‹ ì‹œì‘ ===")
    
    # í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
    if not install_requirements():
        logger.error("í•„ìˆ˜ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìœ„ì˜ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
        return
    
    # GPU ìƒíƒœ í™•ì¸
    device, use_gpu = check_gpu_status()
    
    if not TRANSFORMERS_AVAILABLE:
        logger.error("Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë©”ì¸ í”„ë¡œê·¸ë¨ê³¼ ë™ì¼í•œ ëª¨ë¸ ì‚¬ìš©
    model_name = "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"
    logger.info(f"ëª¨ë¸ ë¡œë”©: {model_name}")
    
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # íŠ¹ìˆ˜ í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ì–‘ìí™” ì„¤ì • (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
        if use_gpu:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
            
            # ëª¨ë¸ ë¡œë“œ (4bit ì–‘ìí™”)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True
            )
        else:
            # CPU ëª¨ë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float32,
                trust_remote_code=True
            )
        
        logger.info("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        
        # LoRA ì„¤ì • (íš¨ìœ¨ì ì¸ íŒŒì¸íŠœë‹)
        if use_gpu:
            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=16,
                lora_alpha=32,
                target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
                lora_dropout=0.1,
            )
            
            model = get_peft_model(model, lora_config)
            logger.info("LoRA ì„¤ì • ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        # error.txtì— ê¸°ë¡
        with open("../.github/prompts/error.txt", "a", encoding="utf-8") as f:
            f.write(f"\nëª¨ë¸ ë¡œë”© ì˜¤ë¥˜: {e}\n")
            f.write("ê°œì„  ì§€ì¹¨:\n")
            f.write("1. HuggingFace í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ\n")
            f.write("2. ëª¨ë¸ í¬ê¸°ê°€ í´ ìˆ˜ ìˆìœ¼ë‹ˆ ë” ì‘ì€ ëª¨ë¸ ì‹œë„\n")
            f.write("3. ì–‘ìí™” ì„¤ì • ì¡°ì •\n")
        return
    
    # ë°ì´í„° ë¡œë“œ
    try:
        data_dir = "datasets/processed_english"
        train_dataset, val_dataset = load_unified_data(data_dir)
        
        # í† í¬ë‚˜ì´ì œì´ì…˜
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=512,
                return_tensors="pt"
            )
        
        logger.info("ë°ì´í„° í† í¬ë‚˜ì´ì œì´ì…˜ ì¤‘...")
        train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        logger.info("í† í¬ë‚˜ì´ì œì´ì…˜ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return
    
    # í•™ìŠµ ì„¤ì •
    if use_gpu:
        training_args = TrainingArguments(
            output_dir="logs/checkpoints/nous_hermes_finetuned",
            overwrite_output_dir=True,
            num_train_epochs=2,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=8,
            gradient_accumulation_steps=2,
            warmup_steps=100,
            learning_rate=2e-4,
            logging_steps=10,
            eval_steps=50,
            save_steps=100,
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=None,
            dataloader_pin_memory=False,
            fp16=True,
            gradient_checkpointing=True,
            remove_unused_columns=False,
        )
    else:
        training_args = TrainingArguments(
            output_dir="logs/checkpoints/nous_hermes_finetuned",
            overwrite_output_dir=True,
            num_train_epochs=1,
            per_device_train_batch_size=1,
            per_device_eval_batch_size=2,
            gradient_accumulation_steps=8,
            warmup_steps=50,
            learning_rate=5e-5,
            logging_steps=5,
            eval_steps=20,
            save_steps=50,
            evaluation_strategy="steps",
            save_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to=None,
            dataloader_pin_memory=False,
            fp16=False,
            gradient_checkpointing=True,
            remove_unused_columns=False,
            no_cuda=True,
        )
    
    # ë°ì´í„° ì½œë ˆì´í„°
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    logger.info("íŒŒì¸íŠœë‹ ì‹œì‘...")
    try:
        trainer.train()
        logger.info("íŒŒì¸íŠœë‹ ì™„ë£Œ!")
        
        # ëª¨ë¸ ì €ì¥
        logger.info("ëª¨ë¸ ì €ì¥ ì¤‘...")
        if use_gpu:
            # LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            model.save_pretrained("models/nous_hermes_finetuned_lora")
        else:
            trainer.save_model("models/nous_hermes_finetuned")
        
        tokenizer.save_pretrained("models/nous_hermes_finetuned")
        logger.info("ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")
        
        # í‰ê°€
        logger.info("ëª¨ë¸ í‰ê°€ ì¤‘...")
        eval_results = trainer.evaluate()
        logger.info(f"í‰ê°€ ê²°ê³¼: {eval_results}")
        
        # ë©”ì¸ í”„ë¡œê·¸ë¨ ì—°ë™ì„ ìœ„í•œ ì •ë³´ ì €ì¥
        model_info = {
            "model_name": model_name,
            "finetuned_path": "models/nous_hermes_finetuned",
            "use_lora": use_gpu,
            "eval_loss": eval_results.get("eval_loss", 0),
            "training_completed": True
        }
        
        with open("models/finetuning_info.json", "w", encoding="utf-8") as f:
            json.dump(model_info, f, indent=2, ensure_ascii=False)
        
        logger.info("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ë©”ì¸ í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"íŒŒì¸íŠœë‹ ì‹¤íŒ¨: {e}")
        with open("../.github/prompts/error.txt", "a", encoding="utf-8") as f:
            f.write(f"\níŒŒì¸íŠœë‹ ì‹¤í–‰ ì˜¤ë¥˜: {e}\n")
            f.write("ê°œì„  ì§€ì¹¨:\n")
            f.write("1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°\n")
            f.write("2. LoRA rank ê°’ ì¤„ì´ê¸°\n")
            f.write("3. ì–‘ìí™” ë¹„íŠ¸ ìˆ˜ ëŠ˜ë¦¬ê¸° (8bit)\n")
            f.write("4. ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ… í™œì„±í™”\n")

if __name__ == "__main__":
    main()
