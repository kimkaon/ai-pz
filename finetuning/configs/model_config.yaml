# 모델 아키텍처 및 설정

# 대화 분류기 모델 설정
conversation_classifier:
  architecture: "transformer_classification"
  input_format: "text"
  output_format: "label"
  
  model_config:
    hidden_size: 768
    num_attention_heads: 12
    num_hidden_layers: 6  # 경량화를 위해 층 수 감소
    intermediate_size: 3072
    dropout: 0.1
    attention_dropout: 0.1
    
  classification_head:
    num_labels: 4
    label_names: ["qna", "technical", "general"]  # daily_chat은 통합모델에서 처리
    activation: "gelu"
    
  training_objective: "cross_entropy"

# QnA 전문 모델 설정
qna_specialist:
  architecture: "transformer_generation"
  input_format: "question"
  output_format: "answer"
  
  model_config:
    hidden_size: 768
    num_attention_heads: 12
    num_hidden_layers: 12
    intermediate_size: 3072
    vocab_size: 50257
    max_position_embeddings: 1024
    
  generation_config:
    max_length: 256
    min_length: 10
    do_sample: true
    temperature: 0.7
    top_p: 0.9
    repetition_penalty: 1.1
    pad_token_id: 50256
    eos_token_id: 50256
    
  training_objective: "causal_lm"
  
  prompt_template: |
    Question: {question}
    Answer: {answer}

# 기술 상담 전문 모델 설정
technical_specialist:
  architecture: "transformer_generation"
  input_format: "technical_query"
  output_format: "technical_response"
  
  model_config:
    hidden_size: 768
    num_attention_heads: 12
    num_hidden_layers: 12
    intermediate_size: 3072
    vocab_size: 50257
    max_position_embeddings: 1024
    
  generation_config:
    max_length: 300
    min_length: 20
    do_sample: true
    temperature: 0.6
    top_p: 0.85
    repetition_penalty: 1.15
    
  training_objective: "causal_lm"
  
  prompt_template: |
    Technical Query: {query}
    Program: {program}
    Solution: {solution}

# 공통 설정
common:
  tokenizer_settings:
    padding_side: "left"
    truncation: true
    add_special_tokens: true
    return_tensors: "pt"
    
  optimizer:
    type: "AdamW"
    lr: 5e-5
    weight_decay: 0.01
    eps: 1e-8
    
  scheduler:
    type: "linear_warmup"
    warmup_ratio: 0.1
    
  hardware:
    gpu_memory_limit: 7.5  # RTX 3070 8GB의 여유분 고려
    gradient_checkpointing: true
    dataloader_num_workers: 2
