#!/usr/bin/env python3
"""
Nous-Hermes-2-Mistral-7B-DPO ì˜ì–´ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
- processed_english ë°ì´í„°ì…‹ ê¸°ë°˜
- LoRAë¥¼ ì´ìš©í•œ íš¨ìœ¨ì  íŒŒì¸íŠœë‹
"""

import os
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    get_linear_schedule_with_warmup,
    get_cosine_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, TaskType
from tqdm import tqdm
import warnings
import gc

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

def clear_memory():
    """GPU ë©”ëª¨ë¦¬ ì •ë¦¬"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

class EnglishChatDataset(Dataset):
    """ì˜ì–´ ëŒ€í™” ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, data_files, tokenizer, max_length=128):  # ê¸¸ì´ ê°ì†Œ (ë©”ëª¨ë¦¬ ì ˆì•½)
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        # ì—¬ëŸ¬ JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        for file_path in data_files:
            if os.path.exists(file_path):
                print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {file_path}")
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            try:
                                item = json.loads(line)
                                if 'input_text' in item and 'target_text' in item:
                                    self.data.append(item)
                            except json.JSONDecodeError:
                                continue
        
        print(f"âœ… ì´ {len(self.data)}ê°œ ë°ì´í„° ë¡œë“œë¨")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ê³ í’ˆì§ˆ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ ì „ì²˜ë¦¬
        input_text = item['input_text'].strip()
        target_text = item['target_text'].strip()
        
        # ë„ˆë¬´ ê¸´ ì‘ë‹µì€ 3-4ë¬¸ì¥ìœ¼ë¡œ ì œí•œ (í’ˆì§ˆ ìœ ì§€í•˜ë©´ì„œ ê°„ê²°í™”)
        sentences = target_text.split('. ')
        if len(sentences) > 4:
            target_text = '. '.join(sentences[:4]) + '.'
        
        # ê³ í’ˆì§ˆ Mistral/Nous-Hermes í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        text = f"<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n{target_text}<|im_end|>"
        
        # í† í°í™”
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # ë¼ë²¨ ë§ˆìŠ¤í‚¹: ì‚¬ìš©ì ì…ë ¥ ë¶€ë¶„ì€ -100ìœ¼ë¡œ ë§ˆìŠ¤í‚¹ (ê³ í’ˆì§ˆ í•™ìŠµ)
        labels = encoding['input_ids'].clone()
        
        # ê°„ë‹¨í•˜ê³  ì•ˆì „í•œ ë§ˆìŠ¤í‚¹ ë°©ì‹
        text_parts = text.split("<|im_start|>assistant\n")
        if len(text_parts) > 1:
            user_part = text_parts[0] + "<|im_start|>assistant\n"
            user_tokens = self.tokenizer.encode(user_part, add_special_tokens=False)
            
            # ì•ˆì „í•œ ë§ˆìŠ¤í‚¹
            mask_length = min(len(user_tokens), labels.shape[-1])
            labels[0][:mask_length] = -100
        else:
            # ëŒ€ì•ˆ: ì „ì²´ ì‹œí€€ìŠ¤ì˜ ì ˆë°˜ë§Œ ë§ˆìŠ¤í‚¹
            mask_length = labels.shape[-1] // 2
            labels[0][:mask_length] = -100
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': labels.flatten()
        }

def load_english_datasets():
    """ì˜ì–´ ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ì— ê´€ê³„ì—†ì´ ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "datasets", "processed_english")
    
    print(f"ğŸ“ ë°ì´í„°ì…‹ ê¸°ë³¸ ê²½ë¡œ: {base_path}")
    
    # ëª¨ë“  ì˜ì–´ í›ˆë ¨ ë°ì´í„° íŒŒì¼ í¬í•¨
    train_files = [
        os.path.join(base_path, "unified_train.jsonl"),
        os.path.join(base_path, "dialogue_qna_train.jsonl"),
        os.path.join(base_path, "dialogue_general_train.jsonl"),
        os.path.join(base_path, "dialogue_technical_train.jsonl"),
        os.path.join(base_path, "classification_train.jsonl")
    ]
    
    # ëª¨ë“  ì˜ì–´ ê²€ì¦ ë°ì´í„° íŒŒì¼ í¬í•¨
    val_files = [
        os.path.join(base_path, "unified_validation.jsonl"),
        os.path.join(base_path, "dialogue_qna_validation.jsonl"),
        os.path.join(base_path, "dialogue_general_validation.jsonl"),
        os.path.join(base_path, "dialogue_technical_validation.jsonl"),
        os.path.join(base_path, "classification_validation.jsonl")
    ]
    
    return train_files, val_files

def main():
    print("ğŸš€ Nous-Hermes-2-Mistral-7B-DPO ì˜ì–´ íŒŒì¸íŠœë‹ ì‹œì‘")
    print("="*60)
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_memory()
    
    # GPU í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        print(f"ğŸ§¹ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì ìš©ë¨")
    
    # ì›ë³¸ Nous-Hermes-2-Mistral-7B-DPO ëª¨ë¸ ì‚¬ìš© (ìµœì í™” ì ìš©)
    model_name = "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"
    print(f"ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ë¡œë”©: {model_name}")
    print("âš¡ ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì • ì ìš©")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        
        # RTX 3070 8GB ìµœì í™”: 4bit ì–‘ìí™”
        from transformers import BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}")
        
        # RTX 3070 8GB ìµœì í™”ëœ LoRA ì„¤ì •
        print("ğŸ”§ RTX 3070 ìµœì í™” LoRA ì„¤ì • ì ìš© ì¤‘...")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,  # ì ì ˆí•œ rank (í’ˆì§ˆê³¼ ë©”ëª¨ë¦¬ ê· í˜•)
            lora_alpha=16,  # 2*r (ì¼ë°˜ì  ì„¤ì •)
            lora_dropout=0.05,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # ì£¼ìš” ì–´í…ì…˜ ëª¨ë“ˆ
            bias="none"
        )
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
        # í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
        model.train()
        
        print("âœ… LoRA íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ")
        print("âœ… 4bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëŒ€í­ ê°ì†Œ")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  HuggingFace Hub ì ‘ê·¼ì´ ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    train_files, val_files = load_english_datasets()
    
    train_dataset = EnglishChatDataset(train_files, tokenizer)
    val_dataset = EnglishChatDataset(val_files, tokenizer)
    
    if len(train_dataset) == 0:
        print("âŒ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°ë¡œë” ìƒì„± - ì•ˆì „í•œ ì„¤ì •
    batch_size = 1  # ë°°ì¹˜ í¬ê¸° ê°ì†Œ (ì•ˆì •ì„±)
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        pin_memory=False,  # ë©”ëª¨ë¦¬ ë¬¸ì œ ë°©ì§€
        drop_last=True  # ë¶ˆì™„ì „í•œ ë°°ì¹˜ ì œê±°
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        pin_memory=False,
        drop_last=False
    )
    
    # 5ì‹œê°„ ì—°ì† í•™ìŠµì„ ìœ„í•œ ìµœì í™”ëœ ì„¤ì •
    num_epochs = 5  # ì—í¬í¬ ì¦ê°€ (5ì‹œê°„ ì¶©ë¶„íˆ í™œìš©)
    learning_rate = 1e-4  # ì ì ˆí•œ í•™ìŠµë¥ 
    gradient_accumulation_steps = 4  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
    max_grad_norm = 1.0  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    
    # ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš© (ê³ í’ˆì§ˆ í•™ìŠµ)
    print(f"ğŸ¯ 5ì‹œê°„ ì—°ì† í•™ìŠµì„ ìœ„í•´ ì „ì²´ {len(train_dataset.data)}ê°œ ë°ì´í„° ì‚¬ìš©")
    
    # ì›œì—…ê³¼ ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ìœ„í•œ ê³ ê¸‰ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = AdamW(
        [p for p in model.parameters() if p.requires_grad], 
        lr=learning_rate,
        weight_decay=0.01,  # ê°€ì¤‘ì¹˜ ê°ì‡  ì¶”ê°€
        eps=1e-8
    )
    total_steps = len(train_loader) * num_epochs // gradient_accumulation_steps
    warmup_steps = total_steps // 20  # 5% ì›œì—…
    
    # ê³ ê¸‰ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    print(f"ğŸ¯ RTX 3070 8GB ìµœì í™” ì„¤ì • (5ì‹œê°„ ì—°ì†):")
    print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…: {gradient_accumulation_steps}")
    print(f"   íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°: {batch_size * gradient_accumulation_steps}")
    print(f"   ì—í¬í¬: {num_epochs}")
    print(f"   í•™ìŠµë¥ : {learning_rate}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    print(f"   ì›œì—… ìŠ¤í…: {warmup_steps}")
    print(f"   í›ˆë ¨ ë°ì´í„°: {len(train_dataset)} ìƒ˜í”Œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {len(val_dataset)} ìƒ˜í”Œ")
    print(f"   ğŸ’¡ 4bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~75% ê°ì†Œ")
    
    # ê³ í’ˆì§ˆ íŒŒì¸íŠœë‹ ì‹¤í–‰
    model.train()
    best_val_loss = float('inf')
    patience = 0
    max_patience = 2
    
    for epoch in range(num_epochs):
        print(f"\nğŸ“š ì—í¬í¬ {epoch + 1}/{num_epochs}")
        
        total_loss = 0
        valid_batches = 0
        progress_bar = tqdm(train_loader, desc=f"í›ˆë ¨ ì¤‘ (ì—í¬í¬ {epoch+1})")
        
        for step, batch in enumerate(progress_bar):
            
            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # ìˆœì „íŒŒ
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            
            # ì†ì‹¤ ìœ íš¨ì„± ê²€ì‚¬
            if loss is None or not torch.isfinite(loss):
                print("âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì†ì‹¤ê°’ì…ë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìœ„í•œ ì†ì‹¤ ì¡°ì •
            loss = loss / gradient_accumulation_steps
            total_loss += loss.item()
            valid_batches += 1
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë° ì—…ë°ì´íŠ¸
            if (step + 1) % gradient_accumulation_steps == 0:
                # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì•ˆì •ì  í›ˆë ¨)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                clear_memory()  # ë©”ëª¨ë¦¬ ì •ë¦¬
            
            # í˜„ì¬ í•™ìŠµë¥  í‘œì‹œ
            current_lr = scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                'loss': f"{loss.item() * gradient_accumulation_steps:.4f}",
                'lr': f"{current_lr:.2e}"
            })
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
            if step % 20 == 0:
                clear_memory()
        
        avg_loss = total_loss / max(valid_batches, 1)
        print(f"ğŸ“Š í‰ê·  í›ˆë ¨ ì†ì‹¤: {avg_loss:.4f}")
        print(f"ğŸ“ˆ í˜„ì¬ í•™ìŠµë¥ : {scheduler.get_last_lr()[0]:.2e}")
        
        # ê²€ì¦ (ì¡°ê¸° ì¢…ë£Œ í¬í•¨)
        if len(val_dataset) > 0:
            model.eval()
            val_loss = 0
            val_steps = 0
            
            with torch.no_grad():
                for batch in tqdm(val_loader, desc="ê²€ì¦ ì¤‘"):
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    labels = batch['labels'].to(device)
                    
                    outputs = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    
                    if outputs.loss is not None and torch.isfinite(outputs.loss):
                        val_loss += outputs.loss.item()
                        val_steps += 1
            
            avg_val_loss = val_loss / max(val_steps, 1)
            print(f"ğŸ” ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
            
            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                patience = 0
                print("âœ… ê²€ì¦ ì†ì‹¤ ê°œì„ ! ëª¨ë¸ ì €ì¥...")
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¤‘ê°„ ì €ì¥
                script_dir = os.path.dirname(os.path.abspath(__file__))
                temp_dir = os.path.join(script_dir, "models", "english_unified_best")
                os.makedirs(temp_dir, exist_ok=True)
                model.save_pretrained(temp_dir)
            else:
                patience += 1
                print(f"â³ ê²€ì¦ ì†ì‹¤ ë¯¸ê°œì„  ({patience}/{max_patience})")
                
            if patience >= max_patience:
                print("ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: ê²€ì¦ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•ŠìŒ")
                break
            
            model.train()
    
    # ëª¨ë¸ ì €ì¥
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "models", "english_unified")
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ’¾ LoRA ëª¨ë¸ ì €ì¥: {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # ì„¤ì • íŒŒì¼ ì €ì¥
    config_info = {
        "base_model": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
        "training_data": "processed_english datasets (full)",
        "model_type": "High-Quality LoRA fine-tuned",
        "epochs": num_epochs,
        "learning_rate": learning_rate,
        "batch_size": batch_size,
        "gradient_accumulation_steps": gradient_accumulation_steps,
        "lora_r": 16,
        "lora_alpha": 32,
        "max_length": 256,
        "training_time": "~5 hours",
        "scheduler": "cosine_with_warmup",
        "best_val_loss": best_val_loss
    }
    
    with open(f"{output_dir}/training_config.json", "w", encoding="utf-8") as f:
        json.dump(config_info, f, indent=2)
    
    # ê³ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ìƒì„±
    print(f"\nğŸ§ª ê³ í’ˆì§ˆ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    model.eval()
    
    # ë‹¤ì–‘í•œ ìœ í˜•ì˜ í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        "What is artificial intelligence?",
        "How does machine learning work?", 
        "Explain deep learning in simple terms.",
        "What are the main benefits of renewable energy?",
        "How do neural networks learn from data?",
        "What is the difference between AI and ML?",
        "Describe the concept of natural language processing.",
        "How does computer vision work?"
    ]
    
    with torch.no_grad():
        for i, prompt in enumerate(test_prompts):
            print(f"\nğŸ”¸ í…ŒìŠ¤íŠ¸ {i+1}: {prompt}")
            
            # Mistral/Nous-Hermes í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ í† í°í™”
            input_text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            inputs = tokenizer(input_text, return_tensors='pt').to(device)
            
            # ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„±
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,  # ì¶©ë¶„í•œ ê¸¸ì´
                do_sample=True,
                temperature=0.7,  # ì°½ì˜ì ì´ì§€ë§Œ ì¼ê´€ëœ ì‘ë‹µ
                top_p=0.9,  # ë‹¤ì–‘ì„±ê³¼ í’ˆì§ˆ ê· í˜•
                repetition_penalty=1.1,  # ë°˜ë³µ ë°©ì§€
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                early_stopping=True,
                no_repeat_ngram_size=3  # 3-gram ë°˜ë³µ ë°©ì§€
            )
            
            # ì‘ë‹µ ë””ì½”ë”©
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            assistant_response = response.split("<|im_start|>assistant\n")[-1].replace("<|im_end|>", "").strip()
            
            print(f"ğŸ’¬ ì‘ë‹µ: {assistant_response}")
            
            # ì‘ë‹µ í’ˆì§ˆ ê°„ë‹¨ ì²´í¬
            if len(assistant_response.split()) < 5:
                print("âš ï¸ ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")
            elif len(assistant_response) > 500:
                print("âš ï¸ ì‘ë‹µì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤.")
            else:
                print("âœ… ì ì ˆí•œ ê¸¸ì´ì˜ ì‘ë‹µ")
    
    print(f"\nğŸ‰ ê³ í’ˆì§ˆ Nous-Hermes-2-Mistral ì˜ì–´ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print(f"ğŸ“ ìµœì¢… ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print(f"ğŸ“ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ìœ„ì¹˜: {os.path.join(script_dir, 'models', 'english_unified_best')}")
    print(f"ğŸ† ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
    print("="*60)

if __name__ == "__main__":
    main()
