#!/usr/bin/env python3
"""
ì‹¤ìš©ì ì¸ GPU ê¸°ë°˜ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
Nous-Hermes-2-Mistralê³¼ í˜¸í™˜ë˜ëŠ” ëª¨ë¸ë¡œ íŒŒì¸íŠœë‹
"""

import torch
import os
import json
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset, load_dataset
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
import argparse

def setup_environment():
    """í™˜ê²½ ì„¤ì • ë° í™•ì¸"""
    print("ğŸš€ íŒŒì¸íŠœë‹ í™˜ê²½ ì„¤ì •")
    print(f"PyTorch: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"CUDA version: {torch.version.cuda}")
        torch.cuda.empty_cache()
    else:
        print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

def load_model_and_tokenizer(model_name="microsoft/DialoGPT-medium"):
    """
    ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    ë” ê°€ë²¼ìš´ DialoGPTë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ (Mistral ëŒ€ì‹ )
    """
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
        trust_remote_code=True
    )
    
    print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ - íŒŒë¼ë¯¸í„°: {model.num_parameters():,}")
    return model, tokenizer

def create_korean_dataset():
    """í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°ì…‹ ìƒì„±"""
    korean_conversations = [
        {
            "user": "ì•ˆë…•í•˜ì„¸ìš”!",
            "assistant": "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        },
        {
            "user": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
            "assistant": "ì£„ì†¡í•˜ì§€ë§Œ í˜„ì¬ ë‚ ì”¨ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚ ì”¨ ì•±ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
        },
        {
            "user": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "assistant": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•:\n1. sort() ë©”ì„œë“œ: my_list.sort() (ì›ë³¸ ìˆ˜ì •)\n2. sorted() í•¨ìˆ˜: new_list = sorted(my_list) (ìƒˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)"
        },
        {
            "user": "ê°ì‚¬í•©ë‹ˆë‹¤!",
            "assistant": "ì²œë§Œì—ìš”! ì–¸ì œë“ ì§€ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ë¬¼ì–´ë³´ì„¸ìš”."
        },
        {
            "user": "ì»´í“¨í„°ê°€ ëŠë ¤ìš”. ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?",
            "assistant": "ì»´í“¨í„° ì†ë„ ê°œì„  ë°©ë²•:\n1. ë¶ˆí•„ìš”í•œ í”„ë¡œê·¸ë¨ ì¢…ë£Œ\n2. ë””ìŠ¤í¬ ì •ë¦¬\n3. ë°±ê·¸ë¼ìš´ë“œ ì•± í™•ì¸\n4. ì¬ì‹œì‘\n5. í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ ê³ ë ¤"
        },
        {
            "user": "ì¢‹ì€ í•˜ë£¨ ë˜ì„¸ìš”!",
            "assistant": "ê°ì‚¬í•©ë‹ˆë‹¤! ì¢‹ì€ í•˜ë£¨ ë˜ì‹œê³ , ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ë§ì”€í•´ ì£¼ì„¸ìš”."
        }
    ]
    
    # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    formatted_data = []
    for conv in korean_conversations:
        text = f"ì‚¬ìš©ì: {conv['user']}\nì–´ì‹œìŠ¤í„´íŠ¸: {conv['assistant']}"
        formatted_data.append({"text": text})
    
    return Dataset.from_list(formatted_data)

def setup_lora(model):
    """LoRA (Low-Rank Adaptation) ì„¤ì •"""
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=8,  # ë” ì‘ì€ rankë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        lora_alpha=16,
        lora_dropout=0.1,
        # DialoGPTì— ë§ëŠ” target modules
        target_modules=["c_attn", "c_proj"]
    )
    
    model = get_peft_model(model, lora_config)
    print(f"ğŸ”§ LoRA ì ìš© ì™„ë£Œ")
    print(f"ğŸ“Š í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {model.num_parameters():,}")
    
    return model

def train_model(model, tokenizer, dataset, output_dir="./finetuned_model"):
    """ëª¨ë¸ í›ˆë ¨"""
    print("ğŸ¯ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
    
    # í† í¬ë‚˜ì´ì§•
    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=256,  # ë” ì§§ì€ ê¸¸ì´ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            return_tensors="pt"
        )
    
    tokenized_dataset = dataset.map(tokenize_function, batched=True)
    
    # ë°ì´í„° ì½œë ‰í„°
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
    )
    
    # í›ˆë ¨ ì„¤ì •
    training_args = TrainingArguments(
        output_dir=output_dir,
        overwrite_output_dir=True,
        num_train_epochs=3,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        learning_rate=1e-4,
        warmup_steps=50,
        logging_steps=10,
        save_steps=100,
        save_total_limit=2,
        evaluation_strategy="no",
        fp16=torch.cuda.is_available(),
        dataloader_pin_memory=False,
        remove_unused_columns=False,
    )
    
    # íŠ¸ë ˆì´ë„ˆ
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )
    
    # í›ˆë ¨ ì‹¤í–‰
    trainer.train()
    
    # ëª¨ë¸ ì €ì¥
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    print(f"âœ… í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ ì €ì¥ë¨: {output_dir}")
    return output_dir

def test_inference(model, tokenizer):
    """ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    
    test_prompts = [
        "ì‚¬ìš©ì: ì•ˆë…•í•˜ì„¸ìš”!\nì–´ì‹œìŠ¤í„´íŠ¸:",
        "ì‚¬ìš©ì: Pythonì„ ë°°ìš°ê³  ì‹¶ì–´ìš”.\nì–´ì‹œìŠ¤í„´íŠ¸:",
    ]
    
    for prompt in test_prompts:
        print(f"\nì…ë ¥: {prompt.split('ì–´ì‹œìŠ¤í„´íŠ¸:')[0].strip()}")
        
        inputs = tokenizer(prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        assistant_response = response.split("ì–´ì‹œìŠ¤í„´íŠ¸:")[-1].strip()
        print(f"ì‘ë‹µ: {assistant_response}")

def save_config(output_dir, model_name, training_params):
    """í›ˆë ¨ ì„¤ì • ì €ì¥"""
    config = {
        "base_model": model_name,
        "training_params": training_params,
        "timestamp": str(torch.cuda.current_device() if torch.cuda.is_available() else "cpu"),
        "gpu_used": torch.cuda.is_available()
    }
    
    config_path = os.path.join(output_dir, "training_config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“ ì„¤ì • ì €ì¥ë¨: {config_path}")

def main():
    parser = argparse.ArgumentParser(description="GPU ê¸°ë°˜ í•œêµ­ì–´ ëŒ€í™” ëª¨ë¸ íŒŒì¸íŠœë‹")
    parser.add_argument("--model", default="microsoft/DialoGPT-medium", help="ë² ì´ìŠ¤ ëª¨ë¸")
    parser.add_argument("--output", default="./finetuned_korean_chat", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--epochs", type=int, default=3, help="í›ˆë ¨ ì—í¬í¬")
    parser.add_argument("--test-only", action="store_true", help="ì¶”ë¡ ë§Œ í…ŒìŠ¤íŠ¸")
    
    args = parser.parse_args()
    
    # í™˜ê²½ ì„¤ì •
    setup_environment()
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer(args.model)
    
    if args.test_only:
        # ì¶”ë¡ ë§Œ í…ŒìŠ¤íŠ¸
        test_inference(model, tokenizer)
        return
    
    # ë°ì´í„°ì…‹ ìƒì„±
    print("ğŸ“š ë°ì´í„°ì…‹ ì¤€ë¹„")
    dataset = create_korean_dataset()
    print(f"ë°ì´í„° ìˆ˜: {len(dataset)}")
    
    # LoRA ì ìš©
    model = setup_lora(model)
    
    # í›ˆë ¨
    output_dir = train_model(model, tokenizer, dataset, args.output)
    
    # ì„¤ì • ì €ì¥
    save_config(output_dir, args.model, {"epochs": args.epochs})
    
    # ì¶”ë¡  í…ŒìŠ¤íŠ¸
    test_inference(model, tokenizer)
    
    print("\nğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë¸ ìœ„ì¹˜: {output_dir}")
    print("ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„: GGUF ë³€í™˜ í›„ main.pyì—ì„œ ì‚¬ìš©")

if __name__ == "__main__":
    main()
