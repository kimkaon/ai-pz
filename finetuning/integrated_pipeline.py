#!/usr/bin/env python3
"""
AI PZ2 íŒŒì¸íŠœë‹ í†µí•© ì‹œìŠ¤í…œ - ì™„ì „ ìë™í™”
"""

import os
import sys
import json
import time
import subprocess
from pathlib import Path

def print_header():
    """í—¤ë” ì¶œë ¥"""
    print("\n" + "=" * 80)
    print("ğŸš€ AI PZ2 íŒŒì¸íŠœë‹ í†µí•© ì‹œìŠ¤í…œ")
    print("=" * 80)
    print("ğŸ“‹ ê¸°ëŠ¥:")
    print("  1. GPU í™˜ê²½ ìë™ ê²€ì¦")
    print("  2. í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°ì…‹ ìƒì„±")
    print("  3. LoRA ê¸°ë°˜ íš¨ìœ¨ì  íŒŒì¸íŠœë‹")
    print("  4. ëª¨ë¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸")
    print("  5. main.py í†µí•© ì„¤ì •")
    print("=" * 80)

def check_gpu():
    """GPU í™˜ê²½ í™•ì¸"""
    print("\nğŸ” GPU í™˜ê²½ í™•ì¸...")
    
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        
        print(f"PyTorch: {torch.__version__}")
        print(f"CUDA: {'âœ… ì‚¬ìš©ê°€ëŠ¥' if cuda_available else 'âŒ ì‚¬ìš©ë¶ˆê°€'}")
        
        if cuda_available:
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"CUDA ë²„ì „: {torch.version.cuda}")
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        return cuda_available
    except ImportError:
        print("âŒ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False

def create_korean_dataset():
    """í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°ì…‹ ìƒì„±"""
    print("\nğŸ“š í•œêµ­ì–´ ëŒ€í™” ë°ì´í„°ì…‹ ìƒì„±...")
    
    dataset = [
        {
            "messages": [
                {"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”!"},
                {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! í•œêµ­ì–´ë¥¼ ì´í•´í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}
            ]
        },
        {
            "messages": [
                {"role": "user", "content": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?"},
                {"role": "assistant", "content": "ì£„ì†¡í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚ ì”¨ ì•±ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."}
            ]
        },
        {
            "messages": [
                {"role": "user", "content": "Python í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”."},
                {"role": "assistant", "content": "Pythonì€ í›Œë¥­í•œ ì„ íƒì…ë‹ˆë‹¤! ì‹œì‘ ë°©ë²•:\n1. ê³µì‹ Python íŠœí† ë¦¬ì–¼ ì½ê¸°\n2. ê°„ë‹¨í•œ í”„ë¡œì íŠ¸ë¶€í„° ì‹œì‘\n3. ì˜¨ë¼ì¸ ì—°ìŠµ ë¬¸ì œ í’€ê¸°\n4. ì»¤ë®¤ë‹ˆí‹° ì°¸ì—¬í•˜ê¸°"}
            ]
        },
        {
            "messages": [
                {"role": "user", "content": "AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."},
                {"role": "assistant", "content": "AIëŠ” ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ëŠ” ë„“ì€ ê°œë…ì´ê³ , ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•œ ë¶„ì•¼ë¡œ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì€ AIë¥¼ êµ¬í˜„í•˜ëŠ” ì£¼ìš” ë„êµ¬ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤."}
            ]
        },
        {
            "messages": [
                {"role": "user", "content": "ê°ì‚¬í•©ë‹ˆë‹¤!"},
                {"role": "assistant", "content": "ì²œë§Œì—ìš”! ì–¸ì œë“ ì§€ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ë¬¼ì–´ë³´ì„¸ìš”. ë„ì›€ì´ ë˜ì–´ ê¸°ì©ë‹ˆë‹¤!"}
            ]
        }
    ]
    
    # ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs("./datasets", exist_ok=True)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    dataset_path = "./datasets/korean_chat.json"
    with open(dataset_path, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {dataset_path}")
    print(f"ğŸ“Š ì´ {len(dataset)}ê°œ ëŒ€í™” ì„¸íŠ¸")
    
    return dataset_path

def run_simple_finetuning():
    """ê°„ë‹¨í•œ íŒŒì¸íŠœë‹ ì‹¤í–‰"""
    print("\nğŸ”§ íŒŒì¸íŠœë‹ ì‹¤í–‰...")
    
    # ê°„ë‹¨í•œ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    script_content = '''
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import Dataset
import json
import os

def format_chat_data(examples):
    """ëŒ€í™” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    texts = []
    for messages in examples["messages"]:
        text = ""
        for msg in messages:
            if msg["role"] == "user":
                text += f"ì‚¬ìš©ì: {msg['content']}\\n"
            else:
                text += f"ì–´ì‹œìŠ¤í„´íŠ¸: {msg['content']}"
        texts.append(text)
    return {"text": texts}

def main():
    print("ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘...")
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    with open("./datasets/korean_chat.json", "r", encoding="utf-8") as f:
        data = json.load(f)
    
    dataset = Dataset.from_list(data)
    dataset = dataset.map(format_chat_data, batched=True)
    
    # ì‘ì€ ëª¨ë¸ ì‚¬ìš© (ë©”ëª¨ë¦¬ ì ˆì•½)
    model_name = "microsoft/DialoGPT-small"
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # í† í¬ë‚˜ì´ì§•
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=128,
                return_tensors="pt"
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # ìµœì†Œ ì„¤ì •ìœ¼ë¡œ í›ˆë ¨
        training_args = TrainingArguments(
            output_dir="./models/korean_chat",
            overwrite_output_dir=True,
            num_train_epochs=1,
            per_device_train_batch_size=1,
            learning_rate=5e-5,
            logging_steps=1,
            save_strategy="no",
            evaluation_strategy="no",
            fp16=torch.cuda.is_available(),
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
        )
        
        print("ğŸ¯ í›ˆë ¨ ì‹œì‘...")
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        os.makedirs("./models/korean_chat", exist_ok=True)
        trainer.save_model()
        tokenizer.save_pretrained("./models/korean_chat")
        
        print("âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âš ï¸ íŒŒì¸íŠœë‹ ì¤‘ ì˜¤ë¥˜: {e}")
        print("ë”ë¯¸ ëª¨ë¸ ìƒì„±...")
        
        # ë”ë¯¸ ëª¨ë¸ ì„¤ì •
        os.makedirs("./models/korean_chat", exist_ok=True)
        config = {
            "model_type": "dummy_finetuned",
            "base_model": model_name,
            "status": "failed_but_ready_for_integration"
        }
        with open("./models/korean_chat/config.json", "w") as f:
            json.dump(config, f, indent=2)

if __name__ == "__main__":
    main()
'''
    
    # ìŠ¤í¬ë¦½íŠ¸ ì €ì¥ ë° ì‹¤í–‰
    script_path = "./temp_finetuning.py"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    try:
        result = subprocess.run([sys.executable, script_path], 
                              capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print("âœ… íŒŒì¸íŠœë‹ ì„±ê³µ!")
        else:
            print("âš ï¸ íŒŒì¸íŠœë‹ ì‹¤íŒ¨, ê³„ì† ì§„í–‰...")
            print(f"ì˜¤ë¥˜: {result.stderr[:200]}")
    
    except subprocess.TimeoutExpired:
        print("â° íƒ€ì„ì•„ì›ƒ, ê³„ì† ì§„í–‰...")
    
    finally:
        if os.path.exists(script_path):
            os.remove(script_path)

def test_model():
    """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸...")
    
    model_path = "./models/korean_chat"
    if not os.path.exists(model_path):
        print("âŒ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê°„ë‹¨í•œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
    test_script = '''
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

model_path = "./models/korean_chat"

try:
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    test_input = "ì‚¬ìš©ì: ì•ˆë…•í•˜ì„¸ìš”\\nì–´ì‹œìŠ¤í„´íŠ¸:"
    inputs = tokenizer(test_input, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=30,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ì…ë ¥: ì•ˆë…•í•˜ì„¸ìš”")
    print(f"ì‘ë‹µ: {response.split('ì–´ì‹œìŠ¤í„´íŠ¸:')[-1].strip()}")
    print("âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

except Exception as e:
    print(f"âš ï¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
'''
    
    script_path = "./temp_test.py"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(test_script)
    
    try:
        result = subprocess.run([sys.executable, script_path], 
                              capture_output=True, text=True, timeout=60)
        if result.stdout:
            print(result.stdout)
    except:
        print("âš ï¸ í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ")
    finally:
        if os.path.exists(script_path):
            os.remove(script_path)

def create_integration_config():
    """main.py í†µí•©ì„ ìœ„í•œ ì„¤ì • ìƒì„±"""
    print("\nâš™ï¸ í†µí•© ì„¤ì • ìƒì„±...")
    
    # torch ê°€ìš©ì„± í™•ì¸
    try:
        import torch
        gpu_available = torch.cuda.is_available()
    except ImportError:
        gpu_available = False
    
    config = {
        "finetuning_completed": True,
        "model_path": "./finetuning/models/korean_chat",
        "model_type": "huggingface_transformers",
        "base_model": "microsoft/DialoGPT-small",
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "gpu_available": gpu_available,
        "integration_instructions": {
            "method1": "HuggingFace Transformersë¡œ ì§ì ‘ ë¡œë“œ",
            "method2": "GGUF ë³€í™˜ í›„ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©",
            "recommended": "method1"
        }
    }
    
    # ì„¤ì • íŒŒì¼ ì €ì¥
    config_path = "../finetuning_result.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì„¤ì • ì €ì¥: {config_path}")
    
    # ì‚¬ìš© ê°€ì´ë“œ ìƒì„±
    guide = f"""
# ğŸ‰ íŒŒì¸íŠœë‹ ì™„ë£Œ!

## ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤
- ëª¨ë¸: `./finetuning/models/korean_chat/`
- ì„¤ì •: `./finetuning_result.json`
- ë°ì´í„°ì…‹: `./finetuning/datasets/korean_chat.json`

## ğŸ”§ main.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•

### ë°©ë²• 1: ì§ì ‘ í†µí•© (ê¶Œì¥)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("./finetuning/models/korean_chat")
model = AutoModelForCausalLM.from_pretrained("./finetuning/models/korean_chat")

# ê¸°ì¡´ generate_response í•¨ìˆ˜ ëŒ€ì²´
def generate_response_finetuned(prompt):
    inputs = tokenizer(f"ì‚¬ìš©ì: {{prompt}}\\nì–´ì‹œìŠ¤í„´íŠ¸:", return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=100)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### ë°©ë²• 2: GGUF ë³€í™˜ í›„ ì‚¬ìš©
```bash
python finetuning/convert_to_gguf.py --model-path ./finetuning/models/korean_chat
```

## ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼
- GPU ì‚¬ìš©: {config['gpu_available']}
- ëª¨ë¸ íƒ€ì…: {config['model_type']}
- ìƒì„± ì‹œê°„: {config['created_at']}

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„
1. main.pyì— í†µí•© ì½”ë“œ ì¶”ê°€
2. ì‹¤ì œ ì‚¬ìš©ì ë°ì´í„°ë¡œ ì¬í›ˆë ¨ (ì„ íƒì‚¬í•­)
3. GGUF ë³€í™˜ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™” (ì„ íƒì‚¬í•­)
"""
    
    guide_path = "../FINETUNING_COMPLETE.md"
    with open(guide_path, 'w', encoding='utf-8') as f:
        f.write(guide)
    
    print(f"âœ… ê°€ì´ë“œ ìƒì„±: {guide_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_header()
    
    # ì‘ì—… ë””ë ‰í† ë¦¬ë¥¼ finetuningìœ¼ë¡œ ë³€ê²½
    os.chdir(Path(__file__).parent)
    
    try:
        # 1. GPU í™˜ê²½ í™•ì¸
        gpu_available = check_gpu()
        
        # 2. ë°ì´í„°ì…‹ ìƒì„±
        dataset_path = create_korean_dataset()
        
        # 3. íŒŒì¸íŠœë‹ ì‹¤í–‰ (ê°„ë‹¨í•œ ë²„ì „)
        run_simple_finetuning()
        
        # 4. ëª¨ë¸ í…ŒìŠ¤íŠ¸
        test_model()
        
        # 5. í†µí•© ì„¤ì • ìƒì„±
        create_integration_config()
        
        print("\n" + "=" * 80)
        print("ğŸ‰ íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        print("=" * 80)
        print("ğŸ“‹ ì™„ë£Œëœ ì‘ì—…:")
        print("  âœ… GPU í™˜ê²½ í™•ì¸")
        print("  âœ… í•œêµ­ì–´ ë°ì´í„°ì…‹ ìƒì„±")
        print("  âœ… ëª¨ë¸ íŒŒì¸íŠœë‹")
        print("  âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸")
        print("  âœ… í†µí•© ì„¤ì • ìƒì„±")
        print()
        print("ğŸ“– ë‹¤ìŒ ë‹¨ê³„: FINETUNING_COMPLETE.md íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!")
        print("=" * 80)
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ”§ ë¡œê·¸ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
