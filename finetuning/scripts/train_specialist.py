#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œìš©)
Specialist Model Training Script (for Hybrid System)

ê° ë„ë©”ì¸ë³„ ì „ë¬¸ê°€ ëª¨ë¸ì„ ê°œë³„ì ìœ¼ë¡œ í›ˆë ¨í•©ë‹ˆë‹¤.
Trains specialist models for each domain individually.
"""

import json
import os
import sys
import logging
from pathlib import Path
from typing import Dict, List, Tuple
import argparse

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import torch

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SpecialistModelTrainer:
    """ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, base_model_name: str = "microsoft/DialoGPT-medium"):
        """
        Args:
            base_model_name: ê¸°ë³¸ ëª¨ë¸ëª… (Microsoft DialoGPT ì‚¬ìš©)
        """
        self.base_model_name = base_model_name
        self.tokenizer = None
        self.model = None
        
        # ì „ë¬¸ê°€ ë„ë©”ì¸ ì •ì˜
        self.specialist_domains = {
            'qna_specialist': {
                'description': 'Question & Answer specialist',
                'data_prefix': 'dialogue_qna_',
                'output_dir': 'qna_specialist'
            },
            'technical_specialist': {
                'description': 'Technical knowledge specialist',
                'data_prefix': 'dialogue_technical_',
                'output_dir': 'technical_specialist'
            }
        }
    
    def load_base_model(self):
        """ê¸°ë³¸ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        logger.info(f"ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì¤‘: {self.base_model_name}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.base_model_name)
            
            # íŒ¨ë”© í† í° ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            logger.info("âœ… ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def load_specialist_data(self, domain: str) -> List[Dict]:
        """íŠ¹ì • ë„ë©”ì¸ì˜ í›ˆë ¨ ë°ì´í„° ë¡œë”©"""
        
        data_dir = Path(__file__).parent.parent / "datasets" / "processed_english"
        data_prefix = self.specialist_domains[domain]['data_prefix']
        
        train_file = data_dir / f"{data_prefix}train.jsonl"
        
        if not train_file.exists():
            logger.warning(f"âš ï¸ í›ˆë ¨ ë°ì´í„° ì—†ìŒ: {train_file}")
            return []
        
        try:
            data = []
            with open(train_file, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        data.append(json.loads(line))
            
            logger.info(f"ğŸ“ {domain} ë°ì´í„° ë¡œë”©: {len(data)}ê°œ ìƒ˜í”Œ")
            return data
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ ({domain}): {e}")
            return []
    
    def prepare_training_data(self, data: List[Dict]) -> Dataset:
        """í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬"""
        
        # ëŒ€í™” í˜•íƒœë¡œ ë³€í™˜
        formatted_data = []
        
        for item in data:
            # í•œêµ­ì–´ ëŒ“ê¸€ì€ ì°¸ê³ ìš©ìœ¼ë¡œ ìœ ì§€í•˜ë˜, ì‹¤ì œ í›ˆë ¨ì€ ì˜ì–´ë¡œ
            input_text = item.get('input', '')
            output_text = item.get('output', '')
            
            # ëŒ€í™” í˜•ì‹ êµ¬ì„± 
            conversation = f"User: {input_text}\nAssistant: {output_text}"
            formatted_data.append({'text': conversation})
        
        # Hugging Face Datasetìœ¼ë¡œ ë³€í™˜
        dataset = Dataset.from_list(formatted_data)
        
        # í† í¬ë‚˜ì´ì§•
        def tokenize_function(examples):
            return self.tokenizer(
                examples['text'],
                truncation=True,
                padding=True,
                max_length=512,
                return_tensors="pt"
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        logger.info(f"âœ… í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(tokenized_dataset)}ê°œ ìƒ˜í”Œ")
        return tokenized_dataset
    
    def train_specialist_model(self, domain: str, epochs: int = 3, 
                             learning_rate: float = 5e-5) -> bool:
        """íŠ¹ì • ë„ë©”ì¸ì˜ ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨"""
        
        logger.info(f"ğŸ¯ {domain} ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        # ë°ì´í„° ë¡œë”©
        data = self.load_specialist_data(domain)
        if not data:
            logger.error(f"âŒ {domain} í›ˆë ¨ ë°ì´í„° ì—†ìŒ")
            return False
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        dataset = self.prepare_training_data(data)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        output_dir = Path(__file__).parent.parent / "models" / self.specialist_domains[domain]['output_dir']
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # í›ˆë ¨ ì¸ìˆ˜ ì„¤ì •
        training_args = TrainingArguments(
            output_dir=str(output_dir),
            overwrite_output_dir=True,
            num_train_epochs=epochs,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            warmup_steps=100,
            logging_steps=50,
            learning_rate=learning_rate,
            save_steps=500,
            save_total_limit=2,
            prediction_loss_only=True,
            remove_unused_columns=False,
        )
        
        # ë°ì´í„° ì»¬ë ‰í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§ ì‚¬ìš©
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì„¤ì •
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        try:
            # í›ˆë ¨ ì‹¤í–‰
            logger.info(f"ğŸš€ {domain} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            trainer.train()
            
            # ëª¨ë¸ ì €ì¥
            trainer.save_model()
            self.tokenizer.save_pretrained(str(output_dir))
            
            logger.info(f"âœ… {domain} ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ: {output_dir}")
            
            # ëª¨ë¸ ì •ë³´ ì €ì¥
            model_info = {
                'domain': domain,
                'base_model': self.base_model_name,
                'training_samples': len(data),
                'epochs': epochs,
                'learning_rate': learning_rate,
                'description': self.specialist_domains[domain]['description']
            }
            
            with open(output_dir / "model_info.json", 'w', encoding='utf-8') as f:
                json.dump(model_info, f, indent=2, ensure_ascii=False)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ {domain} ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            return False
    
    def train_all_specialists(self, epochs: int = 3):
        """ëª¨ë“  ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨"""
        
        logger.info("ğŸ¯ ëª¨ë“  ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        if not self.load_base_model():
            logger.error("âŒ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ í›ˆë ¨ ì¤‘ë‹¨")
            return
        
        results = {}
        
        for domain in self.specialist_domains:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ“š {domain} í›ˆë ¨ ì¤‘...")
            
            success = self.train_specialist_model(domain, epochs=epochs)
            results[domain] = success
            
            if success:
                logger.info(f"âœ… {domain} ì„±ê³µ")
            else:
                logger.error(f"âŒ {domain} ì‹¤íŒ¨")
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        logger.info(f"\n{'='*60}")
        logger.info("ğŸ“Š ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨ ê²°ê³¼:")
        
        successful = sum(results.values())
        total = len(results)
        
        for domain, success in results.items():
            status = "âœ…" if success else "âŒ"
            logger.info(f"   {status} {domain}")
        
        logger.info(f"\nğŸ¯ ì„±ê³µ: {successful}/{total}")
        
        if successful == total:
            logger.info("ğŸ‰ ëª¨ë“  ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        else:
            logger.warning(f"âš ï¸ {total - successful}ê°œ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description="ì „ë¬¸ê°€ ëª¨ë¸ í›ˆë ¨")
    parser.add_argument('--domain', type=str, choices=['qna_specialist', 'daily_chat_specialist', 'technical_specialist', 'all'], 
                       default='all', help='í›ˆë ¨í•  ë„ë©”ì¸ ì„ íƒ')
    parser.add_argument('--epochs', type=int, default=3, help='í›ˆë ¨ ì—í¬í¬ ìˆ˜')
    parser.add_argument('--learning_rate', type=float, default=5e-5, help='í•™ìŠµë¥ ')
    parser.add_argument('--base_model', type=str, default="microsoft/DialoGPT-medium", help='ê¸°ë³¸ ëª¨ë¸ëª…')
    
    args = parser.parse_args()
    
    trainer = SpecialistModelTrainer(base_model_name=args.base_model)
    
    if args.domain == 'all':
        trainer.train_all_specialists(epochs=args.epochs)
    else:
        if not trainer.load_base_model():
            logger.error("âŒ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
            return
            
        success = trainer.train_specialist_model(args.domain, epochs=args.epochs, learning_rate=args.learning_rate)
        
        if success:
            logger.info(f"âœ… {args.domain} í›ˆë ¨ ì™„ë£Œ")
        else:
            logger.error(f"âŒ {args.domain} í›ˆë ¨ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()
