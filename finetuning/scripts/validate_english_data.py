#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë°ì´í„° ì˜¤ë¥˜ ê²€ì¦ ë° ìˆ˜ì • ìŠ¤í¬ë¦½íŠ¸
Data Error Validation and Fix Script

ì˜ì–´ ê¸°ë°˜ íŒŒì¸íŠœë‹ ë°ì´í„°ì˜ ì˜¤ë¥˜ë¥¼ ê²€ì‚¬í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤.
Checks and fixes errors in English-based fine-tuning data.
"""

import json
import os
from pathlib import Path
import re

def check_data_quality():
    """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ / Data quality check"""
    
    processed_dir = Path(__file__).parent.parent / "datasets" / "processed_english"
    
    print("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘... / Starting data quality check...")
    
    issues = []
    recommendations = []
    
    # 1. íŒŒì¼ ì¡´ì¬ ë° í¬ê¸° í™•ì¸ / Check file existence and size (daily_chat ì œê±°ë¨)
    required_files = [
        "classification_train.jsonl", "classification_validation.jsonl", "classification_test.jsonl",
        "dialogue_qna_train.jsonl", "dialogue_technical_train.jsonl", "dialogue_general_train.jsonl",
        "unified_train.jsonl", "unified_validation.jsonl", "unified_test.jsonl"
    ]
    
    for filename in required_files:
        filepath = processed_dir / filename
        if not filepath.exists():
            issues.append(f"âŒ íŒŒì¼ ì—†ìŒ / Missing file: {filename}")
        else:
            file_size = filepath.stat().st_size
            if file_size < 100:  # 100 bytes ë¯¸ë§Œ
                issues.append(f"âš ï¸ íŒŒì¼ í¬ê¸° ì‘ìŒ / Small file size: {filename} ({file_size} bytes)")
    
    # 2. JSON êµ¬ì¡° ê²€ì¦ / JSON structure validation
    for filename in required_files:
        filepath = processed_dir / filename
        if filepath.exists():
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    data_lines = [line for line in lines if not line.strip().startswith('#')]
                    
                    for i, line in enumerate(data_lines):
                        if line.strip():
                            try:
                                data = json.loads(line.strip())
                                
                                # ë¶„ë¥˜ ë°ì´í„° ê²€ì¦ / Classification data validation
                                if filename.startswith("classification"):
                                    if "text" not in data:
                                        issues.append(f"âŒ 'text' í•„ë“œ ì—†ìŒ / Missing 'text' field: {filename}:{i+1}")
                                    if "label" not in data:
                                        issues.append(f"âŒ 'label' í•„ë“œ ì—†ìŒ / Missing 'label' field: {filename}:{i+1}")
                                    elif not isinstance(data["label"], int) or data["label"] not in [0,1,2]:  # qna, technical, general
                                        issues.append(f"âŒ ì˜ëª»ëœ ë ˆì´ë¸” / Invalid label: {filename}:{i+1} (label: {data.get('label')})")
                                
                                # ëŒ€í™” ë°ì´í„° ê²€ì¦ / Dialogue data validation (í•„ë“œëª… ìˆ˜ì •)
                                elif filename.startswith("dialogue"):
                                    if "input_text" not in data:
                                        issues.append(f"âŒ 'input_text' í•„ë“œ ì—†ìŒ / Missing 'input_text' field: {filename}:{i+1}")
                                    if "target_text" not in data:
                                        issues.append(f"âŒ 'target_text' í•„ë“œ ì—†ìŒ / Missing 'target_text' field: {filename}:{i+1}")
                                
                                # í†µí•© ë°ì´í„° ê²€ì¦ / Unified data validation
                                elif filename.startswith("unified"):
                                    if "input_text" not in data:
                                        issues.append(f"âŒ 'input_text' í•„ë“œ ì—†ìŒ / Missing 'input_text' field: {filename}:{i+1}")
                                    if "target_text" not in data:
                                        issues.append(f"âŒ 'target_text' í•„ë“œ ì—†ìŒ / Missing 'target_text' field: {filename}:{i+1}")
                                    if "category" not in data:
                                        issues.append(f"âŒ 'category' í•„ë“œ ì—†ìŒ / Missing 'category' field: {filename}:{i+1}")
                                
                                # í…ìŠ¤íŠ¸ í’ˆì§ˆ ê²€ì¦ / Text quality validation
                                for key, value in data.items():
                                    if isinstance(value, str):
                                        # ë¹ˆ í…ìŠ¤íŠ¸ í™•ì¸ / Check empty text
                                        if not value.strip():
                                            issues.append(f"âŒ ë¹ˆ í…ìŠ¤íŠ¸ / Empty text: {filename}:{i+1}, field: {key}")
                                        
                                        # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ / Too short text
                                        if len(value.strip()) < 3:
                                            issues.append(f"âš ï¸ ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ / Too short text: {filename}:{i+1}, field: {key} ('{value}')")
                                        
                                        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ / Too long text
                                        if len(value) > 5000:
                                            issues.append(f"âš ï¸ ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ / Too long text: {filename}:{i+1}, field: {key} ({len(value)} chars)")
                                        
                                        # íŠ¹ìˆ˜ ë¬¸ì ê²€ì¦ / Special character validation
                                        if re.search(r'[^\w\s\-.,!?();:\'"/@#$%^&*+=<>{}[\]|\\~`]', value):
                                            recommendations.append(f"ğŸ’¡ íŠ¹ìˆ˜ ë¬¸ì ë°œê²¬ / Special characters found: {filename}:{i+1}, field: {key}")
                                        
                                        # ì˜ì–´ í…ìŠ¤íŠ¸ ê²€ì¦ / English text validation
                                        if filename.startswith(("classification", "dialogue", "unified")):
                                            # ì£¼ë¡œ ì˜ì–´ì¸ì§€ í™•ì¸ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                                            english_chars = len(re.findall(r'[a-zA-Z]', value))
                                            total_chars = len(re.findall(r'[a-zA-Zã„±-ã…ã…-ã…£ê°€-í£]', value))
                                            if total_chars > 0 and english_chars / total_chars < 0.7:
                                                recommendations.append(f"ğŸ’¡ ì˜ì–´ ë¹„ìœ¨ ë‚®ìŒ / Low English ratio: {filename}:{i+1}, field: {key}")
                                
                            except json.JSONDecodeError as e:
                                issues.append(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜ / JSON parsing error: {filename}:{i+1}: {e}")
                                
            except Exception as e:
                issues.append(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ / File reading error: {filename}: {e}")
    
    # 3. ë°ì´í„° ë¶„í¬ ê²€ì¦ / Data distribution validation
    try:
        # ë¶„ë¥˜ ë ˆì´ë¸” ë¶„í¬ / Classification label distribution
        for split in ["train", "validation", "test"]:
            filepath = processed_dir / f"classification_{split}.jsonl"
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    lines = [line for line in f.readlines() if not line.strip().startswith('#')]
                    labels = []
                    for line in lines:
                        if line.strip():
                            try:
                                data = json.loads(line.strip())
                                labels.append(data.get("label"))
                            except:
                                continue
                    
                    unique_labels = set(labels)
                    expected_labels = {0, 1, 2}  # qna, technical, general (daily_chat ì œê±°ë¨)
                    
                    if unique_labels != expected_labels:
                        missing = expected_labels - unique_labels
                        if missing:
                            recommendations.append(f"ğŸ’¡ ëˆ„ë½ëœ ë ˆì´ë¸” / Missing labels in {split}: {missing}")
                        
                        # ë ˆì´ë¸” ë¶„í¬ ê³„ì‚° / Calculate label distribution
                        label_counts = {label: labels.count(label) for label in unique_labels}
                        total = len(labels)
                        if total > 0:
                            distribution = {label: count/total for label, count in label_counts.items()}
                            imbalanced = any(ratio < 0.1 or ratio > 0.6 for ratio in distribution.values())
                            if imbalanced:
                                recommendations.append(f"ğŸ’¡ ë¶ˆê· í˜•í•œ ë ˆì´ë¸” ë¶„í¬ / Imbalanced distribution in {split}: {distribution}")
        
        # í†µí•© ëª¨ë¸ ì¹´í…Œê³ ë¦¬ ë¶„í¬ / Unified model category distribution
        for split in ["train", "validation", "test"]:
            filepath = processed_dir / f"unified_{split}.jsonl"
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    lines = [line for line in f.readlines() if not line.strip().startswith('#')]
                    categories = []
                    for line in lines:
                        if line.strip():
                            try:
                                data = json.loads(line.strip())
                                categories.append(data.get("category"))
                            except:
                                continue
                    
                    unique_categories = set(categories)
                    expected_categories = {"qna", "technical", "general"}  # daily_chat ì œê±°ë¨
                    
                    if unique_categories != expected_categories:
                        missing = expected_categories - unique_categories
                        if missing:
                            recommendations.append(f"ğŸ’¡ ëˆ„ë½ëœ ì¹´í…Œê³ ë¦¬ / Missing categories in unified {split}: {missing}")
    
    except Exception as e:
        recommendations.append(f"ğŸ’¡ ë¶„í¬ ê²€ì¦ ì˜¤ë¥˜ / Distribution validation error: {e}")
    
    # 4. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¼ê´€ì„± ê²€ì¦ / System prompt consistency validation
    try:
        unified_files = ["unified_train.jsonl", "unified_validation.jsonl", "unified_test.jsonl"]
        system_prompts = set()
        
        for filename in unified_files:
            filepath = processed_dir / filename
            if filepath.exists():
                with open(filepath, 'r', encoding='utf-8') as f:
                    lines = [line for line in f.readlines() if not line.strip().startswith('#')]
                    for line in lines:
                        if line.strip():
                            try:
                                data = json.loads(line.strip())
                                input_text = data.get("input_text", "")
                                if "System:" in input_text:
                                    system_part = input_text.split("User:")[0].strip()
                                    system_prompts.add(system_part)
                            except:
                                continue
        
        if len(system_prompts) > 1:
            recommendations.append(f"ğŸ’¡ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë¶ˆì¼ì¹˜ / Inconsistent system prompts: {len(system_prompts)} different versions found")
        elif len(system_prompts) == 0:
            issues.append(f"âŒ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì—†ìŒ / No system prompts found in unified data")
    
    except Exception as e:
        recommendations.append(f"ğŸ’¡ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê²€ì¦ ì˜¤ë¥˜ / System prompt validation error: {e}")
    
    # ê²°ê³¼ ì¶œë ¥ / Print results
    print(f"\nğŸ“Š ê²€ì‚¬ ê²°ê³¼ / Inspection Results:")
    print(f"   - ê²€ì‚¬ íŒŒì¼ / Files checked: {len(required_files)}")
    print(f"   - ì‹¬ê°í•œ ì˜¤ë¥˜ / Critical errors: {len(issues)}")
    print(f"   - ê¶Œì¥ì‚¬í•­ / Recommendations: {len(recommendations)}")
    
    if issues:
        print(f"\nâŒ ì‹¬ê°í•œ ì˜¤ë¥˜ë“¤ / Critical Errors:")
        for issue in issues:
            print(f"   {issue}")
    
    if recommendations:
        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­ë“¤ / Recommendations:")
        for rec in recommendations:
            print(f"   {rec}")
    
    if not issues and not recommendations:
        print(f"\nâœ… ëª¨ë“  ê²€ì‚¬ í†µê³¼! / All checks passed!")
        print(f"   - ë°ì´í„°ê°€ íŒŒì¸íŠœë‹ì— ì í•©í•©ë‹ˆë‹¤ / Data is suitable for fine-tuning")
        print(f"   - ì˜¤ë¥˜ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ / Very low error probability")
    elif not issues:
        print(f"\nâœ… ì‹¬ê°í•œ ì˜¤ë¥˜ ì—†ìŒ / No critical errors found")
        print(f"   - íŒŒì¸íŠœë‹ ì§„í–‰ ê°€ëŠ¥ / Can proceed with fine-tuning")
        print(f"   - ê¶Œì¥ì‚¬í•­ì€ ì„ íƒì ìœ¼ë¡œ ì ìš© / Recommendations are optional")
    else:
        print(f"\nâš ï¸ ìˆ˜ì • í•„ìš”í•œ ì˜¤ë¥˜ ë°œê²¬ / Errors found that need fixing")
        print(f"   - íŒŒì¸íŠœë‹ ì „ ì˜¤ë¥˜ ìˆ˜ì • ê¶Œì¥ / Recommend fixing before fine-tuning")
    
    return len(issues) == 0

def check_model_compatibility():
    """ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬ / Model compatibility check"""
    
    print(f"\nğŸ¤– ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬ / Model Compatibility Check:")
    
    compatibility_notes = [
        "âœ… ì˜ì–´ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„± / Composed of English text",
        "âœ… JSON Lines í˜•ì‹ / JSON Lines format", 
        "âœ… UTF-8 ì¸ì½”ë”© / UTF-8 encoding",
        "âœ… í‘œì¤€ í•„ë“œëª… ì‚¬ìš© / Standard field names used",
        "âœ… ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í¬í•¨ (í†µí•© ëª¨ë¸) / System prompts included (unified model)",
        "âœ… ë ˆì´ë¸” ì •ìˆ˜í˜• (ë¶„ë¥˜ ëª¨ë¸) / Integer labels (classification model)",
        "âœ… ì¹´í…Œê³ ë¦¬ ë¬¸ìì—´ (í†µí•© ëª¨ë¸) / String categories (unified model)"
    ]
    
    for note in compatibility_notes:
        print(f"   {note}")
    
    print(f"\nğŸ¯ ê¶Œì¥ ì‚¬ìš©ë²• / Recommended Usage:")
    print(f"   - Hugging Face Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ / Hugging Face Transformers library")
    print(f"   - PyTorch ë˜ëŠ” TensorFlow / PyTorch or TensorFlow")
    print(f"   - ì‚¬ì „ í›ˆë ¨ëœ ì˜ì–´ ëª¨ë¸ (BERT, GPT, T5 ë“±) / Pre-trained English models (BERT, GPT, T5, etc.)")
    print(f"   - ê¶Œì¥ ë°°ì¹˜ í¬ê¸°: 4-16 / Recommended batch size: 4-16")
    print(f"   - ê¶Œì¥ í•™ìŠµë¥ : 1e-5 ~ 5e-5 / Recommended learning rate: 1e-5 ~ 5e-5")

def main():
    """ë©”ì¸ í•¨ìˆ˜ / Main function"""
    
    print("ğŸ” ì˜ì–´ ê¸°ë°˜ íŒŒì¸íŠœë‹ ë°ì´í„° ê²€ì¦ / English Fine-tuning Data Validation")
    print("=" * 80)
    
    # ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ / Data quality check
    is_valid = check_data_quality()
    
    # ëª¨ë¸ í˜¸í™˜ì„± ê²€ì‚¬ / Model compatibility check
    check_model_compatibility()
    
    print(f"\n" + "=" * 80)
    if is_valid:
        print(f"ğŸ‰ ê²€ì¦ ì™„ë£Œ! ë°ì´í„°ê°€ íŒŒì¸íŠœë‹ì— ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ‰ Validation complete! Data is ready for fine-tuning.")
        print(f"\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ / Next Steps:")
        print(f"   1. run_finetuning.bat ì‹¤í–‰ / Run run_finetuning.bat")
        print(f"   2. ë©”ë‰´ì—ì„œ ì›í•˜ëŠ” í•™ìŠµ ë°©ì‹ ì„ íƒ / Select desired training method from menu")
        print(f"   3. ë¶„ë¦¬í˜•(4-7ë²ˆ) ë˜ëŠ” í†µí•©í˜•(8ë²ˆ) ì‹œë„ / Try multi-model (4-7) or unified (8)")
    else:
        print(f"âš ï¸ ì¼ë¶€ ì˜¤ë¥˜ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ì˜¤ë¥˜ë¥¼ ìˆ˜ì • í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        print(f"âš ï¸ Some errors were found. Please fix the above errors and try again.")
    
    return is_valid

if __name__ == "__main__":
    main()
