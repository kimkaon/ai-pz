#!/usr/bin/env python3
"""
í†µí•©í˜• ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ ëª¨ë“  ëŒ€í™” ìœ í˜•ì„ ì²˜ë¦¬
"""

import os
import json
import argparse
import torch
from pathlib import Path
import logging
from typing import Dict, List, Optional

# Transformers
from transformers import AutoTokenizer, AutoModelForCausalLM

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnifiedModelInference:
    """í†µí•©í˜• ëª¨ë¸ ì¶”ë¡  í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str = "models/unified_model"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ì¶”ë¡  ë””ë°”ì´ìŠ¤: {self.device}")
        
        self.model_path = Path(model_path)
        self.tokenizer = None
        self.model = None
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_prompt = """ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ìƒí™©ì— ì ì‘í•  ìˆ˜ ìˆëŠ” ë©€í‹°íƒœìŠ¤í¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì—­í•  ê°€ì´ë“œ:
- QnA/ì§ˆë¬¸ë‹µë³€: ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ì‚¬ì‹¤ ê¸°ë°˜ ë‹µë³€
- ì¼ìƒëŒ€í™”: ì¹œê·¼í•˜ê³  ê°ì •ì ì¸ ê³µê°í˜• ëŒ€í™”
- ê¸°ìˆ ìƒë‹´: ì „ë¬¸ì ì´ê³  ë‹¨ê³„ë³„ ì„¤ëª…
- ì¼ë°˜ëŒ€í™”: ìƒí™©ì— ë§ëŠ” ì ì ˆí•œ í†¤ê³¼ ìŠ¤íƒ€ì¼

ì‚¬ìš©ìì˜ ì…ë ¥ ìœ í˜•ì„ íŒŒì•…í•˜ê³  ê·¸ì— ë§ëŠ” ìµœì ì˜ ì‘ë‹µ ìŠ¤íƒ€ì¼ì„ ì„ íƒí•˜ì„¸ìš”."""
    
    def load_model(self) -> bool:
        """í†µí•© ëª¨ë¸ ë¡œë“œ"""
        if not self.model_path.exists():
            logger.error(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.model_path}")
            return False
        
        try:
            logger.info(f"í†µí•© ëª¨ë¸ ë¡œë”©: {self.model_path}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(str(self.model_path))
            self.model = AutoModelForCausalLM.from_pretrained(
                str(self.model_path),
                torch_dtype=torch.float16
            )
            self.model.to(self.device)
            self.model.eval()
            
            logger.info("í†µí•© ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def detect_conversation_type(self, text: str) -> str:
        """ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ëŒ€í™” ìœ í˜• ì¶”ì • (íœ´ë¦¬ìŠ¤í‹±)"""
        text_lower = text.lower()
        
        # ì§ˆë¬¸ íŒ¨í„´
        question_patterns = ['?', 'ë¬´ì—‡', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””ì„œ', 'ëˆ„ê°€', 'ë­', 'ì–´ë–¤']
        if any(pattern in text_lower for pattern in question_patterns):
            return "ì§ˆë¬¸ë‹µë³€"
        
        # ê¸°ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ
        tech_keywords = ['í”„ë¡œê·¸ë˜ë°', 'ì½”ë”©', 'íŒŒì´ì¬', 'python', 'cad', 'ìŠ¤ì¼€ì¹˜ì—…', 'ì†Œí”„íŠ¸ì›¨ì–´', 'ë²„ê·¸', 'ì˜¤ë¥˜']
        if any(keyword in text_lower for keyword in tech_keywords):
            return "ê¸°ìˆ ìƒë‹´"
        
        # ì¼ìƒ ëŒ€í™” íŒ¨í„´
        daily_patterns = ['ì•ˆë…•', 'ë°˜ê°€ì›Œ', 'ê¸°ë¶„', 'ë‚ ì”¨', 'ì·¨ë¯¸', 'ì¢‹ì•„í•´', 'ì‹«ì–´í•´']
        if any(pattern in text_lower for pattern in daily_patterns):
            return "ì¼ìƒëŒ€í™”"
        
        return "ì¼ë°˜ëŒ€í™”"
    
    def generate_response(self, user_input: str, 
                         conversation_type: Optional[str] = None,
                         max_length: int = 256, 
                         temperature: float = 0.8) -> Dict[str, str]:
        """í†µí•© ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.model or not self.tokenizer:
            if not self.load_model():
                return {"error": "ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        try:
            # ëŒ€í™” ìœ í˜• ìë™ ê°ì§€ (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
            if conversation_type is None:
                conversation_type = self.detect_conversation_type(user_input)
            
            # ì…ë ¥ í¬ë§·íŒ…
            formatted_input = f"ì‹œìŠ¤í…œ: {self.system_prompt}\n\nì‚¬ìš©ì: {user_input}\nì–´ì‹œìŠ¤í„´íŠ¸:"
            
            # í† í¬ë‚˜ì´ì§•
            input_ids = self.tokenizer.encode(formatted_input, return_tensors="pt").to(self.device)
            
            # ìƒì„±
            with torch.no_grad():
                generated_ids = self.model.generate(
                    input_ids,
                    max_new_tokens=max_length,
                    do_sample=True,
                    temperature=temperature,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # ë””ì½”ë”© (ì…ë ¥ ë¶€ë¶„ ì œì™¸)
            generated_text = self.tokenizer.decode(
                generated_ids[0][len(input_ids[0]):], 
                skip_special_tokens=True
            ).strip()
            
            return {
                "response": generated_text,
                "detected_type": conversation_type,
                "model_type": "unified"
            }
            
        except Exception as e:
            logger.error(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return {"error": f"ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}"}
    
    def interactive_chat(self):
        """ëŒ€í™”í˜• ì±„íŒ…"""
        if not self.load_model():
            print("âŒ ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*50)
        print("ğŸ¤– í†µí•©í˜• AI ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("="*50)
        print("ğŸ’¡ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”!")
        print("   - ì§ˆë¬¸: 'íŒŒì´ì¬ì´ ë­ì•¼?'")
        print("   - ì¼ìƒ: 'ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œ?'") 
        print("   - ê¸°ìˆ : 'ì½”ë”© ì—ëŸ¬ë¥¼ ì–´ë–»ê²Œ ì°¾ì•„?'")
        print("ì¢…ë£Œ: 'quit' ì…ë ¥")
        print()
        
        conversation_count = 0
        
        while True:
            try:
                conversation_count += 1
                user_input = input(f"ğŸ’¬ #{conversation_count} ì‚¬ìš©ì: ").strip()
                
                if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                    break
                
                if not user_input:
                    continue
                
                # ì‘ë‹µ ìƒì„±
                print("ğŸ¤” ìƒê° ì¤‘...")
                result = self.generate_response(user_input)
                
                if "error" in result:
                    print(f"âŒ ì˜¤ë¥˜: {result['error']}")
                    continue
                
                print(f"ğŸ” ê°ì§€ëœ ëŒ€í™” ìœ í˜•: {result['detected_type']}")
                print(f"ğŸ¤– AI: {result['response']}")
                print("-" * 50)
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"ì±„íŒ… ì˜¤ë¥˜: {e}")
                continue
        
        print("\nğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    
    def batch_test(self, test_inputs: List[str]):
        """ë°°ì¹˜ í…ŒìŠ¤íŠ¸"""
        if not self.load_model():
            print("âŒ ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return
        
        logger.info("ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        results = []
        
        for i, text in enumerate(test_inputs, 1):
            print(f"\n[{i}/{len(test_inputs)}] í…ŒìŠ¤íŠ¸: {text}")
            
            result = self.generate_response(text)
            result['input'] = text
            results.append(result)
            
            if "error" not in result:
                print(f"ìœ í˜•: {result['detected_type']}")
                print(f"ì‘ë‹µ: {result['response']}")
            else:
                print(f"ì˜¤ë¥˜: {result['error']}")
            
            print("-" * 50)
        
        # ê²°ê³¼ ì €ì¥
        output_path = Path("models/unified_model/batch_test_results.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_path}")

def main():
    parser = argparse.ArgumentParser(description="í†µí•©í˜• ëª¨ë¸ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--model_path", default="models/unified_model", help="ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--mode", choices=["interactive", "batch"], default="interactive", help="ì‹¤í–‰ ëª¨ë“œ")
    parser.add_argument("--test_inputs", nargs="+", help="ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì…ë ¥ë“¤")
    
    args = parser.parse_args()
    
    inference = UnifiedModelInference(args.model_path)
    
    if args.mode == "interactive":
        inference.interactive_chat()
    else:
        if not args.test_inputs:
            test_inputs = [
                "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì˜¤ëŠ˜ ê¸°ë¶„ì´ ì–´ë–¤ê°€ìš”?",
                "CAD ì†Œí”„íŠ¸ì›¨ì–´ì—ì„œ 3D ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
                "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤"
            ]
        else:
            test_inputs = args.test_inputs
        
        inference.batch_test(test_inputs)

if __name__ == "__main__":
    main()
