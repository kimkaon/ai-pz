#!/usr/bin/env python3
"""
íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ GGUF í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
llama-cpp-pythonê³¼ í˜¸í™˜ë˜ë„ë¡ ë³€í™˜
"""

import os
import sys
import subprocess
import argparse
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def setup_llama_cpp():
    """llama.cpp ì„¤ì • ë° ì„¤ì¹˜"""
    logger.info("=== llama.cpp ì„¤ì • ===")
    
    # llama.cpp í´ë¡  (ì´ë¯¸ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸)
    llama_cpp_dir = Path("llama.cpp")
    if not llama_cpp_dir.exists():
        logger.info("llama.cpp í´ë¡  ì¤‘...")
        subprocess.run([
            "git", "clone", "https://github.com/ggerganov/llama.cpp.git"
        ], check=True)
    else:
        logger.info("llama.cpp ì´ë¯¸ ì¡´ì¬í•¨")
    
    # ì˜ì¡´ì„± í™•ì¸
    try:
        import torch
        import transformers
        logger.info("âœ… PyTorch ë° Transformers í™•ì¸ë¨")
    except ImportError as e:
        logger.error(f"âŒ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ëˆ„ë½: {e}")
        return False
    
    return True

def convert_to_gguf(model_path, output_path, quantization="q5_k_m"):
    """
    HuggingFace ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜
    
    Args:
        model_path: íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ
        output_path: ì¶œë ¥ GGUF íŒŒì¼ ê²½ë¡œ
        quantization: ì–‘ìí™” íƒ€ì… (q4_0, q4_1, q5_0, q5_1, q5_k_m, q8_0)
    """
    logger.info(f"ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path} -> {output_path}")
    
    model_path = Path(model_path)
    output_path = Path(output_path)
    
    if not model_path.exists():
        logger.error(f"âŒ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        return False
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        # Step 1: HF to GGML ë³€í™˜
        temp_ggml = output_path.with_suffix('.ggml')
        logger.info("Step 1: HuggingFace -> GGML ë³€í™˜")
        
        convert_cmd = [
            sys.executable, "llama.cpp/convert.py",
            str(model_path),
            "--outtype", "f16",
            "--outfile", str(temp_ggml)
        ]
        
        result = subprocess.run(convert_cmd, capture_output=True, text=True)
        if result.returncode != 0:
            logger.error(f"GGML ë³€í™˜ ì‹¤íŒ¨: {result.stderr}")
            return False
        
        # Step 2: GGML to GGUF ì–‘ìí™”
        logger.info(f"Step 2: GGML -> GGUF ì–‘ìí™” ({quantization})")
        
        # llama.cpp ë¹Œë“œ (Windows)
        if os.name == 'nt':  # Windows
            quantize_exe = "llama.cpp/quantize.exe"
            if not Path(quantize_exe).exists():
                logger.info("quantize.exe ë¹Œë“œ ì¤‘...")
                build_cmd = ["cmake", "-B", "llama.cpp/build", "-S", "llama.cpp"]
                subprocess.run(build_cmd)
                make_cmd = ["cmake", "--build", "llama.cpp/build", "--config", "Release"]
                subprocess.run(make_cmd)
        else:
            quantize_exe = "llama.cpp/quantize"
        
        quantize_cmd = [
            quantize_exe,
            str(temp_ggml),
            str(output_path),
            quantization
        ]
        
        result = subprocess.run(quantize_cmd, capture_output=True, text=True)
        if result.returncode != 0:
            logger.error(f"GGUF ì–‘ìí™” ì‹¤íŒ¨: {result.stderr}")
            return False
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if temp_ggml.exists():
            temp_ggml.unlink()
        
        logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {output_path}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        size_mb = output_path.stat().st_size / (1024 * 1024)
        logger.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {size_mb:.1f}MB")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ GGUFë¡œ ë³€í™˜")
    parser.add_argument("--model_path", required=True, help="íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--output_path", required=True, help="ì¶œë ¥ GGUF íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--quantization", default="q5_k_m", 
                       choices=["q4_0", "q4_1", "q5_0", "q5_1", "q5_k_m", "q8_0"],
                       help="ì–‘ìí™” íƒ€ì…")
    
    args = parser.parse_args()
    
    # llama.cpp ì„¤ì •
    if not setup_llama_cpp():
        logger.error("llama.cpp ì„¤ì • ì‹¤íŒ¨")
        return 1
    
    # ë³€í™˜ ì‹¤í–‰
    success = convert_to_gguf(args.model_path, args.output_path, args.quantization)
    
    if success:
        logger.info("ğŸ‰ ëª¨ë¸ ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info(f"ë©”ì¸ í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´:")
        logger.info(f"  - íŒŒì¼ ê²½ë¡œ: {args.output_path}")
        logger.info(f"  - nous_hermes2_mistral_loader.pyì—ì„œ model_path ìˆ˜ì •")
        return 0
    else:
        logger.error("âŒ ëª¨ë¸ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return 1

if __name__ == "__main__":
    sys.exit(main())
