#!/usr/bin/env python3
"""
ì˜ì–´ ëŒ€í™” ë°ì´í„° ê¸°ë°˜ ê°„ë‹¨í•œ íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
- PyTorch ê¸°ë³¸ DataLoader ì‚¬ìš©
- TensorFlow/datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜ì¡´ì„± ì—†ìŒ
- ì˜ì–´ ëŒ€í™”/QnA ë°ì´í„°ì…‹ìœ¼ë¡œ í†µí•© ëª¨ë¸ ìƒì„±
"""

import os
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    get_linear_schedule_with_warmup
)
from peft import LoraConfig, get_peft_model, TaskType
from tqdm import tqdm
import warnings

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

class EnglishChatDataset(Dataset):
    """ì˜ì–´ ëŒ€í™” ë°ì´í„°ì…‹ í´ë˜ìŠ¤"""
    
    def __init__(self, data_files, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        # ì—¬ëŸ¬ JSONL íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ
        for file_path in data_files:
            if os.path.exists(file_path):
                print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {file_path}")
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            try:
                                item = json.loads(line)
                                if 'input_text' in item and 'target_text' in item:
                                    self.data.append(item)
                            except json.JSONDecodeError:
                                continue
        
        print(f"âœ… ì´ {len(self.data)}ê°œ ë°ì´í„° ë¡œë“œë¨")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # Mistral/Nous-Hermes í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        text = f"<|im_start|>user\n{item['input_text']}<|im_end|>\n<|im_start|>assistant\n{item['target_text']}<|im_end|>"
        
        # í† í°í™”
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

def load_english_datasets():
    """ì˜ì–´ ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ ë°˜í™˜ - processed_englishì˜ ëª¨ë“  ë°ì´í„° í™œìš©"""
    # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ì— ê´€ê³„ì—†ì´ ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "datasets", "processed_english")
    
    print(f"ğŸ“ ë°ì´í„°ì…‹ ê¸°ë³¸ ê²½ë¡œ: {base_path}")
    
    # ëª¨ë“  ì˜ì–´ í›ˆë ¨ ë°ì´í„° íŒŒì¼ í¬í•¨
    train_files = [
        os.path.join(base_path, "unified_train.jsonl"),
        os.path.join(base_path, "dialogue_qna_train.jsonl"),
        os.path.join(base_path, "dialogue_general_train.jsonl"),
        os.path.join(base_path, "dialogue_technical_train.jsonl"),
        os.path.join(base_path, "classification_train.jsonl")
    ]
    
    # ëª¨ë“  ì˜ì–´ ê²€ì¦ ë°ì´í„° íŒŒì¼ í¬í•¨
    val_files = [
        os.path.join(base_path, "unified_validation.jsonl"),
        os.path.join(base_path, "dialogue_qna_validation.jsonl"),
        os.path.join(base_path, "dialogue_general_validation.jsonl"),
        os.path.join(base_path, "dialogue_technical_validation.jsonl"),
        os.path.join(base_path, "classification_validation.jsonl")
    ]
    
    return train_files, val_files

def main():
    print("ğŸš€ ì˜ì–´ ëŒ€í™” íŒŒì¸íŠœë‹ ì‹œì‘")
    print("="*50)
    
    # GPU í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ - Nous-Hermes-2-Mistral-7B-DPO ì‚¬ìš©
    model_name = "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"  # ì›ë³¸ HuggingFace ëª¨ë¸
    print(f"ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ë¡œë”©: {model_name}")
    print("âš ï¸  ì£¼ì˜: ì´ ëª¨ë¸ì€ í¬ê¸°ê°€ í½ë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œì— ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}")
        
        # LoRA ì„¤ì • ì ìš© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        print("ğŸ”§ LoRA ì„¤ì • ì ìš© ì¤‘...")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,  # rank
            lora_alpha=32,  # scaling parameter
            lora_dropout=0.1,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]  # Mistral íƒ€ê²Ÿ ëª¨ë“ˆ
        )
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê³  HuggingFace Hub ì ‘ê·¼ì´ ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    train_files, val_files = load_english_datasets()
    
    train_dataset = EnglishChatDataset(train_files, tokenizer)
    val_dataset = EnglishChatDataset(val_files, tokenizer)
    
    if len(train_dataset) == 0:
        print("âŒ í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„°ë¡œë” ìƒì„± - 7B ëª¨ë¸ì— ë§ê²Œ ë°°ì¹˜ í¬ê¸° ì¡°ì •
    batch_size = 1 if torch.cuda.is_available() else 1  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì‘ì€ ë°°ì¹˜
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # ìµœì í™” ì„¤ì • - 7B ëª¨ë¸ì— ë§ê²Œ ì¡°ì •
    num_epochs = 2  # ì—í¬í¬ ìˆ˜ ì¤„ì„
    learning_rate = 2e-4  # LoRAì— ë§ëŠ” ë” ë†’ì€ í•™ìŠµë¥ 
    gradient_accumulation_steps = 4  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ìœ¼ë¡œ íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ê°€
    
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    total_steps = len(train_loader) * num_epochs // gradient_accumulation_steps
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=max(1, total_steps // 10),  # ì „ì²´ ìŠ¤í…ì˜ 10%ë¥¼ ì›Œë°ì—…ìœ¼ë¡œ
        num_training_steps=total_steps
    )
    
    print(f"ğŸ¯ í›ˆë ¨ ì„¤ì •:")
    print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…: {gradient_accumulation_steps}")
    print(f"   íš¨ê³¼ì  ë°°ì¹˜ í¬ê¸°: {batch_size * gradient_accumulation_steps}")
    print(f"   ì—í¬í¬: {num_epochs}")
    print(f"   í•™ìŠµë¥ : {learning_rate}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    
    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    model.train()
    
    for epoch in range(num_epochs):
        print(f"\nğŸ“š ì—í¬í¬ {epoch + 1}/{num_epochs}")
        
        total_loss = 0
        progress_bar = tqdm(train_loader, desc=f"í›ˆë ¨ ì¤‘")
        
        for step, batch in enumerate(progress_bar):
            
            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # ìˆœì „íŒŒ
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss / gradient_accumulation_steps  # ëˆ„ì ì„ ìœ„í•´ ë‚˜ëˆ„ê¸°
            total_loss += loss.item()
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
            if (step + 1) % gradient_accumulation_steps == 0:
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
            
            progress_bar.set_postfix({'loss': loss.item() * gradient_accumulation_steps})
        
        avg_loss = total_loss / len(train_loader)
        print(f"ğŸ“Š í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
        
        # ê²€ì¦
        if len(val_dataset) > 0:
            model.eval()
            val_loss = 0
            
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    labels = batch['labels'].to(device)
                    
                    outputs = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    
                    val_loss += outputs.loss.item()
            
            avg_val_loss = val_loss / len(val_loader)
            print(f"ğŸ” ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
            
            model.train()
    
    # ëª¨ë¸ ì €ì¥ - LoRA ì–´ëŒ‘í„°ì™€ í† í¬ë‚˜ì´ì € ì €ì¥
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_dir = os.path.join(script_dir, "models", "english_unified")
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nğŸ’¾ LoRA ëª¨ë¸ ì €ì¥: {output_dir}")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    
    # ì„¤ì • íŒŒì¼ ì €ì¥
    config_info = {
        "base_model": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
        "training_data": "processed_english datasets",
        "model_type": "LoRA fine-tuned",
        "epochs": num_epochs,
        "learning_rate": learning_rate,
        "batch_size": batch_size,
        "gradient_accumulation_steps": gradient_accumulation_steps
    }
    
    with open(f"{output_dir}/training_config.json", "w", encoding="utf-8") as f:
        json.dump(config_info, f, indent=2)
    
    # í…ŒìŠ¤íŠ¸ ìƒì„±
    print(f"\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    model.eval()
    
    test_prompts = [
        "What is artificial intelligence?",
        "How does machine learning work?",
        "Explain deep learning briefly.",
        "What are the benefits of renewable energy?",
        "How do neural networks learn?"
    ]
    
    with torch.no_grad():
        for prompt in test_prompts:
            print(f"\nì…ë ¥: {prompt}")
            
            # Mistral/Nous-Hermes í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ í† í°í™”
            input_text = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            inputs = tokenizer(input_text, return_tensors='pt').to(device)
            
            # ì‘ë‹µ ìƒì„±
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
            
            # ì‘ë‹µ ë””ì½”ë”©
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            assistant_response = response.split("<|im_start|>assistant\n")[-1].replace("<|im_end|>", "").strip()
            
            print(f"ì‘ë‹µ: {assistant_response}")
    
    print(f"\nğŸ‰ ì˜ì–´ ëŒ€í™” íŒŒì¸íŠœë‹ ì™„ë£Œ!")
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    print("="*50)

if __name__ == "__main__":
    main()
