#!/usr/bin/env python3
"""
RTX 3070 8GB ìµœì í™”: ì‘ì€ ëª¨ë¸ 5ì‹œê°„ ì—°ì† í•™ìŠµ
"""

import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForCausalLM, get_cosine_schedule_with_warmup, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, TaskType
from tqdm import tqdm
import gc
import warnings

warnings.filterwarnings("ignore")

def clear_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

class SimpleDataset(Dataset):
    def __init__(self, data_files, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = []
        
        for file_path in data_files:
            if os.path.exists(file_path):
                print(f"ğŸ“‚ ë°ì´í„° ë¡œë”©: {file_path}")
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            try:
                                item = json.loads(line)
                                if 'input_text' in item and 'target_text' in item:
                                    # í’ˆì§ˆ í•„í„°ë§: ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë°ì´í„° ì œì™¸
                                    input_len = len(item['input_text'].strip())
                                    target_len = len(item['target_text'].strip())
                                    
                                    if 5 <= input_len <= 300 and 10 <= target_len <= 500:
                                        self.data.append(item)
                            except json.JSONDecodeError:
                                continue
        
        print(f"âœ… ì´ {len(self.data)}ê°œ ê³ í’ˆì§ˆ ë°ì´í„° ë¡œë“œë¨")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ê°œì„ ëœ ì‘ë‹µ ì „ì²˜ë¦¬
        input_text = item['input_text'].strip()
        target_text = item['target_text'].strip()
        
        # 1. ë„ˆë¬´ ê¸´ ì‘ë‹µ ì œí•œ (í’ˆì§ˆ í–¥ìƒ)
        sentences = target_text.split('. ')
        if len(sentences) > 3:
            # ì˜ë¯¸ ìˆëŠ” ë§ˆë¬´ë¦¬ë¥¼ ìœ„í•´ 3ë¬¸ì¥ìœ¼ë¡œ ì œí•œ
            target_text = '. '.join(sentences[:3])
            if not target_text.endswith('.'):
                target_text += '.'
        
        # 2. ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” í˜•ì‹ (Mistral ìµœì í™”)
        text = f"<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n{target_text}<|im_end|>"
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # 3. ê°œì„ ëœ ë¼ë²¨ ë§ˆìŠ¤í‚¹ (ì‚¬ìš©ì ì…ë ¥ ë¶€ë¶„ë§Œ ë§ˆìŠ¤í‚¹)
        labels = encoding['input_ids'].clone()
        
        # assistant ë¶€ë¶„ë§Œ í•™ìŠµí•˜ë„ë¡ ì •í™•í•œ ë§ˆìŠ¤í‚¹
        assistant_start = text.find("<|im_start|>assistant\n")
        if assistant_start != -1:
            user_part = text[:assistant_start + len("<|im_start|>assistant\n")]
            user_tokens = self.tokenizer.encode(user_part, add_special_tokens=False)
            
            if len(user_tokens) < labels.shape[-1]:
                labels[0][:len(user_tokens)] = -100
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': labels.flatten()
        }

def main():
    print("ğŸš€ RTX 3070 8GB ìµœì í™”: 5ì‹œê°„ ì—°ì† í•™ìŠµ")
    print("="*60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {device}")
    
    # ì„±ê³µ ê²€ì¦ëœ Mistral 7B ëª¨ë¸ ì‚¬ìš© + 4bit ì–‘ìí™”
    model_name = "NousResearch/Nous-Hermes-2-Mistral-7B-DPO"
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
    
    try:
        # 4bit ì–‘ìí™”ë¡œ RTX 3070ì— ìµœì í™”
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # Mistral 7B ìµœì í™” LoRA ì„¤ì •
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,  # ë†’ì€ rank (í’ˆì§ˆ ìš°ì„ )
            lora_alpha=32,
            lora_dropout=0.05,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # Mistral ì „ìš© ëª¨ë“ˆ
            bias="none"
        )
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        model.train()
        
        print("âœ… LoRA ì„¤ì • ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    script_dir = os.path.dirname(os.path.abspath(__file__))
    base_path = os.path.join(script_dir, "datasets", "processed_english")
    
    train_files = [
        os.path.join(base_path, "unified_train.jsonl"),
        os.path.join(base_path, "dialogue_qna_train.jsonl"),
        os.path.join(base_path, "dialogue_general_train.jsonl"),
        os.path.join(base_path, "dialogue_technical_train.jsonl"),
        os.path.join(base_path, "classification_train.jsonl")
    ]
    
    dataset = SimpleDataset(train_files, tokenizer, max_length=384)  # ìµœì í™”ëœ ê¸¸ì´
    
    # RTX 3070 8GB ìµœì í™”: batch_size 1 + gradient accumulation
    batch_size = 1
    gradient_accumulation_steps = 4  # ì‹¤ì§ˆì ìœ¼ë¡œ batch_size 4ì™€ ë™ì¼
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    
    # 5ì‹œê°„ ìµœì í™” ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± + ì•ˆì •ì„±)
    num_epochs = 6  # ë” íš¨ìœ¨ì ì¸ ì—í¬í¬ ìˆ˜
    learning_rate = 8e-5  # ì•ˆì •ì ì¸ í•™ìŠµë¥ 
    max_grad_norm = 1.0  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
    
    optimizer = AdamW(
        [p for p in model.parameters() if p.requires_grad], 
        lr=learning_rate, 
        weight_decay=0.01,
        eps=1e-8
    )
    
    total_steps = len(dataloader) * num_epochs // gradient_accumulation_steps
    warmup_steps = total_steps // 20
    scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )
    
    print(f"ğŸ¯ RTX 3070 ìµœì í™” ì„¤ì •:")
    print(f"   ì—í¬í¬: {num_epochs}")
    print(f"   ì‹¤ì§ˆ ë°°ì¹˜ í¬ê¸°: {batch_size * gradient_accumulation_steps}")
    print(f"   í•™ìŠµë¥ : {learning_rate}")
    print(f"   ì´ ìŠ¤í…: {total_steps}")
    print(f"   ë°ì´í„°: {len(dataset)} ìƒ˜í”Œ")
    print(f"   ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: 384")
    
    # ê²€ì¦ìš© ë°ì´í„° ë¡œë“œ (í’ˆì§ˆ ëª¨ë‹ˆí„°ë§)
    val_files = [
        os.path.join(base_path, "unified_validation.jsonl"),
        os.path.join(base_path, "dialogue_qna_validation.jsonl")
    ]
    val_dataset = SimpleDataset(val_files, tokenizer, max_length=384)
    val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False) if len(val_dataset) > 0 else None
    
    best_val_loss = float('inf')
    
    # ê³ í’ˆì§ˆ í›ˆë ¨ ì‹œì‘
    model.train()
    for epoch in range(num_epochs):
        print(f"\nğŸ“š ì—í¬í¬ {epoch + 1}/{num_epochs}")
        
        total_loss = 0
        valid_batches = 0
        progress_bar = tqdm(dataloader, desc=f"í›ˆë ¨ ì¤‘ (ì—í¬í¬ {epoch+1})")
        
        for step, batch in enumerate(progress_bar):
            
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            
            if loss is None or not torch.isfinite(loss):
                continue
            
            # Gradient accumulationìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
            loss = loss / gradient_accumulation_steps
            total_loss += loss.item()
            valid_batches += 1
            
            loss.backward()
            
            # Gradient accumulation ë‹¨ê³„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
            if (step + 1) % gradient_accumulation_steps == 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                clear_memory()
            
            # ì§„í–‰ë¥  í‘œì‹œ
            current_lr = scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                'loss': f"{loss.item() * gradient_accumulation_steps:.4f}",
                'lr': f"{current_lr:.2e}"
            })
            
            if step % 20 == 0:
                clear_memory()
        
        avg_loss = total_loss / max(valid_batches, 1)
        print(f"ğŸ“Š í‰ê·  í›ˆë ¨ ì†ì‹¤: {avg_loss:.4f}")
        
        # ê²€ì¦ (í’ˆì§ˆ ëª¨ë‹ˆí„°ë§)
        if val_dataloader and len(val_dataset) > 0:
            model.eval()
            val_loss = 0
            val_steps = 0
            
            with torch.no_grad():
                for batch in tqdm(val_dataloader, desc="ê²€ì¦ ì¤‘"):
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    labels = batch['labels'].to(device)
                    
                    outputs = model(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        labels=labels
                    )
                    
                    if outputs.loss is not None and torch.isfinite(outputs.loss):
                        val_loss += outputs.loss.item()
                        val_steps += 1
            
            if val_steps > 0:
                avg_val_loss = val_loss / val_steps
                print(f"ï¿½ ê²€ì¦ ì†ì‹¤: {avg_val_loss:.4f}")
                
                # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
                if avg_val_loss < best_val_loss:
                    best_val_loss = avg_val_loss
                    print("âœ… ê²€ì¦ ì†ì‹¤ ê°œì„ ! ìµœê³  ëª¨ë¸ ì €ì¥...")
                    output_dir = "models/rtx3070_optimized_best"
                    os.makedirs(output_dir, exist_ok=True)
                    model.save_pretrained(output_dir)
                    tokenizer.save_pretrained(output_dir)
            
            model.train()
        
        # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % 2 == 0:
            output_dir = f"models/rtx3070_checkpoint_epoch_{epoch+1}"
            os.makedirs(output_dir, exist_ok=True)
            model.save_pretrained(output_dir)
            print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {output_dir}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_dir = "models/rtx3070_optimized_final"
    os.makedirs(final_dir, exist_ok=True)
    model.save_pretrained(final_dir)
    tokenizer.save_pretrained(final_dir)
    
    # í›ˆë ¨ ê²°ê³¼ ì •ë³´ ì €ì¥
    result_info = {
        "model": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
        "training_type": "RTX 3070 ìµœì í™” LoRA íŒŒì¸íŠœë‹",
        "epochs": num_epochs,
        "learning_rate": learning_rate,
        "batch_size": batch_size,
        "gradient_accumulation_steps": gradient_accumulation_steps,
        "max_length": 384,
        "lora_r": 16,
        "lora_alpha": 32,
        "best_validation_loss": best_val_loss,
        "optimizations": [
            "4bit ì–‘ìí™”",
            "Mistral ì „ìš© LoRA ëª¨ë“ˆ",
            "Gradient accumulation",
            "ê°œì„ ëœ ë¼ë²¨ ë§ˆìŠ¤í‚¹",
            "ê²€ì¦ ê¸°ë°˜ ìµœê³  ëª¨ë¸ ì €ì¥"
        ]
    }
    
    with open(f"{final_dir}/training_info.json", "w", encoding="utf-8") as f:
        json.dump(result_info, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ‰ RTX 3070 ìµœì í™” í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ ìµœì¢… ëª¨ë¸: {final_dir}")
    print(f"ğŸ“ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: models/rtx3070_optimized_best")
    print(f"ğŸ† ìµœê³  ê²€ì¦ ì†ì‹¤: {best_val_loss:.4f}")
    print("="*60)

if __name__ == "__main__":
    main()
