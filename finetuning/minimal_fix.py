#!/usr/bin/env python3
"""
ìµœì†Œ ì„¤ì • íŒŒì¸íŠœë‹ - ê·¸ë˜ë””ì–¸íŠ¸ ë¬¸ì œ í•´ê²°ìš©
"""

import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType
from tqdm import tqdm
import gc
import warnings

warnings.filterwarnings("ignore")

def clear_memory():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

class SimpleDataset(Dataset):
    def __init__(self, data, tokenizer, max_length=128):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.data = data[:10]  # ìµœì†Œ ë°ì´í„°ë§Œ ì‚¬ìš©
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # ê°„ë‹¨í•œ í˜•ì‹
        text = f"User: {item['input_text']}\nAssistant: {item['target_text']}"
        
        # í† í°í™”
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()
        }

def main():
    print("ğŸš€ ìµœì†Œ ì„¤ì • íŒŒì¸íŠœë‹ ì‹œì‘")
    
    # GPU í™•ì¸
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {device}")
    
    # ê°€ì¥ ì‘ì€ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
    model_name = "microsoft/DialoGPT-small"
    print(f"ğŸ“¥ ëª¨ë¸ ë¡œë”©: {model_name}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ìµœì†Œ ì„¤ì •
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,  # float32 ì‚¬ìš©
            device_map={"": 0}
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ìµœì†Œ LoRA ì„¤ì •
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=2,  # ìµœì†Œ rank
            lora_alpha=4,
            lora_dropout=0.1,
            target_modules=["c_attn"],  # DialoGPTì˜ ì–´í…ì…˜ ëª¨ë“ˆ
            bias="none"
        )
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
        # í›ˆë ¨ ëª¨ë“œ
        model.train()
        
        print("âœ… LoRA ì„¤ì • ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ê°€ì§œ ë°ì´í„° ìƒì„±
    fake_data = [
        {"input_text": "What is AI?", "target_text": "AI is artificial intelligence."},
        {"input_text": "How are you?", "target_text": "I am fine, thank you."},
        {"input_text": "What is Python?", "target_text": "Python is a programming language."},
        {"input_text": "Hello", "target_text": "Hello! How can I help you?"},
        {"input_text": "Goodbye", "target_text": "Goodbye! Have a nice day!"}
    ]
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = SimpleDataset(fake_data, tokenizer)
    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)
    
    # ìµœì†Œ í›ˆë ¨ ì„¤ì •
    optimizer = AdamW(model.parameters(), lr=1e-4)
    num_epochs = 1
    
    print(f"ğŸ¯ í›ˆë ¨ ì‹œì‘ (ì—í¬í¬: {num_epochs}, ë°ì´í„°: {len(dataset)})")
    
    # í›ˆë ¨
    for epoch in range(num_epochs):
        print(f"\nğŸ“š ì—í¬í¬ {epoch + 1}/{num_epochs}")
        
        total_loss = 0
        for step, batch in enumerate(tqdm(dataloader, desc="í›ˆë ¨ ì¤‘")):
            
            # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            # ìˆœì „íŒŒ
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            total_loss += loss.item()
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            optimizer.step()
            optimizer.zero_grad()
            
            clear_memory()
        
        avg_loss = total_loss / len(dataloader)
        print(f"ğŸ“Š í‰ê·  ì†ì‹¤: {avg_loss:.4f}")
    
    # í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸")
    model.eval()
    
    test_input = "What is machine learning?"
    inputs = tokenizer(f"User: {test_input}\nAssistant:", return_tensors='pt').to(device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=20,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"ì…ë ¥: {test_input}")
    print(f"ì‘ë‹µ: {response}")
    
    print("\nğŸ‰ ìµœì†Œ ì„¤ì • íŒŒì¸íŠœë‹ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
