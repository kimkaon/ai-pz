#!/usr/bin/env python3
"""
GPU ê¸°ë°˜ íŒŒì¸íŠœë‹ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Nous-Hermes-2-Mistral ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê°„ë‹¨í•œ íŒŒì¸íŠœë‹ í…ŒìŠ¤íŠ¸
"""

import torch
import os
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model, TaskType
import json

def check_gpu_setup():
    """GPU ì„¤ì • í™•ì¸"""
    print("=== GPU ì„¤ì • í™•ì¸ ===")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU count: {torch.cuda.device_count()}")
        print(f"Current GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        
        # CUDA ë©”ëª¨ë¦¬ ì •ë¦¬
        torch.cuda.empty_cache()
        print(f"Available GPU memory: {torch.cuda.memory_reserved(0) / 1e9:.1f} GB")
    else:
        print("âš ï¸ CUDA not available! CPU will be used.")
    
    print("=" * 50)

def create_sample_dataset():
    """íŒŒì¸íŠœë‹ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±"""
    # í•œêµ­ì–´ ëŒ€í™” ë°ì´í„° ìƒ˜í”Œ
    sample_data = [
        {
            "instruction": "ì•ˆë…•í•˜ì„¸ìš”",
            "input": "",
            "output": "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        },
        {
            "instruction": "ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì–´ë•Œìš”?",
            "input": "",
            "output": "ì£„ì†¡í•˜ì§€ë§Œ ì‹¤ì‹œê°„ ë‚ ì”¨ ì •ë³´ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚ ì”¨ ì•±ì´ë‚˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
        },
        {
            "instruction": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
            "input": "",
            "output": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ë ¬í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤:\n1. sort() ë©”ì„œë“œ: my_list.sort() - ì›ë³¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³€ê²½\n2. sorted() í•¨ìˆ˜: sorted(my_list) - ìƒˆë¡œìš´ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"
        },
        {
            "instruction": "ê°ì‚¬í•©ë‹ˆë‹¤",
            "input": "",
            "output": "ì²œë§Œì—ìš”! ì–¸ì œë“ ì§€ ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”."
        }
    ]
    
    # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    formatted_data = []
    for item in sample_data:
        if item["input"]:
            text = f"### ì§€ì‹œì‚¬í•­:\n{item['instruction']}\n\n### ì…ë ¥:\n{item['input']}\n\n### ì‘ë‹µ:\n{item['output']}"
        else:
            text = f"### ì§€ì‹œì‚¬í•­:\n{item['instruction']}\n\n### ì‘ë‹µ:\n{item['output']}"
        
        formatted_data.append({"text": text})
    
    return Dataset.from_list(formatted_data)

def test_model_loading():
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    print("=== ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ===")
    
    # Mistral ê¸°ë°˜ ëª¨ë¸ (í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë²„ì „)
    model_name = "mistralai/Mistral-7B-v0.1"
    
    try:
        print(f"í† í¬ë‚˜ì´ì € ë¡œë”©: {model_name}")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # íŒ¨ë”© í† í° ì„¤ì • (Mistralì€ ê¸°ë³¸ì ìœ¼ë¡œ íŒ¨ë”© í† í°ì´ ì—†ìŒ)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"ëª¨ë¸ ë¡œë”©: {model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        print(f"âœ“ ëª¨ë¸ ë¡œë”© ì„±ê³µ")
        print(f"âœ“ ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {model.num_parameters():,}")
        print(f"âœ“ ëª¨ë¸ ë””ë°”ì´ìŠ¤: {next(model.parameters()).device}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None

def setup_lora_config():
    """LoRA êµ¬ì„± ì„¤ì •"""
    lora_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=16,  # LoRA rank
        lora_alpha=32,  # LoRA scaling parameter
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # Mistral ì•„í‚¤í…ì²˜ì— ë§ëŠ” ëª¨ë“ˆ
    )
    return lora_config

def test_minimal_training(model, tokenizer, dataset):
    """ìµœì†Œí•œì˜ í›ˆë ¨ í…ŒìŠ¤íŠ¸"""
    print("=== ìµœì†Œ í›ˆë ¨ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # LoRA ì ìš©
        lora_config = setup_lora_config()
        model = get_peft_model(model, lora_config)
        
        print(f"âœ“ LoRA ì ìš© ì™„ë£Œ")
        print(f"âœ“ í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {model.num_parameters()}")
        
        # í† í¬ë‚˜ì´ì§•
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                padding="max_length",
                max_length=512,
                return_tensors="pt"
            )
        
        tokenized_dataset = dataset.map(tokenize_function, batched=True)
        
        # ë°ì´í„° ì½œë ‰í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # í›ˆë ¨ ì¸ì ì„¤ì • (ë§¤ìš° ê°€ë²¼ìš´ ì„¤ì •)
        training_args = TrainingArguments(
            output_dir="./test_training_output",
            overwrite_output_dir=True,
            num_train_epochs=1,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=1,
            learning_rate=5e-5,
            warmup_steps=10,
            logging_steps=1,
            save_strategy="no",
            evaluation_strategy="no",
            fp16=torch.cuda.is_available(),  # GPUê°€ ìˆìœ¼ë©´ fp16 ì‚¬ìš©
            dataloader_pin_memory=False,
        )
        
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=data_collator,
        )
        
        print("âœ“ íŠ¸ë ˆì´ë„ˆ ì„¤ì • ì™„ë£Œ")
        print("ğŸš€ í…ŒìŠ¤íŠ¸ í›ˆë ¨ ì‹œì‘...")
        
        # í›ˆë ¨ ì‹¤í–‰
        trainer.train()
        
        print("âœ… í…ŒìŠ¤íŠ¸ í›ˆë ¨ ì™„ë£Œ!")
        return True
        
    except Exception as e:
        print(f"âŒ í›ˆë ¨ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def test_inference(model, tokenizer):
    """ì¶”ë¡  í…ŒìŠ¤íŠ¸"""
    print("=== ì¶”ë¡  í…ŒìŠ¤íŠ¸ ===")
    
    try:
        test_prompt = "### ì§€ì‹œì‚¬í•­:\nì•ˆë…•í•˜ì„¸ìš”\n\n### ì‘ë‹µ:\n"
        
        inputs = tokenizer(test_prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ“ ìƒì„±ëœ ì‘ë‹µ:\n{response}")
        return True
        
    except Exception as e:
        print(f"âŒ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ GPU ê¸°ë°˜ íŒŒì¸íŠœë‹ í™˜ê²½ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # 1. GPU ì„¤ì • í™•ì¸
    check_gpu_setup()
    
    # 2. ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±
    print("ğŸ“ ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±...")
    dataset = create_sample_dataset()
    print(f"âœ“ ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
    
    # 3. ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    model, tokenizer = test_model_loading()
    if model is None:
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ë¡œ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
        return
    
    # 4. ìµœì†Œ í›ˆë ¨ í…ŒìŠ¤íŠ¸
    training_success = test_minimal_training(model, tokenizer, dataset)
    
    # 5. ì¶”ë¡  í…ŒìŠ¤íŠ¸
    if training_success:
        inference_success = test_inference(model, tokenizer)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ GPU ê¸°ë°˜ íŒŒì¸íŠœë‹ í™˜ê²½ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    if training_success:
        print("âœ… íŒŒì¸íŠœë‹ í™˜ê²½ì´ ì •ìƒì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("ğŸ’¡ ì´ì œ ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í™˜ê²½ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
