#!/usr/bin/env python3
"""
main.pyμ— νμΈνλ‹ λ¨λΈμ„ ν†µν•©ν•λ” ν—¬νΌ μ¤ν¬λ¦½νΈ
"""

import os
import json
import sys
from pathlib import Path

def check_finetuning_status():
    """νμΈνλ‹ μƒνƒ ν™•μΈ"""
    config_path = "finetuning_result.json"
    
    if not os.path.exists(config_path):
        print("β νμΈνλ‹μ΄ μ™„λ£λμ§€ μ•μ•μµλ‹λ‹¤.")
        print("π’΅ λ¨Όμ € 'python finetuning/integrated_pipeline.py'λ¥Ό μ‹¤ν–‰ν•μ„Έμ”.")
        return None
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    print("β… νμΈνλ‹ κ²°κ³Ό λ°κ²¬")
    print(f"π“… μƒμ„± μ‹κ°„: {config['created_at']}")
    print(f"π”§ λ¨λΈ νƒ€μ…: {config['model_type']}")
    print(f"β΅ GPU μ‚¬μ©: {config['gpu_available']}")
    
    return config

def create_main_integration():
    """main.py ν†µν•© μ½”λ“ μƒμ„±"""
    integration_code = '''
# =====================================================
# νμΈνλ‹ λ¨λΈ ν†µν•© μ½”λ“ (main.pyμ— μ¶”κ°€)
# =====================================================

# 1. ν•„μ”ν• λΌμ΄λΈλ¬λ¦¬ μ„ν¬νΈ (νμΌ μƒλ‹¨μ— μ¶”κ°€)
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    FINETUNED_AVAILABLE = True
except ImportError:
    FINETUNED_AVAILABLE = False
    print("β οΈ transformers λΌμ΄λΈλ¬λ¦¬κ°€ μ—†μ–΄ νμΈνλ‹ λ¨λΈμ„ μ‚¬μ©ν•  μ μ—†μµλ‹λ‹¤.")

# 2. νμΈνλ‹ λ¨λΈ λ΅λ” ν΄λμ¤
class FinetuningModelLoader:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.loaded = False
    
    def load_model(self, model_path="./finetuning/models/korean_chat"):
        """νμΈνλ‹λ λ¨λΈ λ΅λ“"""
        if not FINETUNED_AVAILABLE:
            return False
        
        try:
            if os.path.exists(model_path):
                print(f"π¤– νμΈνλ‹ λ¨λΈ λ΅λ”©: {model_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None
                )
                
                self.loaded = True
                print("β… νμΈνλ‹ λ¨λΈ λ΅λ“ μ™„λ£")
                return True
            else:
                print(f"β λ¨λΈ κ²½λ΅λ¥Ό μ°Ύμ„ μ μ—†μ: {model_path}")
                return False
        
        except Exception as e:
            print(f"β νμΈνλ‹ λ¨λΈ λ΅λ“ μ‹¤ν¨: {e}")
            return False
    
    def generate_response(self, prompt, max_tokens=100):
        """νμΈνλ‹ λ¨λΈλ΅ μ‘λ‹µ μƒμ„±"""
        if not self.loaded:
            return None
        
        try:
            # ν•κµ­μ–΄ λ€ν™” ν•μ‹μΌλ΅ ν¬λ§·
            formatted_prompt = f"μ‚¬μ©μ: {prompt}\\nμ–΄μ‹μ¤ν„΄νΈ:"
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            
            if torch.cuda.is_available() and hasattr(self.model, 'device'):
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # μ–΄μ‹μ¤ν„΄νΈ μ‘λ‹µ λ¶€λ¶„λ§ μ¶”μ¶
            if "μ–΄μ‹μ¤ν„΄νΈ:" in full_response:
                response = full_response.split("μ–΄μ‹μ¤ν„΄νΈ:")[-1].strip()
                return response
            else:
                return full_response.replace(formatted_prompt, "").strip()
        
        except Exception as e:
            print(f"β νμΈνλ‹ μ‘λ‹µ μƒμ„± μ‹¤ν¨: {e}")
            return None

# 3. μ „μ—­ μΈμ¤ν„΄μ¤ μƒμ„± (main.pyμ μ μ ν• μ„μΉμ— μ¶”κ°€)
finetuned_model = FinetuningModelLoader()

# 4. κΈ°μ΅΄ generate_response ν•¨μ μμ •
def generate_response_with_finetuning(prompt, model_type="auto"):
    """
    νμΈνλ‹ λ¨λΈμ„ μ°μ„  μ‚¬μ©ν•λ” μ‘λ‹µ μƒμ„± ν•¨μ
    
    Args:
        prompt: μ‚¬μ©μ μ…λ ¥
        model_type: "finetuned", "original", "auto"
    """
    
    # νμΈνλ‹ λ¨λΈ μ‹λ„
    if model_type in ["auto", "finetuned"] and finetuned_model.loaded:
        finetuned_response = finetuned_model.generate_response(prompt)
        if finetuned_response:
            return finetuned_response
    
    # κΈ°μ΅΄ λ¨λΈ ν΄λ°± (κΈ°μ΅΄ generate_response ν•¨μ νΈμ¶)
    if model_type in ["auto", "original"]:
        return generate_response(prompt)  # κΈ°μ΅΄ ν•¨μ νΈμ¶
    
    return "μ£„μ†΅ν•©λ‹λ‹¤. μ‘λ‹µμ„ μƒμ„±ν•  μ μ—†μµλ‹λ‹¤."

# 5. μ΄κΈ°ν™” μ½”λ“ (main() ν•¨μ μ‹μ‘ λ¶€λ¶„μ— μ¶”κ°€)
def initialize_finetuning():
    """νμΈνλ‹ λ¨λΈ μ΄κΈ°ν™”"""
    config_path = "finetuning_result.json"
    
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            if config.get('finetuning_completed'):
                model_path = config.get('model_path', './finetuning/models/korean_chat')
                
                if finetuned_model.load_model(model_path):
                    print("π‰ νμΈνλ‹ λ¨λΈμ΄ ν™μ„±ν™”λμ—μµλ‹λ‹¤!")
                    return True
                else:
                    print("β οΈ νμΈνλ‹ λ¨λΈ λ΅λ“ μ‹¤ν¨, κΈ°μ΅΄ λ¨λΈ μ‚¬μ©")
        
        except Exception as e:
            print(f"β οΈ νμΈνλ‹ μ„¤μ • λ΅λ“ μ‹¤ν¨: {e}")
    
    return False

# =====================================================
# μ‚¬μ© λ°©λ²•:
# 1. μ„ μ½”λ“λ¥Ό main.pyμ— μ μ ν μ¶”κ°€
# 2. main() ν•¨μ μ‹μ‘ λ¶€λ¶„μ— initialize_finetuning() νΈμ¶ μ¶”κ°€
# 3. κΈ°μ΅΄ generate_response νΈμ¶μ„ generate_response_with_finetuningμΌλ΅ λ³€κ²½
# =====================================================
'''
    
    with open("main_integration.py", 'w', encoding='utf-8') as f:
        f.write(integration_code)
    
    print("β… main.py ν†µν•© μ½”λ“ μƒμ„±: main_integration.py")
    print("π“– μ΄ νμΌμ λ‚΄μ©μ„ main.pyμ— λ³µμ‚¬ν•μ—¬ μ‚¬μ©ν•μ„Έμ”.")

def create_simple_test():
    """κ°„λ‹¨ν• ν…μ¤νΈ μ¤ν¬λ¦½νΈ μƒμ„±"""
    test_code = '''
#!/usr/bin/env python3
"""
νμΈνλ‹ λ¨λΈ κ°„λ‹¨ ν…μ¤νΈ
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# main_integration.pyμ—μ„ ν΄λμ¤ μ„ν¬νΈ
try:
    from main_integration import FinetuningModelLoader
    
    def test_finetuned_model():
        """νμΈνλ‹ λ¨λΈ ν…μ¤νΈ"""
        print("π§ νμΈνλ‹ λ¨λΈ ν…μ¤νΈ μ‹μ‘...")
        
        loader = FinetuningModelLoader()
        
        if loader.load_model():
            test_prompts = [
                "μ•λ…•ν•μ„Έμ”",
                "Pythonμ„ λ°°μ°κ³  μ‹¶μ–΄μ”",
                "λ‚ μ”¨κ°€ μ–΄λ•μ”?",
                "AIμ— λ€ν•΄ μ„¤λ…ν•΄μ£Όμ„Έμ”"
            ]
            
            for prompt in test_prompts:
                print(f"\\nμ‚¬μ©μ: {prompt}")
                response = loader.generate_response(prompt)
                print(f"μ–΄μ‹μ¤ν„΄νΈ: {response}")
        
        else:
            print("β λ¨λΈ λ΅λ“ μ‹¤ν¨")

    if __name__ == "__main__":
        test_finetuned_model()

except ImportError:
    print("β main_integration.pyλ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤.")
    print("π’΅ λ¨Όμ € 'python finetuning/integration_helper.py'λ¥Ό μ‹¤ν–‰ν•μ„Έμ”.")
'''
    
    with open("test_finetuned.py", 'w', encoding='utf-8') as f:
        f.write(test_code)
    
    print("β… ν…μ¤νΈ μ¤ν¬λ¦½νΈ μƒμ„±: test_finetuned.py")

def main():
    """λ©”μΈ ν•¨μ"""
    print("π”§ νμΈνλ‹ ν†µν•© ν—¬νΌ")
    print("=" * 50)
    
    # νμΈνλ‹ μƒνƒ ν™•μΈ
    config = check_finetuning_status()
    if not config:
        return
    
    # ν†µν•© μ½”λ“ μƒμ„±
    print("\nπ“ ν†µν•© μ½”λ“ μƒμ„± μ¤‘...")
    create_main_integration()
    
    # ν…μ¤νΈ μ¤ν¬λ¦½νΈ μƒμ„±
    print("\nπ§ ν…μ¤νΈ μ¤ν¬λ¦½νΈ μƒμ„± μ¤‘...")
    create_simple_test()
    
    print("\n" + "=" * 50)
    print("β… ν†µν•© μ¤€λΉ„ μ™„λ£!")
    print("\nπ“‹ λ‹¤μ λ‹¨κ³„:")
    print("1. main_integration.pyμ λ‚΄μ©μ„ main.pyμ— ν†µν•©")
    print("2. test_finetuned.pyλ΅ ν…μ¤νΈ μ‹¤ν–‰")
    print("3. main.pyμ—μ„ νμΈνλ‹ λ¨λΈ μ‚¬μ©")
    print("\nπ’΅ νμΈνλ‹ λ¨λΈμ€ ν•κµ­μ–΄ λ€ν™”μ— μµμ ν™”λμ–΄ μμµλ‹λ‹¤!")

if __name__ == "__main__":
    main()
