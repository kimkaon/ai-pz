import torch
import numpy as np
import warnings
import os

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings("ignore")
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # TensorFlow ë¡œê·¸ ìˆ¨ê¸°ê¸°
os.environ['PYTORCH_WARNINGS'] = '0'  # PyTorch ê²½ê³  ìˆ¨ê¸°ê¸°

print("main.pyì—ì„œ GPU ì´ë¦„:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A")

from config_gpu import setup_gpu
setup_gpu(cuda_device=0)
from voice_utils import record_voice, save_temp_wav, transcribe, select_input_device, record_voice_until_silence
from nous_hermes2_mistral_loader import load_nous_hermes2_mistral, chat_nous_hermes2_mistral, chat_nous_hermes2_mistral_stream
from mic_settings import save_mic_index, load_mic_index
from prompt_templates import make_role_prompt
from openvoice_tts import synthesize_with_openvoice, synthesize_quick_tts
from log_settings import toggle_verbose_mode, is_verbose_mode, log_print, get_current_settings

# í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ í†µí•© ê¸°ëŠ¥ ì¶”ê°€
try:
    from finetuning_integration import (
        process_with_finetuned_models_main, 
        request_specialist_mode,
        get_finetuning_stats,
        integrate_finetuned_response  # í˜¸í™˜ì„± ìœ ì§€
    )
    FINETUNING_ENABLED = True
    log_print("í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ", "model_loading")
except ImportError as e:
    log_print(f"í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ë¡œë“œ ì‹¤íŒ¨: {e}", "model_loading")
    FINETUNING_ENABLED = False

# ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ í†µí•© ê¸°ëŠ¥ ì¶”ê°€
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    ENGLISH_FINETUNED_AVAILABLE = True
    log_print("ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì™„ë£Œ", "model_loading")
except ImportError as e:
    ENGLISH_FINETUNED_AVAILABLE = False
    log_print(f"ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}", "model_loading")

import sounddevice as sd
try:
    import soundfile as sf
    SOUNDFILE_AVAILABLE = True
except ImportError:
    SOUNDFILE_AVAILABLE = False
    print("âš ï¸ soundfile ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. ìŒì„± ê¸°ëŠ¥ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
import re
import threading
import queue
import time

def clean_llm_response(response):
    """
    LLM ì‘ë‹µì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ TTSì— ì í•©í•œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    """
    if not response:
        return ""
    
    # ì—¬ëŸ¬ ê°€ì§€ íŒ¨í„´ìœ¼ë¡œ ì •ë¦¬
    cleaned = response.strip()
    
    # ì—¬ëŸ¬ ì¤„ì— ê±¸ì¹œ ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¤„ë³„ë¡œ ë¶„ë¦¬
    lines = cleaned.split('\n')
    filtered_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Type:, Classification:, Category: íŒ¨í„´ ì œê±°
        if re.match(r'^(Type|Classification|Category)\s*:', line, flags=re.IGNORECASE):
            continue
            
        # ëŒ€ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶„ë¥˜ë§Œ ìˆëŠ” ì¤„ ì œê±°
        if re.match(r'^\[.*?\]$', line):
            continue
            
        # ë¶„ë¥˜ëª…ë§Œ ìˆëŠ” ì¤„ ì œê±°
        if line.lower() in ['qna', 'daily chat', 'specific program', 'unknown']:
            continue
        
        # Answer:, QnA: ë“±ì˜ ì ‘ë‘ì‚¬ ì œê±°
        line = re.sub(r'^(answer|qna|daily chat|specific program|unknown)\s*:\s*', '', line, flags=re.IGNORECASE)
        
        # ëŒ€ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶„ë¥˜ ì •ë³´ ì œê±°
        line = re.sub(r'^\[.*?\]\s*', '', line)
        
        if line:  # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì¶”ê°€
            filtered_lines.append(line)
    
    # ì¤„ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°
    cleaned = ' '.join(filtered_lines)
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    cleaned = re.sub(r'\s+', ' ', cleaned)
    
    # ì–‘ìª½ ê³µë°± ì œê±°
    cleaned = cleaned.strip()
    
    # ë¹ˆ ë¬¸ìì—´ì´ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
    if not cleaned:
        return "Sorry, I couldn't generate a proper response."
    
    return cleaned

def try_finetuned_response(prompt):
    """
    ì„ íƒëœ ëª¨ë¸ì— ë”°ë¼ íŒŒì¸íŠœë‹ ì‘ë‹µ ìƒì„± ì‹œë„
    Returns: (response, used_finetuned, model_info) íŠœí”Œ
    """
    current_model = MODEL_SELECTION["current"]
    
    # RTX 3070 ë¬´ì œí•œ ëª¨ë¸ ì‚¬ìš©
    if current_model == "rtx3070_unfiltered":
        if not rtx3070_unfiltered_model.loaded:
            if not rtx3070_unfiltered_model.load_model():
                log_print("RTX 3070 ë¬´ì œí•œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±", "model_loading")
                return None, False, {}
        
        response = rtx3070_unfiltered_model.generate_response(prompt)
        if response:
            model_info = {
                'model_used': 'rtx3070_unfiltered',
                'category': 'unfiltered_conversation',
                'confidence': 0.95,
                'quality_level': 'high'
            }
            log_print(f"RTX 3070 ë¬´ì œí•œ ëª¨ë¸ ì‚¬ìš©: {response[:50]}...", "model_loading")
            return response, True, model_info
        
        log_print("RTX 3070 ë¬´ì œí•œ ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨", "model_loading")
        return None, False, {}
    
    # RTX 3070 ì–¸ì–´ ì œí•œ ëª¨ë¸ ì‚¬ìš©
    elif current_model == "rtx3070_language_limited":
        if not rtx3070_language_limited_model.loaded:
            if not rtx3070_language_limited_model.load_model():
                log_print("RTX 3070 ì–¸ì–´ ì œí•œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±", "model_loading")
                return None, False, {}
        
        response = rtx3070_language_limited_model.generate_response(prompt)
        if response:
            model_info = {
                'model_used': 'rtx3070_language_limited',
                'category': 'language_limited_conversation',
                'confidence': 0.95,
                'quality_level': 'high'
            }
            log_print(f"RTX 3070 ì–¸ì–´ ì œí•œ ëª¨ë¸ ì‚¬ìš©: {response[:50]}...", "model_loading")
            return response, True, model_info
        
        log_print("RTX 3070 ì–¸ì–´ ì œí•œ ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨", "model_loading")
        return None, False, {}
    
    # ì˜ì–´ í†µí•© íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©
    elif current_model == "english_unified":
        if english_finetuned_model.loaded:
            response = english_finetuned_model.generate_response(prompt)
            if response:
                model_info = {
                    'model_used': 'english_unified',
                    'category': 'english_conversation',
                    'confidence': 0.9,
                    'quality_level': 'high'
                }
                log_print(f"ì˜ì–´ í†µí•© ëª¨ë¸ ì‚¬ìš©: {response[:50]}...", "model_loading")
                return response, True, model_info
        
        log_print("ì˜ì–´ í†µí•© ëª¨ë¸ ì‚¬ìš© ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë¸ë¡œ í´ë°±", "model_loading")
        return None, False, {}
    
    # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ëª¨ë¸ ì‚¬ìš©
    elif current_model == "hybrid" and FINETUNING_ENABLED:
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬
            result = process_with_finetuned_models_main(prompt)
            
            if result and result.get('response'):
                model_info = {
                    'category': result.get('category', 'unknown'),
                    'confidence': result.get('confidence', 0.0),
                    'model_used': result.get('model_used', 'unknown'),
                    'quality_level': result.get('quality_level', 'unknown'),
                    'can_upgrade': result.get('can_upgrade', False)
                }
                
                log_print(f"í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì‚¬ìš©: {model_info['model_used']} ({model_info['category']}, ì‹ ë¢°ë„: {model_info['confidence']:.3f})", "model_loading")
                
                return result['response'], True, model_info
            else:
                log_print("í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨", "model_loading")
                return None, False, {}
                
        except Exception as e:
            log_print(f"í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì˜¤ë¥˜: {e}", "model_loading")
            # Fallback to basic integrated response
            try:
                fallback_response = integrate_finetuned_response(prompt)
                return fallback_response, True, {'model_used': 'fallback', 'category': 'unknown'}
            except:
                return None, False, {}
    
    # ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (íŒŒì¸íŠœë‹ ì—†ìŒ)
    return None, False, {}

def stream_chat_with_tts(llm, prompt, conversation_history, mode, device_index=None):
    """
    ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… + TTS
    - LLMì—ì„œ í† í° ìƒì„±ê³¼ ë™ì‹œì— í™”ë©´ ì¶œë ¥
    - ë¬¸ì¥ ë‹¨ìœ„ë¡œ TTS ìƒì„± ë° ì¬ìƒ
    """
    from nous_hermes2_mistral_loader import chat_nous_hermes2_mistral_stream
    from openvoice_tts import synthesize_quick_tts
    
    # ì»¨í…ìŠ¤íŠ¸ í¬í•¨ í”„ë¡¬í”„íŠ¸ ìƒì„±
    context_prompt = build_conversation_context(conversation_history, prompt)
    
    print("ğŸ¤” AIê°€ ë‹µë³€ ì¤‘...")
    print("ğŸ¤– AI: ", end="", flush=True)
    
    # ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
    full_response = ""
    sentence_buffer = ""
    sentence_count = 0
    
    # TTS íì™€ ìŠ¤ë ˆë“œ
    tts_queue = queue.Queue()
    tts_thread = None
    
    def tts_worker():
        """TTS ì²˜ë¦¬ ì›Œì»¤ ìŠ¤ë ˆë“œ"""
        while True:
            try:
                item = tts_queue.get(timeout=1)
                if item is None:  # ì¢…ë£Œ ì‹ í˜¸
                    break
                    
                sentence, file_path = item
                try:
                    synthesize_quick_tts(sentence, file_path, language="English")
                    
                    # ì˜¤ë””ì˜¤ ì¬ìƒ
                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                        if SOUNDFILE_AVAILABLE:
                            data, fs = sf.read(file_path, dtype='float32')
                            sd.play(data, fs)
                            sd.wait()
                            log_print(f"ğŸ”Š ë¬¸ì¥ ì¬ìƒ ì™„ë£Œ: {sentence[:30]}...", "tts_status")
                        else:
                            log_print("âš ï¸ soundfile ì—†ìŒ - ì˜¤ë””ì˜¤ ì¬ìƒ ê±´ë„ˆëœ€", "tts_status")
                    
                except Exception as e:
                    log_print(f"TTS ì²˜ë¦¬ ì˜¤ë¥˜: {e}", "tts_debug")
                finally:
                    tts_queue.task_done()
                    
            except queue.Empty:
                continue
            except Exception as e:
                log_print(f"TTS ì›Œì»¤ ì˜¤ë¥˜: {e}", "tts_debug")
                break
    
    # TTS ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
    tts_thread = threading.Thread(target=tts_worker, daemon=True)
    tts_thread.start()
    
    try:
        # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í† í° ë°›ê¸°
        for token in chat_nous_hermes2_mistral_stream(llm, context_prompt):
            # í™”ë©´ì— ì‹¤ì‹œê°„ ì¶œë ¥
            print(token, end="", flush=True)
            full_response += token
            sentence_buffer += token
            
            # ë¬¸ì¥ ë ê°ì§€ (.!?ë¡œ ëë‚˜ê³  ê³µë°±ì´ ìˆëŠ” ê²½ìš°)
            if any(punct in token for punct in ['. ', '! ', '? ', '.\n', '!\n', '?\n']):
                sentence = sentence_buffer.strip()
                if len(sentence) > 5:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                    sentence_count += 1
                    
                    # TTS íŒŒì¼ ê²½ë¡œ
                    tts_output_dir = "tts_output"
                    if not os.path.exists(tts_output_dir):
                        os.makedirs(tts_output_dir)
                    
                    tts_wav_path = os.path.join(tts_output_dir, f"stream_{sentence_count}.wav")
                    
                    # TTS íì— ì¶”ê°€
                    tts_queue.put((sentence, tts_wav_path))
                    log_print(f"ğŸµ TTS íì— ì¶”ê°€: {sentence[:30]}...", "tts_status")
                
                sentence_buffer = ""
        
        print()  # ì¤„ë°”ê¿ˆ
        
        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
        if sentence_buffer.strip() and len(sentence_buffer.strip()) > 5:
            sentence_count += 1
            tts_wav_path = os.path.join("tts_output", f"stream_{sentence_count}.wav")
            tts_queue.put((sentence_buffer.strip(), tts_wav_path))
        
        # TTS í ì™„ë£Œ ëŒ€ê¸°
        log_print("ğŸµ ëª¨ë“  TTS ì²˜ë¦¬ ëŒ€ê¸° ì¤‘...", "tts_status")
        tts_queue.join()
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì¢…ë£Œ
        tts_queue.put(None)
        if tts_thread:
            tts_thread.join(timeout=5)
        
        log_print("âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì™„ë£Œ", "tts_status")
        
    except Exception as e:
        print(f"\nâŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
        log_print(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„¸ ì˜¤ë¥˜: {e}", "tts_debug")
        
        # ì˜¤ë¥˜ ë°œìƒì‹œ ì›Œì»¤ ìŠ¤ë ˆë“œ ì •ë¦¬
        try:
            tts_queue.put(None)
            if tts_thread:
                tts_thread.join(timeout=2)
        except:
            pass
    
    # ì‘ë‹µ ì •ë¦¬
    clean_response = clean_llm_response(full_response)
    return clean_response

def main():
    print("ğŸ¤– AI ì–´ì‹œìŠ¤í„´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    llm = load_nous_hermes2_mistral()
    print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œì— LLM ì¸ìŠ¤í„´ìŠ¤ ì „ë‹¬
    if FINETUNING_ENABLED:
        try:
            from hybrid_finetuning_integration import initialize_hybrid_system
            initialize_hybrid_system(llm)
        except ImportError:
            pass

    # ë§ˆì´í¬ ì„¤ì • ë¶ˆëŸ¬ê¸°ê¸° ë˜ëŠ” ì¬ì„¤ì •
    device_index = load_mic_index()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ê°ì§€
    detect_available_models()
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ìš©
    conversation_history = []

    # ì´ˆê¸° ì„¤ì • ë©”ë‰´
    while True:
        current_verbose = "ON" if is_verbose_mode() else "OFF"
        
        # í˜„ì¬ ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        model_info = get_model_info()
        current_model_name = model_info["current_name"]
        
        # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ìƒíƒœ í™•ì¸
        finetuning_status = "OFF"
        if FINETUNING_ENABLED:
            try:
                stats = get_finetuning_stats()
                if stats.get('status') == 'active':
                    finetuning_status = "ON (Hybrid)"
                    loaded_specialists = stats.get('loaded_specialists', [])
                    if loaded_specialists:
                        finetuning_status += f" + {len(loaded_specialists)} specialists"
                elif stats.get('status') == 'unavailable':
                    finetuning_status = "UNAVAILABLE"
                else:
                    finetuning_status = "ERROR"
            except:
                finetuning_status = "ERROR"
        
        print(f"\n{'='*60}")
        print("ğŸ¯ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì„¤ì • (ë‹¤ì¤‘ ëª¨ë¸ ëª¨ë“œ)")
        print(f"{'='*60}")
        print(f"ï¿½ í˜„ì¬ í™œì„± ëª¨ë¸: {current_model_name}")
        print(f"ï¿½ğŸ§  íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ: {finetuning_status}")
        if FINETUNING_ENABLED and finetuning_status.startswith("ON"):
            try:
                stats = get_finetuning_stats()
                print(f"   ğŸ’¾ í†µí•©ëª¨ë¸: {'ë¡œë“œë¨' if stats.get('unified_model_loaded') else 'ë¯¸ë¡œë“œ'}")
                print(f"   ğŸ“ˆ í†µê³„: í†µí•©ì‘ë‹µ {stats.get('unified_responses', 0)}íšŒ, ì „ë¬¸ì‘ë‹µ {stats.get('specialist_responses', 0)}íšŒ")
            except:
                pass
        print(f"ğŸ“ ìƒì„¸ ë¡œê·¸: {current_verbose}")
        print(f"{'='*60}")
        
        mode = input(f"ì„ íƒ (s: ìŒì„±ëŒ€í™”, m: í…ìŠ¤íŠ¸ëŒ€í™”, p: ë§ˆì´í¬ì„¤ì •, md: ëª¨ë¸ì„ íƒ, lg: ë¡œê·¸[{current_verbose}], ft: íŒŒì¸íŠœë‹ê´€ë¦¬, q: ì¢…ë£Œ): ").strip().lower()
        
        if mode == "q":
            print("ğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ì •ë¦¬
            if FINETUNING_ENABLED:
                try:
                    from finetuning_integration import cleanup_finetuning_system
                    cleanup_finetuning_system()
                except:
                    pass
            return
        elif mode == "md":
            # ëª¨ë¸ ì„ íƒ ë©”ë‰´
            print(f"\n{'='*50}")
            print("ğŸ¤– ëª¨ë¸ ì„ íƒ")
            print(f"{'='*50}")
            
            model_info = get_model_info()
            available_models = model_info["available"]
            current_model = model_info["current"]
            
            # ëª¨ë¸ ì„¤ëª…
            model_descriptions = {
                "original": "ê¸°ë³¸ Nous-Hermes-2-Mistral (ë‹¤êµ­ì–´ ì§€ì›)",
                "hybrid": "í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ëª¨ë¸ (í•œêµ­ì–´ íŠ¹í™”)",
                "english_unified": "ì˜ì–´ í†µí•© íŒŒì¸íŠœë‹ ëª¨ë¸ (ì˜ì–´ ëŒ€í™”/QnA íŠ¹í™”)",
                "rtx3070_unfiltered": "RTX 3070 ìµœì í™” ëª¨ë¸ (ì˜ì–´/í•œêµ­ì–´ ì œí•œ)",
                "rtx3070_language_limited": "RTX 3070 ìµœì í™” ëª¨ë¸ (ì˜ì–´/í•œêµ­ì–´ ì œí•œ)"
            }
            
            print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
            for i, model_type in enumerate(available_models, 1):
                marker = "â˜…" if model_type == current_model else " "
                description = model_descriptions.get(model_type, model_type)
                print(f"{marker} {i}. {description}")
                
                # RTX 3070 ìµœì í™” ëª¨ë¸ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…
                if model_type == "rtx3070_unfiltered":
                    print("    ğŸš€ ìµœê³  ì„±ëŠ¥ì˜ RTX 3070 ìµœì í™” ëª¨ë¸")
                    print("    ğŸŒ ì–¸ì–´ ì œí•œ: ì˜ì–´/í•œêµ­ì–´ë§Œ ì§€ì› (ë‹¤ë¥¸ ì–¸ì–´ ì°¨ë‹¨)")
                    rtx_path = "./models/rtx3070_optimized_best"
                    if os.path.exists(rtx_path):
                        print("         âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨")
                    else:
                        print("         âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                
                # RTX 3070 ì–¸ì–´ ì œí•œ ëª¨ë¸ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…
                elif model_type == "rtx3070_language_limited":
                    print("    ğŸš€ ìµœê³  ì„±ëŠ¥ì˜ RTX 3070 ìµœì í™” ëª¨ë¸")
                    print("    ğŸŒ ì–¸ì–´ ì œí•œ: ì˜ì–´/í•œêµ­ì–´ë§Œ ì§€ì› (ë‹¤ë¥¸ ì–¸ì–´ ì°¨ë‹¨)")
                    rtx_path = "./models/rtx3070_optimized_best"
                    if os.path.exists(rtx_path):
                        print("         âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨")
                    else:
                        print("         âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                
                # ì˜ì–´ ëª¨ë¸ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª…
                elif model_type == "english_unified":
                    print("    âš ï¸  ì£¼ì˜: ì´ ëª¨ë¸ì€ ì˜ì–´ ëŒ€í™”ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                    print("         í•œêµ­ì–´ ì‘ë‹µ í’ˆì§ˆì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    if os.path.exists("./finetuning/models/unified_model"):
                        print("         âœ… ëª¨ë¸ íŒŒì¼ í™•ì¸ë¨")
                    else:
                        print("         âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            
            print(f"\ní˜„ì¬ í™œì„± ëª¨ë¸: {model_descriptions.get(current_model, current_model)}")
            print("\n0. ëŒì•„ê°€ê¸°")
            
            try:
                choice = input("\nì„ íƒí•  ëª¨ë¸ ë²ˆí˜¸: ").strip()
                
                if choice == "0":
                    continue
                elif choice.isdigit():
                    model_index = int(choice) - 1
                    if 0 <= model_index < len(available_models):
                        selected_model = available_models[model_index]
                        
                        if selected_model == current_model:
                            print(f"ì´ë¯¸ '{model_descriptions[selected_model]}' ëª¨ë¸ì´ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                        else:
                            # RTX 3070 ìµœì í™” ëª¨ë¸ ì„ íƒì‹œ í™•ì¸
                            if selected_model == "rtx3070_unfiltered":
                                print("\nğŸ’¡ RTX 3070 ìµœì í™” ëª¨ë¸ì„ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.")
                                print("   ì´ ëª¨ë¸ì€ ì˜ì–´ì™€ í•œêµ­ì–´ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
                                print("   ë‹¤ë¥¸ ì–¸ì–´ ì…ë ¥ì€ ìë™ìœ¼ë¡œ ì°¨ë‹¨ë©ë‹ˆë‹¤.")
                                confirm = input("   ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
                                if confirm in ['n', 'no']:
                                    print("ëª¨ë¸ ë³€ê²½ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                                    continue
                            
                            # RTX 3070 ì–¸ì–´ ì œí•œ ëª¨ë¸ ì„ íƒì‹œ í™•ì¸
                            elif selected_model == "rtx3070_language_limited":
                                print("\nğŸ’¡ RTX 3070 ìµœì í™” ì–¸ì–´ ì œí•œ ëª¨ë¸ì„ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.")
                                print("   ì´ ëª¨ë¸ì€ ì˜ì–´ì™€ í•œêµ­ì–´ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
                                print("   ë‹¤ë¥¸ ì–¸ì–´ ì…ë ¥ì€ ìë™ìœ¼ë¡œ ì°¨ë‹¨ë©ë‹ˆë‹¤.")
                                confirm = input("   ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (Y/n): ").strip().lower()
                                if confirm in ['n', 'no']:
                                    print("ëª¨ë¸ ë³€ê²½ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                                    continue
                            
                            # ì˜ì–´ ëª¨ë¸ ì„ íƒì‹œ í™•ì¸
                            elif selected_model == "english_unified":
                                print("\nâš ï¸  ì˜ì–´ í†µí•© íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì„ íƒí•˜ì…¨ìŠµë‹ˆë‹¤.")
                                print("   ì´ ëª¨ë¸ì€ ì˜ì–´ ëŒ€í™”ì™€ QnAì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                                print("   í•œêµ­ì–´ ì‘ë‹µ í’ˆì§ˆì´ í˜„ì €íˆ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                                confirm = input("   ì •ë§ë¡œ ì´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
                                if confirm not in ['y', 'yes']:
                                    print("ëª¨ë¸ ë³€ê²½ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                                    continue
                            
                            print(f"ğŸ”„ ëª¨ë¸ì„ '{model_descriptions[selected_model]}'ë¡œ ë³€ê²½ ì¤‘...")
                            success, message = switch_model(selected_model)
                            
                            if success:
                                print(f"âœ… {message}")
                                log_print(f"ëª¨ë¸ ë³€ê²½: {current_model} -> {selected_model}", "model_loading")
                            else:
                                print(f"âŒ ëª¨ë¸ ë³€ê²½ ì‹¤íŒ¨: {message}")
                    else:
                        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
                else:
                    print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤.")
                    
            except ValueError:
                print("âŒ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            input("\nê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
            continue
        elif mode == "lg":
            settings = toggle_verbose_mode()
            new_verbose = "ON" if settings["verbose_mode"] else "OFF"
            print(f"âœ… ë¡œê·¸ ëª¨ë“œê°€ [{new_verbose}]ë¡œ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            log_print(f"ë¡œê·¸ ì„¤ì •: {settings}", "general")
            continue
        elif mode == "ft":
            # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ê´€ë¦¬ ë©”ë‰´
            if not FINETUNING_ENABLED:
                print("âŒ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                print("   hybrid_finetuning_integration.pyë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                continue
            
            print(f"\n{'='*40}")
            print("ğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ê´€ë¦¬")
            print(f"{'='*40}")
            
            try:
                stats = get_finetuning_stats()
                if stats.get('status') == 'active':
                    print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ìƒíƒœ:")
                    print(f"   ğŸ§  í†µí•©ëª¨ë¸: {'í™œì„±' if stats.get('unified_model_loaded') else 'ë¯¸í™œì„±'}")
                    print(f"   ğŸ¯ ë¡œë“œëœ ì „ë¬¸ëª¨ë¸: {stats.get('loaded_specialists', [])}")
                    print(f"   ğŸ“Š ì‘ë‹µ í†µê³„: í†µí•© {stats.get('unified_responses', 0)}íšŒ, ì „ë¬¸ {stats.get('specialist_responses', 0)}íšŒ")
                    print(f"   ğŸ”„ ëª¨ë¸ ì „í™˜: {stats.get('model_switches', 0)}íšŒ")
                    print(f"   â±ï¸ í‰ê·  ì‘ë‹µì‹œê°„: {stats.get('avg_response_time', 0):.2f}ì´ˆ")
                
                    print(f"\ní•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ê´€ë¦¬ ì˜µì…˜:")
                    print("1. ì¼ë°˜ ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸ ì‘ë‹µ")
                    print("2. ì „ë¬¸ê°€ ëª¨ë“œë¡œ í…ŒìŠ¤íŠ¸ ì‘ë‹µ")
                    print("3. ì „ë¬¸ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½)")
                    print("4. íŒŒì¸íŠœë‹ í™˜ê²½ ì—´ê¸°")
                    print("0. ëŒì•„ê°€ê¸°")
                    
                    ft_choice = input("ì„ íƒ: ").strip()
                    
                    if ft_choice == "1":
                        test_prompt = input("í…ŒìŠ¤íŠ¸ ì…ë ¥: ").strip()
                        if test_prompt:
                            finetuned_response, used, model_info = try_finetuned_response(test_prompt)
                            if used and finetuned_response:
                                print(f"ğŸ¤– [{model_info.get('model_used', 'unknown')}] {finetuned_response}")
                                print(f"ğŸ“Š ì •ë³´: {model_info}")
                            else:
                                print("âŒ í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    elif ft_choice == "2":
                        test_prompt = input("ì „ë¬¸ê°€ ëª¨ë“œ í…ŒìŠ¤íŠ¸ ì…ë ¥: ").strip()
                        if test_prompt:
                            try:
                                specialist_result = request_specialist_mode(test_prompt)
                                print(f"ğŸ“ [{specialist_result.get('model_used', 'unknown')}] {specialist_result.get('response', 'ì‘ë‹µ ì—†ìŒ')}")
                                print(f"ğŸ“Š ì •ë³´: ì¹´í…Œê³ ë¦¬ {specialist_result.get('category')}, í’ˆì§ˆ {specialist_result.get('quality_level')}")
                            except Exception as e:
                                print(f"âŒ ì „ë¬¸ê°€ ëª¨ë“œ ì˜¤ë¥˜: {e}")
                    
                    elif ft_choice == "3":
                        print("ğŸ—‘ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ëŠ” ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.")
                        print("   í•„ìš”ì‹œ í”„ë¡œê·¸ë¨ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.")
                    
                    elif ft_choice == "4":
                        print("ğŸ“ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸:")
                        print("   python finetuning/scripts/generate_english_datasets.py")
                        print("   python finetuning/scripts/validate_english_data.py") 
                        print("   ë˜ëŠ” run_finetuning.bat ì‹¤í–‰")
                else:
                    print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {stats.get('error', 'Unknown error')}")
                    
            except Exception as e:
                print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
                
            else:
                print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ“ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•˜ë ¤ë©´ run_finetuning.batë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            
            input("\nê³„ì†í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
            continue
        elif mode == "p":
            device_index = select_input_device()
            print(f"âœ… ì„ íƒëœ ë§ˆì´í¬ index: {device_index}")
            continue
        elif mode in ["s", "m"]:
            break
        else:
            print("âŒ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")

    # ì‹¤ì‹œê°„ ëŒ€í™” ëª¨ë“œ ì‹œì‘
    model_info = get_model_info()
    current_model_name = model_info["current_name"]
    
    print(f"\n{'='*60}")
    if mode == "s":
        print("ğŸ¤ ìŒì„± ëŒ€í™” ëª¨ë“œ ì‹œì‘ (ì¢…ë£Œ: 'over' ë˜ëŠ” 'quit' ë§í•˜ê¸°)")
    else:
        print("ğŸ’¬ í…ìŠ¤íŠ¸ ëŒ€í™” ëª¨ë“œ ì‹œì‘ (ì¢…ë£Œ: 'over', 'quit', ë˜ëŠ” ë¹ˆ ì…ë ¥)")
    print(f"ğŸ¤– í™œì„± ëª¨ë¸: {current_model_name}")
    
    # ì˜ì–´ ëª¨ë¸ ì‚¬ìš©ì‹œ ì¶”ê°€ ì•ˆë‚´
    if model_info["current"] == "english_unified":
        print("âš ï¸  ì˜ì–´ ëŒ€í™”ì— ìµœì í™”ëœ ëª¨ë¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ ì‘ë‹µ í’ˆì§ˆì´ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    print(f"{'='*60}")
    print("ğŸ’¡ íŒ: ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ì„¸ìš”. ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ê¸°ì–µí•©ë‹ˆë‹¤!")
    print("ğŸ’¡ ëª¨ë¸ ë³€ê²½ì€ ë©”ì¸ ë©”ë‰´ì—ì„œ 'md'ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    print()

    # ì‹¤ì‹œê°„ ëŒ€í™” ë£¨í”„
    conversation_count = 0
    while True:
        conversation_count += 1
        
        # ì…ë ¥ ë°›ê¸°
        if mode == "s":
            # ìŒì„± ì…ë ¥
            from voice_utils import wait_for_voice
            print(f"ğŸ¤ ëŒ€í™” #{conversation_count} - ë§ì”€í•˜ì„¸ìš”...")
            
            if not wait_for_voice(device_index=device_index):
                continue
                
            audio, fs = record_voice_until_silence(device_index=device_index)
            
            if np.abs(audio).max() < 0.01:
                log_print("â— ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", "audio_processing")
                continue
                
            wav_path = save_temp_wav(audio, fs, silence_threshold=0.01)
            if wav_path is None:
                log_print("â— ìŒì„± ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.", "audio_processing")
                continue
                
            log_print(f"ğŸ“ ìŒì„± íŒŒì¼ ì €ì¥: {wav_path}", "audio_processing")
            prompt = transcribe(wav_path)
            print(f"ğŸ—£ï¸ ì¸ì‹ëœ ë‚´ìš©: \"{prompt}\"")
            
            if not prompt.strip():
                print("â— ìŒì„± ì¸ì‹ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                continue
        else:
            # í…ìŠ¤íŠ¸ ì…ë ¥
            prompt = input(f"ğŸ’¬ ëŒ€í™” #{conversation_count} - ë©”ì‹œì§€ ì…ë ¥: ").strip()
            if not prompt:
                print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

        # ì¢…ë£Œ ëª…ë ¹ì–´ ì²´í¬
        if any(keyword in prompt.lower() for keyword in ["over", "quit", "exit", "bye", "goodbye"]):
            print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 1. ë¨¼ì € í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ëª¨ë¸ë¡œ ì‘ë‹µ ì‹œë„
        finetuned_response, used_finetuned, model_info = try_finetuned_response(prompt)
        
        if used_finetuned and finetuned_response:
            # ëª¨ë¸ ì •ë³´ í‘œì‹œ
            model_used = model_info.get('model_used', 'unknown')
            category = model_info.get('category', 'unknown')
            quality_level = model_info.get('quality_level', 'standard')
            can_upgrade = model_info.get('can_upgrade', False)
            
            print(f"ğŸ¤– AI ({model_used}/{quality_level}): {finetuned_response}")
            
            # ì—…ê·¸ë ˆì´ë“œ ì œì•ˆ (ì „ë¬¸ëª¨ë¸ë¡œ ì „í™˜ ê°€ëŠ¥í•œ ê²½ìš°)
            if can_upgrade:
                print(f"ğŸ’¡ ë” ì „ë¬¸ì ì¸ ë‹µë³€ì´ í•„ìš”í•˜ì‹œë©´ 'ì „ë¬¸ê°€ ëª¨ë“œ'ë¼ê³  ë§ì”€í•´ ì£¼ì„¸ìš”!")
            
            # íŒŒì¸íŠœë‹ ì‘ë‹µì„ TTSë¡œ ì¶œë ¥
            if mode == "s":  # ìŒì„± ëª¨ë“œì¼ ë•Œë§Œ TTS
                try:
                    tts_output_dir = "tts_output"
                    if not os.path.exists(tts_output_dir):
                        os.makedirs(tts_output_dir)
                    
                    tts_wav_path = os.path.join(tts_output_dir, f"{model_used}_response.wav")
                    log_print(f"ğŸµ {model_used} ì‘ë‹µ ìŒì„± í•©ì„± ì¤‘...", "tts_status")
                    synthesize_with_openvoice(finetuned_response, tts_wav_path, language="English", verbose=is_verbose_mode())
                    
                    if os.path.exists(tts_wav_path) and os.path.getsize(tts_wav_path) > 0:
                        if SOUNDFILE_AVAILABLE:
                            data, fs = sf.read(tts_wav_path, dtype='float32')
                            log_print(f"ğŸ”Š {model_used} ì‘ë‹µ ì¬ìƒ ì¤‘...", "tts_status")
                            sd.play(data, fs)
                            sd.wait()
                            log_print(f"âœ… {model_used} ì‘ë‹µ ì¬ìƒ ì™„ë£Œ", "tts_status")
                        else:
                            log_print(f"âš ï¸ soundfile ì—†ìŒ - {model_used} ì‘ë‹µ ì¬ìƒ ê±´ë„ˆëœ€", "tts_status")
                    else:
                        print(f"âŒ {model_used} TTS íŒŒì¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                except Exception as tts_e:
                    print(f"âŒ {model_used} TTS ì˜¤ë¥˜: {tts_e}")
            
            response = finetuned_response
        else:
            # 2. íŒŒì¸íŠœë‹ ì‘ë‹µì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì‚¬ìš©
            log_print("ê¸°ì¡´ LLM ëª¨ë¸ ì‚¬ìš©", "model_loading")
            
            # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            context_prompt = build_conversation_context(conversation_history, prompt)
            
            # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            try:
                response = stream_chat_with_tts(llm, prompt, conversation_history, mode, device_index)
            except Exception as e:
                print(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨, ì¼ë°˜ ëª¨ë“œë¡œ ì „í™˜: {e}")
                # ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ fallback
                print("ğŸ¤” ë‹µë³€ ìƒì„± ì¤‘...")
                raw_response = chat_nous_hermes2_mistral(llm, context_prompt)
                response = clean_llm_response(raw_response)
            
            if is_verbose_mode():
                print(f"\nğŸ¤– ì›ë³¸ ì‘ë‹µ:\n{raw_response}\n")
            
            print(f"ğŸ¤– AI: {response}")
            
            # ê¸°ì¡´ TTS ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
            tts_output_dir = "tts_output"
            if not os.path.exists(tts_output_dir):
                os.makedirs(tts_output_dir)
            
            tts_wav_path = os.path.join(tts_output_dir, "response.wav")
            try:
                import warnings
                warnings.filterwarnings("ignore")
                
                log_print("ğŸµ ìŒì„± í•©ì„± ì¤‘...", "tts_status")
                synthesize_with_openvoice(response, tts_wav_path, language="English", verbose=is_verbose_mode())
                
                if os.path.exists(tts_wav_path) and os.path.getsize(tts_wav_path) > 0:
                    if SOUNDFILE_AVAILABLE:
                        data, fs = sf.read(tts_wav_path, dtype='float32')
                        log_print("ğŸ”Š ìŒì„± ì¬ìƒ ì¤‘...", "tts_status")
                        sd.play(data, fs)
                        sd.wait()
                        log_print("âœ… ìŒì„± ì¬ìƒ ì™„ë£Œ", "tts_status")
                    else:
                        log_print("âš ï¸ soundfile ì—†ìŒ - ìŒì„± ì¬ìƒ ê±´ë„ˆëœ€", "tts_status")
                else:
                    print("âŒ TTS íŒŒì¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            except Exception as tts_e:
                print(f"âŒ TTS ì˜¤ë¥˜: {tts_e}")
                log_print(f"TTS ìƒì„¸ ì˜¤ë¥˜: {tts_e}", "tts_debug")
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        conversation_history.append({"user": prompt, "assistant": response})
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ë„ˆë¬´ ê¸¸ë©´ ìµœê·¼ 5ê°œë§Œ ìœ ì§€
        if len(conversation_history) > 5:
            conversation_history = conversation_history[-5:]
        
        print(f"\n{'-'*50}")

def build_conversation_context(history, current_prompt):
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±
    """
    if not history:
        return make_role_prompt(current_prompt)
    
    # ìµœê·¼ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context = "You are a helpful AI assistant. Here's the recent conversation context:\n\n"
    
    for i, conv in enumerate(history[-3:], 1):  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ í¬í•¨
        context += f"User: {conv['user']}\n"
        context += f"Assistant: {conv['assistant']}\n\n"
    
    context += f"Current User Input: {current_prompt}\n\n"
    context += "Please respond naturally, considering the conversation context. Keep your response conversational and helpful."
    
    return context

# ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë” í´ë˜ìŠ¤
class EnglishFinetuningModelLoader:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.loaded = False
        self.model_type = "none"  # "unified", "classification", "none"
    
    def load_unified_model(self, model_path="./finetuning/models/unified_model"):
        """í†µí•©í˜• ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ"""
        if not ENGLISH_FINETUNED_AVAILABLE:
            log_print("ì˜ì–´ íŒŒì¸íŠœë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹„í™œì„±í™”", "model_loading")
            return False
        
        try:
            if os.path.exists(model_path):
                log_print(f"ì˜ì–´ í†µí•© ëª¨ë¸ ë¡œë”©: {model_path}", "model_loading")
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None
                )
                
                self.loaded = True
                self.model_type = "unified"
                log_print("ì˜ì–´ í†µí•© íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ", "model_loading")
                return True
            else:
                log_print(f"ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {model_path}", "model_loading")
                return False
        
        except Exception as e:
            log_print(f"ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", "model_loading")
            return False
    
    def generate_response(self, prompt, max_tokens=150):
        """ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±"""
        if not self.loaded:
            return None
        
        try:
            # ì˜ì–´ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í¬ë§·
            if self.model_type == "unified":
                formatted_prompt = f"System: You are a helpful AI assistant that can handle various types of conversations including Q&A, technical support, and general chat.\n\nUser: {prompt}\nAssistant:"
            else:
                formatted_prompt = f"User: {prompt}\nAssistant:"
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt")
            
            if torch.cuda.is_available() and hasattr(self.model, 'device'):
                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=3
                )
            
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Assistant ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "Assistant:" in full_response:
                response = full_response.split("Assistant:")[-1].strip()
                return response
            else:
                return full_response.replace(formatted_prompt, "").strip()
        
        except Exception as e:
            log_print(f"ì˜ì–´ íŒŒì¸íŠœë‹ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}", "model_loading")
            return None

# ì „ì—­ ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
english_finetuned_model = EnglishFinetuningModelLoader()

# RTX 3070 ìµœì í™” ëª¨ë¸ í´ë˜ìŠ¤ (ì–¸ì–´ ì œí•œ ì˜µì…˜ í¬í•¨)
class RTX3070OptimizedLoader:
    def __init__(self, language_restriction=False):
        self.model = None
        self.tokenizer = None
        self.loaded = False
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.language_restriction = language_restriction
        
        # ì–¸ì–´ ì œí•œ ì„¤ì •
        if language_restriction:
            import re
            self.language_patterns = {
                'ko': re.compile(r'[\uAC00-\uD7A3]'),  # í•œê¸€
                'en': re.compile(r'[A-Za-z]'),          # ì˜ì–´
                'other': re.compile(r'[\u4E00-\u9FFF\u3040-\u309F\u30A0-\u30FF\u0400-\u04FF\u0600-\u06FF]')  # ê¸°íƒ€ ì–¸ì–´
            }
    
    def load_model(self):
        """RTX 3070 ìµœì í™” ëª¨ë¸ ë¡œë“œ"""
        try:
            from transformers import BitsAndBytesConfig
            from peft import PeftModel
            
            # ëª¨ë¸ ê²½ë¡œ ì°¾ê¸°
            model_paths = ["./models/rtx3070_optimized_best", "./models/rtx3070_optimized_final"]
            model_path = None
            
            for path in model_paths:
                if os.path.exists(path):
                    model_path = path
                    break
            
            if not model_path:
                mode_text = "ì–¸ì–´ ì œí•œ" if self.language_restriction else "ë¬´ì œí•œ"
                log_print(f"RTX 3070 ìµœì í™” ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ ({mode_text})", "model_loading")
                return False
            
            mode_text = "ì–¸ì–´ ì œí•œ" if self.language_restriction else "ë¬´ì œí•œ"
            log_print(f"RTX 3070 {mode_text} ëª¨ë¸ ë¡œë”©: {model_path}", "model_loading")
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # 4bit ì–‘ìí™” ì„¤ì •
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
            )
            
            # ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
            base = AutoModelForCausalLM.from_pretrained(
                "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
                quantization_config=bnb_config,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
            
            # LoRA ì–´ëŒ‘í„° ë¡œë“œ
            self.model = PeftModel.from_pretrained(base, model_path)
            self.model.eval()
            
            self.loaded = True
            log_print(f"RTX 3070 {mode_text} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ", "model_loading")
            return True
            
        except Exception as e:
            mode_text = "ì–¸ì–´ ì œí•œ" if self.language_restriction else "ë¬´ì œí•œ"
            log_print(f"RTX 3070 {mode_text} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}", "model_loading")
            return False
    
    def detect_language(self, text):
        """ì–¸ì–´ ê°ì§€ (ì–¸ì–´ ì œí•œ ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)"""
        if not self.language_restriction:
            return 'unlimited'
        
        korean_chars = len(self.language_patterns['ko'].findall(text))
        english_chars = len(self.language_patterns['en'].findall(text))
        other_chars = len(self.language_patterns['other'].findall(text))
        
        total_chars = korean_chars + english_chars + other_chars
        
        if total_chars == 0:
            return 'en'
        
        if other_chars / total_chars > 0.2:
            return 'other'
        
        if korean_chars > english_chars:
            return 'ko'
        else:
            return 'en'
    
    def validate_language(self, user_input):
        """ì–¸ì–´ ìœ íš¨ì„± ê²€ì‚¬ (ì–¸ì–´ ì œí•œ ëª¨ë“œì—ì„œë§Œ ì‚¬ìš©)"""
        if not self.language_restriction:
            return True, 'unlimited'
        
        detected_lang = self.detect_language(user_input)
        
        if detected_lang == 'other':
            return False, "âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì–¸ì–´ì…ë‹ˆë‹¤. ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´ë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
        
        return True, detected_lang
    
    def generate_response(self, prompt, max_tokens=200):
        """ì‘ë‹µ ìƒì„± (ì–¸ì–´ ì œí•œ ì˜µì…˜ í¬í•¨)"""
        if not self.loaded:
            return None
        
        # ì–¸ì–´ ì œí•œ ëª¨ë“œì—ì„œ ì–¸ì–´ ìœ íš¨ì„± ê²€ì‚¬
        if self.language_restriction:
            is_valid, lang_result = self.validate_language(prompt)
            if not is_valid:
                return lang_result
            
            detected_lang = lang_result
            log_print(f"ì–¸ì–´ ê°ì§€ë¨: {detected_lang}", "model_loading")
        
        try:
            # í”„ë¡¬í”„íŠ¸ í˜•ì‹í™”
            if self.language_restriction:
                # ì–¸ì–´ ì œí•œ ëª¨ë“œì—ì„œëŠ” ë” ì—„ê²©í•œ í”„ë¡¬í”„íŠ¸
                formatted_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
            else:
                # ë¬´ì œí•œ ëª¨ë“œì—ì„œëŠ” ë‹¨ìˆœí•œ í”„ë¡¬í”„íŠ¸
                formatted_prompt = f"User: {prompt}\nAssistant:"
            
            inputs = self.tokenizer.encode(formatted_prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.9,
                    do_sample=True,
                    top_p=0.95,
                    top_k=40,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # ì‘ë‹µì—ì„œ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°
            if self.language_restriction:
                if "<|im_start|>assistant\n" in response:
                    response = response.split("<|im_start|>assistant\n")[-1]
                if "<|im_end|>" in response:
                    response = response.split("<|im_end|>")[0]
            else:
                if "Assistant:" in response:
                    response = response.split("Assistant:")[-1]
            
            return response.strip()
        
        except Exception as e:
            mode_text = "ì–¸ì–´ ì œí•œ" if self.language_restriction else "ë¬´ì œí•œ"
            log_print(f"RTX 3070 {mode_text} ì‘ë‹µ ìƒì„± ì‹¤íŒ¨: {e}", "model_loading")
            return None

# ì „ì—­ RTX 3070 ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë“¤ (ëª¨ë‘ ì˜ì–´/í•œêµ­ì–´ ì œí•œ)
rtx3070_unfiltered_model = RTX3070OptimizedLoader(language_restriction=True)
rtx3070_language_limited_model = RTX3070OptimizedLoader(language_restriction=True)

# ëª¨ë¸ ì„ íƒ ìƒíƒœ ì €ì¥
MODEL_SELECTION = {
    "current": "original",  # "original", "hybrid", "english_unified"
    "available_models": []
}

def detect_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ê°ì§€"""
    available = ["original"]  # ê¸°ë³¸ ëª¨ë¸ì€ í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
    
    # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ëª¨ë¸ í™•ì¸
    if FINETUNING_ENABLED:
        try:
            stats = get_finetuning_stats()
            if stats.get('status') == 'active':
                available.append("hybrid")
        except:
            pass
    
    # ì˜ì–´ íŒŒì¸íŠœë‹ ëª¨ë¸ í™•ì¸
    if ENGLISH_FINETUNED_AVAILABLE:
        unified_path = "./finetuning/models/unified_model"
        if os.path.exists(unified_path):
            available.append("english_unified")
    
    # RTX 3070 ìµœì í™” ëª¨ë¸ë“¤ í™•ì¸
    rtx_best_path = "./models/rtx3070_optimized_best"
    rtx_final_path = "./models/rtx3070_optimized_final"
    
    if os.path.exists(rtx_best_path):
        available.append("rtx3070_unfiltered")
        available.append("rtx3070_language_limited")
    elif os.path.exists(rtx_final_path):
        available.append("rtx3070_unfiltered")
        available.append("rtx3070_language_limited")
    
    MODEL_SELECTION["available_models"] = available
    log_print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available}", "model_loading")
    return available

def switch_model(model_type):
    """ëª¨ë¸ ì „í™˜"""
    if model_type not in MODEL_SELECTION["available_models"]:
        return False, f"ëª¨ë¸ '{model_type}'ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # RTX 3070 ë¬´ì œí•œ ëª¨ë¸ ë¡œë“œ
    if model_type == "rtx3070_unfiltered":
        if rtx3070_unfiltered_model.load_model():
            MODEL_SELECTION["current"] = model_type
            return True, "RTX 3070 ìµœì í™” (ë¬´ì œí•œ) ëª¨ë¸ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            return False, "RTX 3070 ë¬´ì œí•œ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    # RTX 3070 ì–¸ì–´ ì œí•œ ëª¨ë¸ ë¡œë“œ
    elif model_type == "rtx3070_language_limited":
        if rtx3070_language_limited_model.load_model():
            MODEL_SELECTION["current"] = model_type
            return True, "RTX 3070 ìµœì í™” (ì–¸ì–´ ì œí•œ) ëª¨ë¸ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            return False, "RTX 3070 ì–¸ì–´ ì œí•œ ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    # ì˜ì–´ í†µí•© ëª¨ë¸ ë¡œë“œ
    elif model_type == "english_unified":
        if english_finetuned_model.load_unified_model():
            MODEL_SELECTION["current"] = model_type
            return True, "ì˜ì–´ í†µí•© íŒŒì¸íŠœë‹ ëª¨ë¸ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."
        else:
            return False, "ì˜ì–´ í†µí•© ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    # ë‹¤ë¥¸ ëª¨ë¸ë“¤
    MODEL_SELECTION["current"] = model_type
    return True, f"ëª¨ë¸ì´ '{model_type}'ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤."

def get_model_info():
    """í˜„ì¬ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    current = MODEL_SELECTION["current"]
    available = MODEL_SELECTION["available_models"]
    
    model_names = {
        "original": "ê¸°ë³¸ Nous-Hermes-2-Mistral",
        "hybrid": "í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ (í•œêµ­ì–´)",
        "english_unified": "ì˜ì–´ í†µí•© íŒŒì¸íŠœë‹",
        "rtx3070_unfiltered": "RTX 3070 ìµœì í™” (ì˜ì–´/í•œêµ­ì–´)",
        "rtx3070_language_limited": "RTX 3070 ìµœì í™” (ì˜ì–´/í•œêµ­ì–´)"
    }
    
    return {
        "current": current,
        "current_name": model_names.get(current, current),
        "available": available,
        "available_names": [model_names.get(m, m) for m in available]
    }

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    main()