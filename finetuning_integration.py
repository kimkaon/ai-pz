#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ í†µí•© ì‹œìŠ¤í…œ (ë©”ì¸ ì¸í„°í˜ì´ìŠ¤)
Hybrid Fine-tuning Integration System (Main Interface)

í†µí•©í˜• ëª¨ë¸ ê¸°ë³¸ + ì „ë¬¸ëª¨ë¸ ë™ì  ë¡œë”© ë°©ì‹
Unified model as primary + dynamic specialist model loading
"""

import sys
from pathlib import Path
import logging

# í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from hybrid_finetuning_integration import process_with_finetuned_models, get_system_stats
    HYBRID_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì„í¬íŠ¸ ì‹¤íŒ¨ / Failed to import hybrid system: {e}")
    HYBRID_AVAILABLE = False

# ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ fallback
try:
    from prompt_templates import make_role_prompt
    PROMPT_TEMPLATES_AVAILABLE = True
except ImportError:
    PROMPT_TEMPLATES_AVAILABLE = False

def process_with_finetuned_models_main(user_input: str, enable_specialist: bool = True) -> dict:
    """
    ë©”ì¸ íŒŒì¸íŠœë‹ ì²˜ë¦¬ í•¨ìˆ˜ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
    Main fine-tuning processing function (hybrid approach)
    
    Args:
        user_input: ì‚¬ìš©ì ì…ë ¥
        enable_specialist: ì „ë¬¸ëª¨ë¸ ì‚¬ìš© í—ˆìš© ì—¬ë¶€
        
    Returns:
        dict: ì‘ë‹µ ê²°ê³¼
    """
    
    if not HYBRID_AVAILABLE:
        return {
            'response': f"í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì‘ë‹µ: {user_input}",
            'category': 'fallback',
            'model_used': 'none',
            'quality_level': 'basic',
            'can_upgrade': False,
            'error': 'Hybrid system not available'
        }
    
    try:
        # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬
        result = process_with_finetuned_models(
            user_input, 
            force_specialist=False  # ìë™ ê²°ì • ë°©ì‹ ì‚¬ìš©
        )
        
        # ì „ë¬¸ëª¨ë¸ ë¹„í™œì„±í™” ì‹œ í†µí•©ëª¨ë¸ë§Œ ì‚¬ìš©
        if not enable_specialist and result.get('model_used', '').endswith('_specialist'):
            # í†µí•©ëª¨ë¸ë¡œ ì¬ì²˜ë¦¬
            result = process_with_finetuned_models(user_input, force_specialist=False)
            # ê°•ì œë¡œ í†µí•©ëª¨ë¸ ê²°ê³¼ë¡œ ë³€ê²½
            result['model_used'] = 'unified'
            result['quality_level'] = 'standard'
            result['response'] = result['response'].replace('[QNA SPECIALIST]', '[UNIFIED]')
            # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì—ì„œëŠ” daily_chatì´ í†µí•©ëª¨ë¸ì—ì„œ ì²˜ë¦¬ë˜ë¯€ë¡œ UNIFIEDë¡œ í‘œì‹œ
            result['response'] = result['response'].replace('[DAILY_CHAT SPECIALIST]', '[UNIFIED]')
            result['response'] = result['response'].replace('[TECHNICAL SPECIALIST]', '[UNIFIED]')
        
        return result
        
    except Exception as e:
        logging.error(f"í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì˜¤ë¥˜ / Hybrid processing error: {e}")
        
        # Fallback to prompt templates
        if PROMPT_TEMPLATES_AVAILABLE:
            try:
                prompt = make_role_prompt(user_input)
                return {
                    'response': f"[FALLBACK] {user_input}ì— ëŒ€í•œ ê¸°ë³¸ ì‘ë‹µì…ë‹ˆë‹¤.",
                    'category': 'fallback',
                    'model_used': 'prompt_template',
                    'quality_level': 'basic',
                    'can_upgrade': True,
                    'error': f'Hybrid error: {str(e)}'
                }
            except Exception as fallback_error:
                logging.error(f"Fallback ì˜¤ë¥˜ / Fallback error: {fallback_error}")
        
        # Final fallback
        return {
            'response': f"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤: {user_input}",
            'category': 'error',
            'model_used': 'none',
            'quality_level': 'error',
            'can_upgrade': False,
            'error': f'All systems failed: {str(e)}'
        }

def get_finetuning_stats() -> dict:
    """íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ / Get fine-tuning system statistics"""
    
    if not HYBRID_AVAILABLE:
        return {
            'status': 'unavailable',
            'error': 'Hybrid system not available'
        }
    
    try:
        stats = get_system_stats()
        stats['status'] = 'active'
        stats['system_type'] = 'hybrid'
        return stats
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'system_type': 'hybrid'
        }

def request_specialist_mode(user_input: str, category: str = None) -> dict:
    """
    ì „ë¬¸ê°€ ëª¨ë“œ ëª…ì‹œì  ìš”ì²­
    Explicit request for specialist mode
    
    Args:
        user_input: ì‚¬ìš©ì ì…ë ¥
        category: ì„ íƒì  ì¹´í…Œê³ ë¦¬ ì§€ì •
        
    Returns:
        dict: ì „ë¬¸ê°€ ì‘ë‹µ ê²°ê³¼
    """
    
    if not HYBRID_AVAILABLE:
        return process_with_finetuned_models_main(user_input, enable_specialist=False)
    
    try:
        # ì „ë¬¸ê°€ ëª¨ë“œë¡œ ê°•ì œ ì²˜ë¦¬
        result = process_with_finetuned_models(user_input, force_specialist=True)
        result['specialist_requested'] = True
        return result
        
    except Exception as e:
        logging.error(f"ì „ë¬¸ê°€ ëª¨ë“œ ì˜¤ë¥˜ / Specialist mode error: {e}")
        return process_with_finetuned_models_main(user_input, enable_specialist=False)

def cleanup_finetuning_system():
    """íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ ì •ë¦¬ / Cleanup fine-tuning system"""
    
    if HYBRID_AVAILABLE:
        try:
            # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì •ë¦¬ (í•„ìš”ì‹œ êµ¬í˜„)
            pass
        except Exception as e:
            logging.error(f"ì‹œìŠ¤í…œ ì •ë¦¬ ì˜¤ë¥˜ / System cleanup error: {e}")

# í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•¨ìˆ˜ëª… ìœ ì§€
def integrate_finetuned_response(user_input: str) -> str:
    """
    ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜
    Wrapper function for backward compatibility
    """
    result = process_with_finetuned_models_main(user_input)
    return result.get('response', user_input)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    test_cases = [
        ("ì•ˆë…•í•˜ì„¸ìš”!", "ì¼ìƒ ì¸ì‚¬"),
        ("íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë°ì´ ë­ì˜ˆìš”?", "ê¸°ìˆ  ì§ˆë¬¸"),
        ("ì§€êµ¬ì˜ ë‚˜ì´ëŠ”?", "ì‚¬ì‹¤ ì§ˆë¬¸"), 
        ("SketchUp ì „ë¬¸ê°€ ëª¨ë“œë¡œ ê³ ê¸‰ ëª¨ë¸ë§ ì•Œë ¤ì£¼ì„¸ìš”", "ì „ë¬¸ê°€ ìš”ì²­"),
        ("ìš”ì¦˜ ë„ˆë¬´ ìš°ìš¸í•´ìš”...", "ê°ì • ìƒë‹´")
    ]
    
    for user_input, description in test_cases:
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸: {description}")
        print(f"ì…ë ¥: {user_input}")
        
        # ì¼ë°˜ ëª¨ë“œ
        result = process_with_finetuned_models_main(user_input)
        print(f"ğŸ¤– [{result['model_used']}] {result['response'][:80]}...")
        print(f"ğŸ“Š ì¹´í…Œê³ ë¦¬: {result['category']}, í’ˆì§ˆ: {result['quality_level']}")
        
        # ì „ë¬¸ê°€ ëª¨ë“œ (ì—…ê·¸ë ˆì´ë“œ ê°€ëŠ¥í•œ ê²½ìš°)
        if result.get('can_upgrade'):
            print("ğŸ¯ ì „ë¬¸ê°€ ëª¨ë“œ ì‹œë„...")
            specialist_result = request_specialist_mode(user_input)
            print(f"ğŸ“ [{specialist_result['model_used']}] {specialist_result['response'][:80]}...")
    
    # ì‹œìŠ¤í…œ í†µê³„
    print(f"\nğŸ“ˆ ì‹œìŠ¤í…œ í†µê³„:")
    stats = get_finetuning_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
