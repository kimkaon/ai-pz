"""
ì‘ë‹µ ì²˜ë¦¬ ëª¨ë“ˆ - ë¦¬íŒ©í„°ë§ëœ ë²„ì „
main.pyì—ì„œ ë¶„ë¦¬ëœ ì‘ë‹µ ìƒì„± ë° ì²˜ë¦¬ ê¸°ëŠ¥
"""

import os
from utils.logging_utils import log_print, clean_llm_response, settings_manager
from core.initialization import get_global_llm

# íŒŒì¸íŠœë‹ ê¸°ëŠ¥ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    from finetuning_integration import process_with_finetuned_models_main, get_finetuning_stats
    FINETUNING_ENABLED = True
except ImportError:
    FINETUNING_ENABLED = False

class ResponseHandler:
    """ì‘ë‹µ ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.response_stats = {
            'total_responses': 0,
            'finetuned_responses': 0,
            'fallback_responses': 0
        }
    
    def generate_response(self, prompt, conversation_history=None):
        """í†µí•© ì‘ë‹µ ìƒì„±"""
        try:
            self.response_stats['total_responses'] += 1
            
            # 1. íŒŒì¸íŠœë‹ ëª¨ë¸ ìš°ì„  ì‹œë„
            response, used_finetuned, model_info = self.try_finetuned_response(prompt)
            if used_finetuned and response:
                self.response_stats['finetuned_responses'] += 1
                log_print(f"íŒŒì¸íŠœë‹ ì‘ë‹µ ì‚¬ìš©: {model_info.get('model_used', 'unknown')}", "model_loading")
                return clean_llm_response(response)
            
            # 2. ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
            response = self._generate_basic_response(prompt, conversation_history)
            if response:
                self.response_stats['fallback_responses'] += 1
                log_print("ê¸°ë³¸ ëª¨ë¸ ì‘ë‹µ ì‚¬ìš©", "model_loading")
                return clean_llm_response(response)
            
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            log_print(f"ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}", "error")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
    
    def try_finetuned_response(self, prompt):
        """íŒŒì¸íŠœë‹ ì‘ë‹µ ì‹œë„"""
        current_model = settings_manager.get('model.current', 'original')
        
        # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ëª¨ë¸
        if FINETUNING_ENABLED and current_model in ['hybrid', 'english_unified']:
            try:
                response_data = process_with_finetuned_models_main(prompt)
                if response_data and response_data.get('response'):
                    model_info = {
                        'model_used': current_model,
                        'category': response_data.get('category', 'unknown'),
                        'confidence': response_data.get('confidence', 0.0),
                        'quality_level': response_data.get('quality_level', 'unknown')
                    }
                    return response_data['response'], True, model_info
            except Exception as e:
                log_print(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ì˜¤ë¥˜: {e}", "model_loading")
        
        # RTX 3070 íŠ¹í™” ëª¨ë¸ë“¤
        elif current_model in ['rtx3070_unfiltered', 'rtx3070_language_limited', 'rtx3070_gguf']:
            try:
                # ì „ì—­ model_manager ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§ì ‘ ì‚¬ìš©
                import models.model_manager as mm
                model_manager = mm.model_manager
                
                log_print(f"ğŸ” ResponseHandlerì—ì„œ ëª¨ë¸ ë§¤ë‹ˆì € ìƒíƒœ í™•ì¸: {id(model_manager)}", "model_loading")
                
                if current_model == 'rtx3070_unfiltered':
                    response = model_manager.rtx3070_unfiltered_model.generate_response(prompt)
                elif current_model == 'rtx3070_language_limited':
                    response = model_manager.rtx3070_language_limited_model.generate_response(prompt)
                elif current_model == 'rtx3070_gguf':
                    response = model_manager.rtx3070_gguf_model.generate_response(prompt)
                
                if response:
                    model_info = {
                        'model_used': current_model,
                        'model_type': 'rtx3070_finetuned'
                    }
                    return response, True, model_info
                    
            except Exception as e:
                log_print(f"RTX 3070 ëª¨ë¸ ì˜¤ë¥˜: {e}", "model_loading")
        
        # ì˜ì–´ í†µí•© ëª¨ë¸
        elif current_model == 'english_unified':
            try:
                from models.model_manager import model_manager
                response = model_manager.english_finetuned_model.generate_response(prompt)
                if response:
                    model_info = {
                        'model_used': current_model,
                        'model_type': 'english_finetuned'
                    }
                    return response, True, model_info
            except Exception as e:
                log_print(f"ì˜ì–´ í†µí•© ëª¨ë¸ ì˜¤ë¥˜: {e}", "model_loading")
        
        return None, False, {}
    
    def _generate_basic_response(self, prompt, conversation_history=None):
        """ê¸°ë³¸ ëª¨ë¸ ì‘ë‹µ ìƒì„±"""
        llm = get_global_llm()
        if not llm:
            return None
        
        try:
            from prompt_templates import make_default_prompt
            formatted_prompt = make_default_prompt(prompt)
            
            from nous_hermes2_mistral_loader import chat_nous_hermes2_mistral
            response = chat_nous_hermes2_mistral(llm, formatted_prompt)
            return response
            
        except Exception as e:
            log_print(f"ê¸°ë³¸ ëª¨ë¸ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}", "error")
            return None
    
    def get_stats(self):
        """ì‘ë‹µ í†µê³„ ë°˜í™˜"""
        return self.response_stats.copy()
