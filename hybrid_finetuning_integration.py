#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ í†µí•© ì‹œìŠ¤í…œ
Hybrid Fine-tuning Integration System

í†µí•©í˜• ëª¨ë¸ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ë˜, í•„ìš”ì‹œ ì „ë¬¸ëª¨ë¸ì„ ë™ì  ë¡œë”©í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ
Uses unified model as primary, with dynamic loading of specialist models when needed
"""

import json
import os
import sys
from pathlib import Path
import logging
from typing import Dict, Optional, Tuple, List
import threading
import time
import gc

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent
sys.path.append(str(current_dir))

from prompt_templates import make_role_prompt, ROLE_TYPES

# ë©”ì¸ LLM ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from nous_hermes2_mistral_loader import chat_nous_hermes2_mistral
    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    chat_nous_hermes2_mistral = None

# ì „ì—­ LLM ì¸ìŠ¤í„´ìŠ¤ (ë©”ì¸ì—ì„œ ì „ë‹¬ë°›ìŒ)
GLOBAL_LLM = None

def set_global_llm(llm_instance):
    """ë©”ì¸ì—ì„œ LLM ì¸ìŠ¤í„´ìŠ¤ ì„¤ì •"""
    global GLOBAL_LLM
    GLOBAL_LLM = llm_instance
import gc

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
current_dir = Path(__file__).parent.parent
sys.path.append(str(current_dir))

from prompt_templates import make_role_prompt, ROLE_TYPES

class HybridFinetunedSystem:
    """í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ / Hybrid fine-tuned system"""
    
    def __init__(self):
        self.finetuning_dir = Path(__file__).parent / "finetuning"
        self.models_dir = self.finetuning_dir / "models"
        
        # ëª¨ë¸ ìƒíƒœ / Model states
        self.unified_model = None
        self.specialist_models = {
            'qna': None,
            'technical': None
        }
        
        # ëª¨ë¸ ë¡œë”© ìƒíƒœ / Model loading states
        self.model_loading = {
            'qna': False,
            'technical': False
        }
        
        # ì„±ëŠ¥ í†µê³„ / Performance statistics
        self.stats = {
            'unified_responses': 0,
            'specialist_responses': 0,
            'model_switches': 0,
            'avg_response_time': 0.0
        }
        
        # ë¡œê¹… ì„¤ì • / Logging setup
        self.setup_logging()
        
    def setup_logging(self):
        """ë¡œê¹… ì„¤ì • / Setup logging"""
        log_dir = self.finetuning_dir / "logs"
        log_dir.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / "hybrid_system.log"),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger("HybridSystem")
        
    def initialize_unified_model(self) -> bool:
        """í†µí•©í˜• ëª¨ë¸ ì´ˆê¸°í™” / Initialize unified model"""
        try:
            self.logger.info("ğŸš€ í†µí•©í˜• ëª¨ë¸ ì´ˆê¸°í™” ì¤‘... / Initializing unified model...")
            
            # í†µí•©í˜• ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ í™•ì¸ / Check unified model checkpoint
            unified_model_path = self.models_dir / "unified_model" / "model.safetensors"
            
            if unified_model_path.exists():
                self.logger.info("âœ… íŒŒì¸íŠœë‹ëœ í†µí•©í˜• ëª¨ë¸ ë°œê²¬ / Found fine-tuned unified model")
                # ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì½”ë“œëŠ” êµ¬í˜„ì— ë”°ë¼ ì¶”ê°€
                # self.unified_model = load_model(unified_model_path)
                self.unified_model = "unified_finetuned"  # í”Œë ˆì´ìŠ¤í™€ë”
            else:
                self.logger.info("ğŸ“ ê¸°ë³¸ LLM ì‚¬ìš© (í†µí•©í˜• ë¯¸íŒŒì¸íŠœë‹) / Using base LLM (unified not fine-tuned)")
                self.unified_model = "base_llm"  # í”Œë ˆì´ìŠ¤í™€ë”
                
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•©í˜• ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨ / Failed to initialize unified model: {e}")
            return False
    
    def classify_and_assess_confidence(self, user_input: str) -> Tuple[str, float, str]:
        """
        ì…ë ¥ ë¶„ë¥˜ ë° ì‹ ë¢°ë„ í‰ê°€ / Classify input and assess confidence
        
        Returns:
            tuple: (category, confidence, reason)
        """
        try:
            # í†µí•©í˜• ëª¨ë¸ì„ ì‚¬ìš©í•œ ë¶„ë¥˜ ë° ì‘ë‹µ ìƒì„±
            # ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¶”ë¡ ì„ í†µí•´ ë¶„ë¥˜ì™€ ì‹ ë¢°ë„ë¥¼ ì–»ìŒ
            
            # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ ì¶”ë¡ ìœ¼ë¡œ ëŒ€ì²´)
            user_lower = user_input.lower()
            
            # QnA íŒ¨í„´ ê°ì§€
            qna_keywords = ['what', 'how', 'why', 'when', 'where', 'who', 'explain', 'define', '?']
            if any(keyword in user_lower for keyword in qna_keywords):
                category = 'qna'
                # ë³µì¡ë„ ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°
                if len(user_input.split()) > 10 or 'complex' in user_lower or 'advanced' in user_lower:
                    confidence = 0.6  # ë³µì¡í•œ ì§ˆë¬¸ì€ ë‚®ì€ ì‹ ë¢°ë„
                    reason = "Complex question detected"
                else:
                    confidence = 0.8  # ê°„ë‹¨í•œ ì§ˆë¬¸ì€ ë†’ì€ ì‹ ë¢°ë„
                    reason = "Simple factual question"
                    
            # ê°ì •/ì¼ìƒ ëŒ€í™” íŒ¨í„´ ê°ì§€ (í†µí•©ëª¨ë¸ì—ì„œ ì²˜ë¦¬)
            elif any(word in user_lower for word in ['feel', 'emotion', 'sad', 'happy', 'stress', 'tired', 'good morning', 'how are you']):
                category = 'general'
                if any(word in user_lower for word in ['depressed', 'anxiety', 'therapy', 'counseling']):
                    confidence = 0.7  # ê°ì • ìƒë‹´ë„ í†µí•©ëª¨ë¸ì—ì„œ ì²˜ë¦¬ (í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ)
                    reason = "Emotional conversation handled by unified model"
                else:
                    confidence = 0.9  # ì¼ë°˜ ì¼ìƒëŒ€í™”ëŠ” í†µí•©ëª¨ë¸ë¡œ ì¶©ë¶„
                    reason = "General daily conversation"
                    
            # ê¸°ìˆ /í”„ë¡œê·¸ë¨ ê´€ë ¨ íŒ¨í„´ ê°ì§€
            elif any(word in user_lower for word in ['sketchup', 'cad', 'programming', 'code', 'software', 'technical', 'algorithm']):
                category = 'technical'
                if any(word in user_lower for word in ['advanced', 'professional', 'expert', 'complex']):
                    confidence = 0.4  # ê³ ê¸‰ ê¸°ìˆ  ì§ˆë¬¸ì€ ì „ë¬¸ëª¨ë¸ í•„ìš”
                    reason = "Advanced technical consultation required"
                else:
                    confidence = 0.7  # ê¸°ë³¸ ê¸°ìˆ  ì§ˆë¬¸ì€ í†µí•©ëª¨ë¸ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
                    reason = "Basic technical question"
                    
            else:
                category = 'general'
                confidence = 0.9  # ì¼ë°˜ ëŒ€í™”ëŠ” í†µí•©ëª¨ë¸ë¡œ ì¶©ë¶„
                reason = "General conversation"
                
            return category, confidence, reason
            
        except Exception as e:
            self.logger.error(f"âŒ ë¶„ë¥˜ ì˜¤ë¥˜ / Classification error: {e}")
            return 'general', 0.5, "Error in classification"
    
    def generate_unified_response(self, user_input: str, category: str) -> str:
        """í†µí•©í˜• ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„± / Generate response using unified model"""
        try:
            self.logger.info(f"ğŸ¤– í†µí•©í˜• ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘... / Generating unified response...")
            
            # ì‹¤ì œ LLM í˜¸ì¶œ
            if GLOBAL_LLM and LLM_AVAILABLE:
                if self.unified_model == "unified_finetuned":
                    # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì‚¬ìš©
                    prompt = f"""You are a versatile AI assistant. Based on the category '{category}', respond appropriately to the user's question.

User: {user_input}
Assistant:"""
                else:
                    # ê¸°ë³¸ LLM ì‚¬ìš© (í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™œìš©)
                    prompt = make_role_prompt(user_input)
                
                # ì‹¤ì œ LLM í˜¸ì¶œ
                response = chat_nous_hermes2_mistral(GLOBAL_LLM, prompt)
                
            else:
                # Fallback: LLMì´ ì—†ì„ ë•Œ
                response = f"[Base LLM] {user_input}ì— ëŒ€í•œ ê¸°ë³¸ ì‘ë‹µì…ë‹ˆë‹¤."
            
            self.stats['unified_responses'] += 1
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"âŒ í†µí•©í˜• ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ / Failed to generate unified response: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def load_specialist_model(self, category: str) -> bool:
        """ì „ë¬¸ëª¨ë¸ ë™ì  ë¡œë”© / Dynamic loading of specialist model"""
        if self.model_loading[category]:
            self.logger.info(f"â³ {category} ëª¨ë¸ ë¡œë”© ì¤‘... / Loading {category} model...")
            return False
            
        try:
            self.model_loading[category] = True
            self.logger.info(f"ğŸ”„ {category} ì „ë¬¸ëª¨ë¸ ë¡œë”© ì‹œì‘ / Starting to load {category} specialist model...")
            
            # ì „ë¬¸ëª¨ë¸ ê²½ë¡œ í™•ì¸ (ëª¨ì˜ ëª¨ë¸)
            specialist_path = self.models_dir / f"{category}_specialist" / "config.json"
            
            if specialist_path.exists():
                # ëª¨ì˜ ëª¨ë¸ ë¡œë”©
                with open(specialist_path, 'r', encoding='utf-8') as f:
                    model_config = json.load(f)
                
                # ëª¨ë¸ ë¡œë”© ì‹œë®¬ë ˆì´ì…˜
                time.sleep(1)  # ë¡œë”© ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                
                self.specialist_models[category] = model_config
                self.logger.info(f"âœ… {category} ì „ë¬¸ëª¨ë¸ ë¡œë”© ì™„ë£Œ / {category} specialist model loaded")
                return True
            else:
                self.logger.warning(f"âš ï¸ {category} ì „ë¬¸ëª¨ë¸ íŒŒì¼ ì—†ìŒ / {category} specialist model file not found")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ {category} ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ / Failed to load {category} model: {e}")
            return False
        finally:
            self.model_loading[category] = False
    
    def generate_specialist_response(self, user_input: str, category: str) -> str:
        """ì „ë¬¸ëª¨ë¸ë¡œ ê³ í’ˆì§ˆ ì‘ë‹µ ìƒì„± / Generate high-quality response using specialist model"""
        try:
            self.logger.info(f"ğŸ¯ {category} ì „ë¬¸ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘... / Generating specialist response...")
            
            if not self.specialist_models[category]:
                if not self.load_specialist_model(category):
                    return "ì „ë¬¸ëª¨ë¸ì„ ë¡œë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í†µí•©ëª¨ë¸ ì‘ë‹µì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
            
            # ëª¨ì˜ ì „ë¬¸ëª¨ë¸ ì‘ë‹µê³¼ ì‹¤ì œ LLM ê²°í•©
            model_config = self.specialist_models[category]
            responses = model_config.get('responses', {})
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì „ë¬¸ ì‘ë‹µ í…œí”Œë¦¿ ì„ íƒ
            user_lower = user_input.lower()
            specialist_context = responses.get('default', f"ì „ë¬¸ì ì¸ {category} ì‘ë‹µ")
            
            for keyword, template_response in responses.items():
                if keyword != 'default' and keyword in user_lower:
                    specialist_context = template_response
                    break
            
            # ì‹¤ì œ LLMì„ ì‚¬ìš©í•´ ì „ë¬¸ê°€ ì‘ë‹µ ìƒì„±
            if GLOBAL_LLM and LLM_AVAILABLE:
                # ì „ë¬¸ê°€ ëª¨ë“œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                if category == 'qna':
                    expert_prompt = f"""You are a knowledgeable QnA specialist. Provide accurate, detailed, and well-structured answers to questions.

User question: {user_input}

Provide a comprehensive, informative response:"""
                elif category == 'technical':
                    expert_prompt = f"""You are a technical specialist with expertise in software, programming, CAD, and technical consultation. Provide professional, detailed technical guidance.

Technical question: {user_input}

Provide expert technical guidance:"""
                else:
                    expert_prompt = f"""You are a specialist assistant for {category}. Provide expert-level responses.

User input: {user_input}

Response:"""
                
                response = chat_nous_hermes2_mistral(GLOBAL_LLM, expert_prompt)
                
            else:
                # Fallback: ëª¨ì˜ ì‘ë‹µ ì‚¬ìš©
                response = specialist_context
            
            self.stats['specialist_responses'] += 1
            self.stats['model_switches'] += 1
            
            return response.strip()
            
        except Exception as e:
            self.logger.error(f"âŒ ì „ë¬¸ëª¨ë¸ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ / Failed to generate specialist response: {e}")
            return self.generate_unified_response(user_input, category)
    
    def should_use_specialist(self, confidence: float, user_input: str, category: str) -> bool:
        """ì „ë¬¸ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ ê²°ì • / Decide whether to use specialist model"""
        
        # ëª…ì‹œì  ì „ë¬¸ê°€ ëª¨ë“œ ìš”ì²­
        expert_keywords = ['expert', 'professional', 'specialist', 'ì „ë¬¸ê°€', 'ì „ë¬¸ì ', 'ê³ ê¸‰', 'advanced']
        if any(keyword in user_input.lower() for keyword in expert_keywords):
            self.logger.info("ğŸ¯ ì‚¬ìš©ìê°€ ì „ë¬¸ê°€ ëª¨ë“œ ìš”ì²­ / User requested expert mode")
            return True
            
        # ì‹ ë¢°ë„ ê¸°ë°˜ ê²°ì •
        confidence_threshold = 0.7
        if confidence < confidence_threshold:
            self.logger.info(f"ğŸ“Š ë‚®ì€ ì‹ ë¢°ë„ë¡œ ì „ë¬¸ëª¨ë¸ ì‚¬ìš© / Using specialist due to low confidence: {confidence}")
            return True
            
        # ì¹´í…Œê³ ë¦¬ë³„ ë³µì¡ë„ ì„ê³„ê°’ (daily_chat ì œê±°)
        complexity_indicators = {
            'qna': ['complex', 'detailed', 'comprehensive', 'research'],
            'technical': ['advanced', 'professional', 'enterprise', 'optimization']
        }
        
        if category in complexity_indicators:
            if any(indicator in user_input.lower() for indicator in complexity_indicators[category]):
                self.logger.info(f"ğŸ”¬ ë³µì¡ë„ ì„ê³„ê°’ ì´ˆê³¼ë¡œ ì „ë¬¸ëª¨ë¸ ì‚¬ìš© / Using specialist due to complexity")
                return True
        
        return False
    
    def generate_response(self, user_input: str, force_specialist: bool = False) -> Dict:
        """
        ë©”ì¸ ì‘ë‹µ ìƒì„± í•¨ìˆ˜ / Main response generation function
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥
            force_specialist: ê°•ì œë¡œ ì „ë¬¸ëª¨ë¸ ì‚¬ìš©
            
        Returns:
            dict: ì‘ë‹µ ì •ë³´
        """
        start_time = time.time()
        
        try:
            self.logger.info(f"ğŸ“ ì…ë ¥ ì²˜ë¦¬ ì‹œì‘ / Starting input processing: {user_input[:50]}...")
            
            # 1. ë¶„ë¥˜ ë° ì‹ ë¢°ë„ í‰ê°€
            category, confidence, reason = self.classify_and_assess_confidence(user_input)
            
            # 2. ì „ë¬¸ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
            use_specialist = force_specialist or self.should_use_specialist(confidence, user_input, category)
            
            # 3. ì‘ë‹µ ìƒì„±
            if use_specialist and category in self.specialist_models:
                response = self.generate_specialist_response(user_input, category)
                model_used = f"{category}_specialist"
                quality_level = "high"
            else:
                response = self.generate_unified_response(user_input, category)
                model_used = "unified"
                quality_level = "standard"
            
            # 4. ì‘ë‹µ ì‹œê°„ ê³„ì‚°
            response_time = time.time() - start_time
            self.stats['avg_response_time'] = (self.stats['avg_response_time'] + response_time) / 2
            
            # 5. ê²°ê³¼ ë°˜í™˜
            result = {
                'response': response,
                'category': category,
                'confidence': confidence,
                'reason': reason,
                'model_used': model_used,
                'quality_level': quality_level,
                'response_time': response_time,
                'can_upgrade': not use_specialist and category in self.specialist_models
            }
            
            self.logger.info(f"âœ… ì‘ë‹µ ì™„ë£Œ / Response completed: {model_used} ({response_time:.2f}s)")
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ / Error during response generation: {e}")
            return {
                'response': "ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                'category': 'error',
                'confidence': 0.0,
                'reason': str(e),
                'model_used': 'error',
                'quality_level': 'error',
                'response_time': time.time() - start_time,
                'can_upgrade': False
            }
    
    def get_stats(self) -> Dict:
        """ì‹œìŠ¤í…œ í†µê³„ ë°˜í™˜ / Return system statistics"""
        return {
            **self.stats,
            'loaded_specialists': [k for k, v in self.specialist_models.items() if v is not None],
            'unified_model_loaded': self.unified_model is not None
        }
    
    def unload_specialist(self, category: str):
        """ì „ë¬¸ëª¨ë¸ ì–¸ë¡œë“œ (ë©”ëª¨ë¦¬ ì ˆì•½) / Unload specialist model (save memory)"""
        if self.specialist_models[category]:
            self.specialist_models[category] = None
            gc.collect()
            self.logger.info(f"ğŸ—‘ï¸ {category} ì „ë¬¸ëª¨ë¸ ì–¸ë¡œë“œ / {category} specialist model unloaded")
    
    def cleanup(self):
        """ì‹œìŠ¤í…œ ì •ë¦¬ / System cleanup"""
        self.logger.info("ğŸ§¹ ì‹œìŠ¤í…œ ì •ë¦¬ ì¤‘... / Cleaning up system...")
        for category in self.specialist_models:
            self.unload_specialist(category)
        self.unified_model = None
        gc.collect()
        self.logger.info("âœ… ì •ë¦¬ ì™„ë£Œ / Cleanup completed")

# ì „ì—­ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
_hybrid_system = None

def get_hybrid_system():
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ê°€ì ¸ì˜¤ê¸° / Get hybrid system singleton"""
    global _hybrid_system
    if _hybrid_system is None:
        _hybrid_system = HybridFinetunedSystem()
        _hybrid_system.initialize_unified_model()
    return _hybrid_system

def process_with_finetuned_models(user_input: str, force_specialist: bool = False) -> Dict:
    """
    í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œìœ¼ë¡œ ì…ë ¥ ì²˜ë¦¬
    Process input with hybrid fine-tuning system
    
    Args:
        user_input: ì‚¬ìš©ì ì…ë ¥
        force_specialist: ê°•ì œë¡œ ì „ë¬¸ëª¨ë¸ ì‚¬ìš©
        
    Returns:
        dict: ì‘ë‹µ ì •ë³´
    """
    system = get_hybrid_system()
    return system.generate_response(user_input, force_specialist)

def get_system_stats() -> Dict:
    """ì‹œìŠ¤í…œ í†µê³„ ì¡°íšŒ / Get system statistics"""
    system = get_hybrid_system()
    return system.get_stats()

def initialize_hybrid_system(llm_instance=None):
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” / Initialize hybrid system"""
    if llm_instance:
        set_global_llm(llm_instance)
    
    system = get_hybrid_system()
    return system

def unload_specialists():
    """ì „ë¬¸ëª¨ë¸ ì–¸ë¡œë“œ / Unload specialist models"""
    system = get_hybrid_system()
    for category in ['qna', 'technical']:
        system.unload_specialist(category)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì¸íŠœë‹ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ / Hybrid Fine-tuning System Test")
    print("=" * 80)
    
    test_inputs = [
        "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ ê¸°ë¶„ ì–´ë•Œìš”?",  # ì¼ìƒëŒ€í™”
        "íŒŒì´ì¬ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì–´ë–»ê²Œ êµ¬í˜„í•˜ë‚˜ìš”?",  # ê¸°ìˆ ì§ˆë¬¸  
        "ìš°ì£¼ì˜ ë‚˜ì´ëŠ” ëª‡ ì‚´ì¸ê°€ìš”?",  # QnA
        "SketchUpì—ì„œ ê³ ê¸‰ 3D ëª¨ë¸ë§ ê¸°ë²•ì„ ì „ë¬¸ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”",  # ì „ë¬¸ê°€ ìš”ì²­
    ]
    
    for i, test_input in enumerate(test_inputs, 1):
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ {i}: {test_input}")
        result = process_with_finetuned_models(test_input)
        print(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬: {result['category']}")
        print(f"ğŸ“Š ì‹ ë¢°ë„: {result['confidence']:.2f}")
        print(f"ğŸ¤– ëª¨ë¸: {result['model_used']}")
        print(f"ğŸ’ í’ˆì§ˆ: {result['quality_level']}")
        print(f"â±ï¸ ì‘ë‹µì‹œê°„: {result['response_time']:.2f}ì´ˆ")
        print(f"ğŸ’¬ ì‘ë‹µ: {result['response'][:100]}...")
        
    print(f"\nğŸ“ˆ ì‹œìŠ¤í…œ í†µê³„:")
    stats = get_system_stats()
    for key, value in stats.items():
        print(f"   {key}: {value}")
